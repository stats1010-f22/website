---
title: "Confidence intervals, statistical tests, and comparison in `R`"
---

::: callout-note
These supplemental notes addends the following lectures: [Confidence intervals](https://stats1010-f22.github.io/website/slides/lect_14.html#/title-slide), [Statistical tests](https://stats1010-f22.github.io/website/slides/lect_15.html#/title-slide), and [Comparison](https://stats1010-f22.github.io/website/slides/lect_16.html#/title-slide). It demonstrates how to do some of the in class examples in `R`, and how to do the same tests but with data instead of calculations. This will be useful for homework and projects.
:::

```{r import_packages}
#|echo: false
#|eval: true
#|message: false

library(tidyverse)
```

## confidence intervals

### for the proportion

For confidence intervals for the proportion use `prop.test()`:

The solution to [example 1](https://stats1010-f22.github.io/website/slides/lect_14.html#/example-1) is:

```{r lect_14-prop_example_1}
#| echo: true
#| eval: true

## The expected counts (x) is .14*35
prop.test(x = .14*350, n = 350, correct = FALSE)
```

The solution to [example 2](https://stats1010-f22.github.io/website/slides/lect_14.html#/example-2) is:

```{r lect_14-prop_example_2}
#| echo: true
#| eval: true


prop.test(x = 35, n = 225, correct = FALSE)
```

The solutions in the slides are not exactly those given by `R`. The slides are using a [normal approximation](https://en.m.wikipedia.org/wiki/Binomial_proportion_confidence_interval) to compute the confidence interval while `R` uses the Wilson interval. The interval from `R` is more accurate because it does two things:

1\. It's wider around $0.5$ than away from $0.5$. This is important because an estimate for the proportion can not be below $0$ or above $1$. It makes sense that it should be shorter at the edges and wider in the middle.

2\. It lets the variance change along the interval. At p_low it uses $p_{low}(1-p_{low})$ and at $p_{high}$ it uses $p_{high}(1-p_{high})$ where as the approximation we use has fixed variance.

### for the mean

To find confidence intervals for the mean use `t.test()`:

[Example 1](https://stats1010-f22.github.io/website/slides/lect_14.html#/example-1-1) and [example 2](https://stats1010-f22.github.io/website/slides/lect_14.html#/example-2-1) do not have a built-in function in `R` to automagically compute them. Instead, I show you how to compute them directly from the data:

```{r lecture_14-mean_data}
#| echo: true
#| eval: true

## default confidence level is 95%
t.test(diamonds$price)

## a 72% confidence level
t.test(diamonds$price, conf.level = 0.72)
```

## statistical tests

### for proportions

The solution for [example 3](https://stats1010-f22.github.io/website/slides/lect_15.html#/example-3) follows:

```{r lect_15-example_3}
#| echo: true
#| eval: true

prop.test(x = 17, n = 36, p = 9/24, alternative = "greater")
```

Again, this is not the same as our calculations because `R` is more accurate.

### for means, directly from the data

The following code does this directly from the data:

```{r lect_15-example_data}
#| echo: true
#| eval: true

## alternative: true mean is not equal to 2000
t.test(diamonds$price, mu = 2000, alternate = "two.sided")

## alternative: true mean is greater than 2000
t.test(diamonds$price, mu = 2000, alternate = "greater")

## a 72% confidence level
t.test(diamonds$price, conf.level = 0.72)
```

## comparison

### two sample $z$ test for proportions

[Example 2](https://stats1010-f22.github.io/website/slides/lect_16.html#/example-2) can almost be done in `R`. This does not specify the difference, just that the $H_A:p_s>p_i$: the population average estimated by $\hat{p_s}$ input first (280/809) is greater than the population average estimated by $\hat{p_i}$ the fraction input second (197/646))

```{r lect_16-example_2}
#| echo: true
#| eval: true

## alternative: true mean is not equal to 2000
prop.test(x = c(280, 197), n = c(809, 646), alternative = "greater")
```

### two sample $z$ test for proportions

[example 3](https://stats1010-f22.github.io/website/slides/lect_16.html#/example-3). Again, the calculation in `R` is more accurate.

```{r lect_16-example_3}
#| echo: true
#| eval: true

## default is 95% confidence interval
prop.test(x = c(280, 197), n = c(809, 646))
```

### two sample $t$ test for means

```{r lect_16-diff_mean}

d.color <- diamonds %>% 
  filter(color == "D") %>% 
  select(carat)
  
j.color <- diamonds %>% 
  filter(color == "J") %>% 
  select(carat)

## 2 - sided test
## Alternative hypothesis: x != y
t.test(x = d.color, y = j.color, 
       alternative = "two.sided")

## Alternative hypothesis: x < y by 0.1
t.test(x = d.color, y = j.color, 
       alternative  = "less", mu = .1)
```

### CI for the difference between two means

```{r lect_16-CI_diff_mean}
#| echo: true
#| eval: true

t.test(x = d.color, y = j.color, conf.level = 0.95)
```

[example 5](https://stats1010-f22.github.io/website/slides/lect_16.html#/in-r-3) is already done in `R`

### Paired comparisons

```{r lect_16-paired}
#| echo: true
#| eval: true

# Weight of mice before treatment
before <-c(200.1, 190.9, 192.7, 213, 241.4, 196.9, 172.2, 185.5, 205.2, 193.7)
# Weight of mice after treatment
after <-c(392.9, 393.2, 345.1, 393, 434, 427.9, 422, 383.9, 392.3, 352.2)

# A tibble
mice <- tibble(
  group = rep(c("before", "after"), each = 10),
  weight = c(before, after)
  )

t.test(weight ~ group, data = mice, paired = TRUE)
```
