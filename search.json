[
  {
    "objectID": "supplemental/chi-squared.html",
    "href": "supplemental/chi-squared.html",
    "title": "Chi-squared test expected values",
    "section": "",
    "text": "Information about the Chi-Squared test can be found here. We have two hypotheses:\n\\(H_0\\) = the variables are independent, there is no relationship between the two categorical variables.\n\\(H_A\\) = Knowing the value of one variable helps to predict the value of the other variable.\nUnder the null hypothesis, the distribution of one variable is independent to the distribution of the other. This means that the expected count for one cell, can be found using the column total times the row total divided by the total number of observations."
  },
  {
    "objectID": "supplemental/exam-3-studyguide.html",
    "href": "supplemental/exam-3-studyguide.html",
    "title": "Study list for exam 3",
    "section": "",
    "text": "The R functions we have discussed in these weeks:\n\n\n\nfunctions\n\n\nqnorm()\n\n\npnorm()"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Equation 1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\tag{1}\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Equation 1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation 2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\tag{2}\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Equation 3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\tag{3}\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Equation 4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\tag{4}\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that’s more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{5}\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\tag{6}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Equation 5 and Equation 6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\tag{7}\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Equation 7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\tag{8}\\]"
  },
  {
    "objectID": "supplemental/exam-revision-3.html",
    "href": "supplemental/exam-revision-3.html",
    "title": "Study list for exam 3",
    "section": "",
    "text": "The R functions we have discussed in these weeks:\n\n\n\nqt()\n\n\nt.test()\n\n\npchisq()\n\n\nchisq.test()\n\n\nlm()\n\n\naugment()\n\n\n\n\nLecture 15 Confidence Intervals\nDistribution of proportion estimate\nConfidence Intervals for Proportions\nSRS Assumption\nCI manipulations\nConfidence Interval for Mean\nt-distributions\nMargin of Error\nSRS condition and Sample size condition\n\n\nLecture 16 Statistical Tests\nNull and alternative hypotheses\nType I and II Errors for Tests Testing a proportion: \\[ \\hat p\\sim \\mathcal{N}\\left(p_0, \\frac{p_0(1-p_0)}{n}\\right)\\] Multiple testing, impact and motivation\n\n\nLecture 17 Comparison\nTwo sample z-test for proportions\nStandard Error formula\nAssumptions for two-sample z-test for proportions\nConfidence Interval for Difference between Means\n\\[(\\bar X_1 - X_2 - t_{\\alpha/2}\\text{se}(\\bar X_1-\\bar X_2),\\bar X_1 - X_2 + t_{\\alpha/2}\\text{se}(\\bar X_1-\\bar X_2))\\]\n\\[\\text{se}(\\bar X_1-\\bar X_2) = \\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}.\\]\nCode for paired t-test in R\nPaired comparisons\n\n\nLecture 18 Inference for Counts\nTesting for independence using \\(\\chi^2\\) test\nTesting for goodness of fit using \\(\\chi^2\\) test\nDegrees of freedom: \\[\\text{df}=(r-1)(c-1)\\]\nR code to perform \\(\\chi^2\\) test and get p-value from test statistic\n\n\nLecture 19 Linear Patterns\nResponse and explanatory variables\n\\(R^2\\) for linear models\nSlope and intercept for linear models\nMathematical formulas for slope and intercept\nResiduals of linear model\nR code for fitting linear models"
  },
  {
    "objectID": "supplemental/poisson.html",
    "href": "supplemental/poisson.html",
    "title": "Limits of the binomial distribution is the poisson",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. James Tanton. They are provided for students who want to dive deeper into the mathematics behind the Poisson distribution. Additional supplemental notes will be added throughout the semester.\nI don’t know statistics! I’ve only ever taken one—very basic—introductory course on the subject from which I feel I learned very little. The rest of my understanding of the subject has been from personal reading and from teaching my own course on the subject! But I’ve secretly been longing for understanding the history and theoretical underpinnings of the topic for a very long time: Why is the central limit theorem true – mathematically and intuitively? How does one actually figure out the chi-squared distribution? Why, really, do you divide by \\(n-1\\) in the formula for standard deviation? And so on. (I figured out one answer to that last question here).\nI was reminded of my lack of knowledge of statistics last week at a university reception. An astronomer was chatting with his graduate student and said “Everything is the Poisson distribution” to then look at me, the mathematician, to say “Right?”\nIt might be shocking to learn that I don’t know what the Poisson distribution is! Well, I didn’t at that moment.\nI have a maxim: It is okay not to know. But it is not okay not to want to find out.\nSo I decided this month to find out about the Poisson distribution and think about its mathematical meaning.In looking up the formula one retrieves:\n\\[P(X=k)=\\frac{\\lambda^k}{k!} e^{-\\lambda}\\] for \\(k\\in\\{0,1, 2,3,\\ldots\\}.\\)\nThis reads: Some random variable \\(X\\) can take on non-negative integer values and the chances that you’ll actually see it adopt a particular value \\(k\\) is given by a crazy formula that depends on some constant parameter \\(\\lambda\\).\nGot it? Is it now clear that everything is Poisson?\nStatisticians and mathematicians do not come up with formulas out of the air. What is the story behind this bizarreness?"
  },
  {
    "objectID": "supplemental/poisson.html#perhaps-everything-is-binomial",
    "href": "supplemental/poisson.html#perhaps-everything-is-binomial",
    "title": "Limits of the binomial distribution is the poisson",
    "section": "Perhaps Everything is Binomial?",
    "text": "Perhaps Everything is Binomial?\nIf I roll a die three times, what are the chances I’ll get exactly two sixes?\nMaybe I’ll first roll a non-six and then two sixes:\n\\[N \\ 6 \\ 6\\] The chances of seeing this are \\(\\frac{5}{6}\\times \\frac{1}{6}\\times\\frac{1}{6}\\). Or, following the implied notation, I could roll \\(6\\ N\\ 6\\) or \\(6\\ 6\\ N\\) with probabilities\n\\(\\frac{1}{6}\\times \\frac{5}{6}\\times\\frac{1}{6}\\) and \\(\\frac{1}{6}\\times \\frac{1}{6}\\times\\frac{5}{6}\\), respectively.\nSo we see that there are \\(3\\) ways I could see two sixes and one non-six in a roll of three, each with the same probability. The chances of me seeing exactly two sixes is thus\n\\[ 3\\left(\\frac{1}{6}\\right)^2\\left(\\frac{5}{6}\\right) \\]\nIn general, if in an experiment I have a probability \\(p\\) of seeing a success, and hence a probability \\(1-p\\) of seeing a failure, and I run the experiment \\(n\\) times in a row, the probability that I will see exactly \\(k\\) successes is \\[\\binom{n}{k}p^k(1-p)^{n-k}\\]\nHere \\(\\binom{n}{k}=\\frac{n!}{k!(n-k)!}\\)is “\\(n\\) choose \\(k\\) ,” the number of ways \\(k\\) successes can be arranged among a string \\(n\\) units long. For example, the chances of seeing exactly two sixes after rolling a die three times is \\[\\frac{3!}{2!1!}\\left(\\frac{1}{6}\\right)^2\\left(1-\\frac{1}{6}\\right)=3\\left(\\frac{1}{6}\\right)^2\\left(\\frac{5}{6}\\right) \\] as we saw.\nWe have here a binomial distribution. In general, for each success probability \\(p\\) and run length \\(n\\) we have the distribution \\[\\mathbb{P}(X=k) = \\binom{n}{k}p^k(1-p)^{n-k}\\] for \\(k\\in\\{0,1, 2,\\ldots,n \\}.\\) Here \\(X\\) is the random variable given as the number of times I see a success in running an experiment \\(n\\) times in a row.”\nIn the 1700s scholars became interested in questions about runs of events that seem to occur in an ongoing fashion and counting the number of occurrences of the event you might expect to see in any selected time period. French mathematicians Siméon Denis Poisson and Abraham de Moivre were the first to develop the mathematics for this.\nI’ve been sitting on my porch for many hours of my life, counting the number of cars that pass by each hour. After many hours of observation I can say that the flow of cars is more or less regular (there is no difference in traffic flow in the day versus the night) and that, on average, \\(12\\) cars pass my house each hour.\nI am about to go sit out on my porch for an hour. What is the probability I will see \\(15\\) cars?\nOne possible start to this question is to treat it as a run of an experiment, not over an hour but, instead, one experiment per minute for \\(60\\) minutes. If I know the chances \\(p\\) of seeing a car in a given minute, then the chances of seeing \\(15\\) cars over a course of \\(60\\) minutes would be \\[\\binom{60}{15} p^{15}(1-p)^{45}.\\] How might I estimate \\(p\\)?\nWell, I expect to see an average of \\(12\\) cars over the course an hour. So, on average, I expect to see one car in any five-minute period. So chances of me selecting a minute with the appearance of a car in it should be \\(p=\\frac{1}{5}\\) That’s it. We have a formula for the chances of seeing \\(15\\) cars over the course of an hour. \\[ \\binom{60}{15}0.2^{15}0.8^{45}\\approx 0.076.\\] There is a problem with our work here: we’ve assumed that cars are spaced apart so that I’ll never see two cars within the same minute. To obviate this objection, let’s work instead with the \\(3600\\) seconds in an hour: I’ve never seen two cars go by within the same second.\nNow, on average, over the \\(3600\\) seconds of an hour, \\(12\\) of them will have a car “in them,” and so the chances of me seeing a car in any particular second is \\(p=\\frac{12}{3600}\\)\nThis now gives \\[\\binom{3600}{15}\\left(\\frac{12}{3600}\\right)^{15} \\left(1-\\frac{12}{3600}\\right)^{3555}\\approx0.080\\] for the chances of me seeing exactly \\(15\\) cars.\nWell, actually, I realise now that I think I have seen two cars go by within the same second: two cars going in opposite directions passed each other in front of my house.\nSo maybe I should repeat this analysis not over every minute or every second, but instead over every nanosecond or every picosecond. I need a very short length of time I can be sure will never have two cars appearing in it.\nLet’s break the hour into \\(n\\), almost instantaneous, units of time. The chances of me seeing a car within any one of those units is \\(p=\\frac{12}{n}\\) and so the probability of me seeing \\(15\\) cars over a run of \\(n\\) of these units of time is \\[\\binom{n}{15}\\left(\\frac{12}{n}\\right)^{15}\\left(1-\\frac{12}{n}\\right)^{n-15}.\\] Ideally, we should compute this for larger and larger values of \\(n\\). That is, we should take the limit as \\(n\\) grows infinitely large."
  },
  {
    "objectID": "supplemental/poisson.html#taking-the-limit",
    "href": "supplemental/poisson.html#taking-the-limit",
    "title": "Limits of the binomial distribution is the poisson",
    "section": "Taking the Limit",
    "text": "Taking the Limit\nThe formula we have reads \\[\\frac{n(n-1)(n-14)}{15!}\\cdot\\frac{12^{15}}{n^{15}}\\cdot\\left(1-\\frac{12}{n}\\right)^n\\cdot\\frac{1}{\\left(1-\\frac{12}{n}\\right)^{15}}.\\] I can see how parts of this formula change as \\(n\\) grows larger and larger.\nFor instance \\[ \\frac{1}{\\left(1-\\frac{12}{n}\\right)^{15}}\\to\\frac{1}{(1-0)^{15}}=1\\] as \\(n\\) grows.\nAlso, \\[ \\frac{n(n-1)\\cdots(n-14)}{n^{15}}=\\left(1-\\frac{1}{n}\\right)\\left(1-\\frac{2}{n}\\right)\\cdots\\left(1-\\frac{14}{n}\\right)\\to1\\cdot\\cdots 1 = 1\\] as \\(n\\) grows.\nAnd the formula \\(\\left(1-\\frac{12}{n}\\right)^{n}\\) is reminiscent of the famous compound-interest formula \\[ \\lim_{n\\to\\infty}\\left(1+\\frac{1}{n}\\right)^n=e\\] or more generally, \\[ \\lim_{n\\to\\infty}\\left(1+\\frac{x}{n}\\right)^n=e^x\\] So we have that \\[ \\left(1-\\frac{12}{n}\\right)^n\\to e^{-12}\\] as \\(n\\) grows.\nSo we see that our formula, \\[\n\\left(\\begin{array}{c}\nn \\\\\n15\n\\end{array}\\right)\\left(\\frac{12}{n}\\right)^{15}\\left(1-\\frac{12}{n}\\right)^{n-15}\n\\] in the absolute ideal of looking at finer and finer intervals over the hour, becomes the formula \\[\n1 \\cdot \\frac{12^{15}}{15 !} \\cdot e^{-12} \\cdot 1=\\frac{12^{15}}{15 !} e^{-12}.\n\\] This has value \\(\\approx 0.072\\). There is about a \\(7\\)% chance I will see \\(15\\) cars over any given hour.\nMost important, this work has led us to the Poisson probability distribution formula!\nIf you have a phenomenon that seems to occur at a more-or-less steady rate, with the number occurrences during any set time length seeming to be more-or-less the same over all periods of that length, and no two events can occur at exactly the same instant, then, if you have good reason to believe that, for a given period length, on average \\(\\lambda\\) events occur, the probability that you will see \\(k\\) events during any particular time period of that length is \\[\\frac{\\lambda^k}{k !} e^{-\\lambda}.\\]\nThis is the Poisson distribution.\nI am guessing that many astronomical phenomena occur at more-or-less steady rates, on average—meteor strikes to the Earth, appearance of Sun spots—and so maybe close to everything in astronomy is indeed Poisson?\nAside: From \\(\\lim_{n\\to\\infty}\\left(1+\\frac{1}{n}\\right)^n=e\\) we see that \\[\n\\left(1+\\frac{x}{n}\\right)^n=\\left(\\left(1+\\frac{1}{n / x}\\right)^{n / x}\\right)^x \\rightarrow(e)^x\n\\]\nif \\(x\\) is positive (since \\(n/x\\) grows large and positive if \\(n\\) does). We also establish \\[\\lim _{n \\rightarrow \\infty}\\left(1-\\frac{1}{n}\\right)^n=e^{-1}\\]\nby observing that \\[\\begin{aligned}\\left(1-\\frac{1}{n}\\right)^n &=\\left(\\frac{n-1}{n}\\right)^n=\\frac{1}{\\left(\\frac{n}{n-1}\\right)^n} \\\\ &=\\frac{1}{\\left(1+\\frac{1}{n-1}\\right)^{n-1} \\cdot\\left(1+\\frac{1}{n-1}\\right)} \\\\ & \\rightarrow \\frac{1}{e \\cdot(1+0)}=\\frac{1}{e} \\end{aligned}\\]\nas \\(n\\) grows.\nFollowing similar algebra, we see that for negative \\(x\\), setting \\(y=-x\\), \\[ \\left(1+\\frac{x}{n}\\right)^n=\\left(1-\\frac{y}{n}\\right)^n=\\left(\\left(1-\\frac{1}{n/y}\\right)^{n/y}\\right)^y\\to e^{-y}=e^{x}.\\] Therefore \\[\n\\lim_{n\\to\\infty}\\left(1+\\frac{x}{n}\\right)^n=e^x\n\\] even if \\(x\\) is negative.\n© 2017 James Tanton tanton.math@gmail.com"
  },
  {
    "objectID": "supplemental/exam-1-studyguide.html",
    "href": "supplemental/exam-1-studyguide.html",
    "title": "Study list for exam 1",
    "section": "",
    "text": "The R functions we have discussed so far these few weeks:\n\n\n\nfunctions\noperators\npipes\n\n\nfilter()\n%/%\n%>%\n\n\ndistinct()\n%%\n“+”\n\n\ncount()\n!=\n\n\n\nselect()\nrowSums()\n\n\n\nmutate()\ncolSums()\n\n\n\nView()\n\n\n\n\nglimpse()\n\n\n\n\npivot_wider()\n\n\n\n\ngroup_by()\n\n\n\n\nchisq.test()\n\n\n\n\nggplot()\n\n\n\n\ngeom_hist()\n\n\n\n\ngeom_bar()\n\n\n\n\ngeom_point()\n\n\n\n\nlabs()\n\n\n\n\nfacet_wrap()\n\n\n\n\n\nBenefits and downsides of these visualization methods: histogram and boxplot.\nConditional probability tables - which variable is it conditioned on.\nMean, median, mode and which is best in skewed distributions. Identify in a plot\nMeasures of spread: standard deviation, variance, quartiles, percentiles, interquartile range.\nChi-squared expected counts given a contingency table of counts.\nIndependent events\nDisjoint events\nVenn diagrams\nCompliment\nThe three laws of probability - Kolomogorov’s axioms\nClassification of variables"
  },
  {
    "objectID": "supplemental/tests_R.html",
    "href": "supplemental/tests_R.html",
    "title": "Confidence intervals, statistical tests, and comparison in R",
    "section": "",
    "text": "Note\n\n\n\nThese supplemental notes addends the following lectures: Confidence intervals, Statistical tests, and Comparison. It demonstrates how to do some of the in class examples in R, and how to do the same tests but with data instead of calculations. This will be useful for homework and projects."
  },
  {
    "objectID": "supplemental/tests_R.html#confidence-intervals",
    "href": "supplemental/tests_R.html#confidence-intervals",
    "title": "Confidence intervals, statistical tests, and comparison in R",
    "section": "confidence intervals",
    "text": "confidence intervals\n\nfor the proportion\nFor confidence intervals for the proportion use prop.test():\nThe solution to example 1 is:\n\n## The expected counts (x) is .14*35\nprop.test(x = .14*350, n = 350, correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  0.14 * 350 out of 350, null probability 0.5\nX-squared = 181.44, df = 1, p-value < 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.1075436 0.1802730\nsample estimates:\n   p \n0.14 \n\n\nThe solution to example 2 is:\n\nprop.test(x = 35, n = 225, correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  35 out of 225, null probability 0.5\nX-squared = 106.78, df = 1, p-value < 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.1140250 0.2086502\nsample estimates:\n        p \n0.1555556 \n\n\nThe solutions in the slides are not exactly those given by R. The slides are using a normal approximation to compute the confidence interval while R uses the Wilson interval. The interval from R is more accurate because it does two things:\n1. It’s wider around \\(0.5\\) than away from \\(0.5\\). This is important because an estimate for the proportion can not be below \\(0\\) or above \\(1\\). It makes sense that it should be shorter at the edges and wider in the middle.\n2. It lets the variance change along the interval. At p_low it uses \\(p_{low}(1-p_{low})\\) and at \\(p_{high}\\) it uses \\(p_{high}(1-p_{high})\\) where as the approximation we use has fixed variance.\n\n\nfor the mean\nTo find confidence intervals for the mean use t.test():\nExample 1 and example 2 do not have a built-in function in R to automagically compute them. Instead, I show you how to compute them directly from the data:\n\n## default confidence level is 95%\nt.test(diamonds$price)\n\n\n    One Sample t-test\n\ndata:  diamonds$price\nt = 228.95, df = 53939, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 3899.132 3966.467\nsample estimates:\nmean of x \n   3932.8 \n\n## a 72% confidence level\nt.test(diamonds$price, conf.level = 0.72)\n\n\n    One Sample t-test\n\ndata:  diamonds$price\nt = 228.95, df = 53939, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n72 percent confidence interval:\n 3914.243 3951.357\nsample estimates:\nmean of x \n   3932.8"
  },
  {
    "objectID": "supplemental/tests_R.html#statistical-tests",
    "href": "supplemental/tests_R.html#statistical-tests",
    "title": "Confidence intervals, statistical tests, and comparison in R",
    "section": "statistical tests",
    "text": "statistical tests\n\nfor proportions\nThe solution for example 3 follows:\n\nprop.test(x = 17, n = 36, p = 9/24, alternative = \"greater\")\n\n\n    1-sample proportions test with continuity correction\n\ndata:  17 out of 36, null probability 9/24\nX-squared = 1.0667, df = 1, p-value = 0.1508\nalternative hypothesis: true p is greater than 0.375\n95 percent confidence interval:\n 0.3294798 1.0000000\nsample estimates:\n        p \n0.4722222 \n\n\nAgain, this is not the same as our calculations because R is more accurate.\n\n\nfor means, directly from the data\nThe following code does this directly from the data:\n\n## alternative: true mean is not equal to 2000\nt.test(diamonds$price, mu = 2000, alternate = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  diamonds$price\nt = 112.52, df = 53939, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 2000\n95 percent confidence interval:\n 3899.132 3966.467\nsample estimates:\nmean of x \n   3932.8 \n\n## alternative: true mean is greater than 2000\nt.test(diamonds$price, mu = 2000, alternate = \"greater\")\n\n\n    One Sample t-test\n\ndata:  diamonds$price\nt = 112.52, df = 53939, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 2000\n95 percent confidence interval:\n 3899.132 3966.467\nsample estimates:\nmean of x \n   3932.8 \n\n## a 72% confidence level\nt.test(diamonds$price, conf.level = 0.72)\n\n\n    One Sample t-test\n\ndata:  diamonds$price\nt = 228.95, df = 53939, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n72 percent confidence interval:\n 3914.243 3951.357\nsample estimates:\nmean of x \n   3932.8"
  },
  {
    "objectID": "supplemental/tests_R.html#comparison",
    "href": "supplemental/tests_R.html#comparison",
    "title": "Confidence intervals, statistical tests, and comparison in R",
    "section": "comparison",
    "text": "comparison\n\ntwo sample \\(z\\) test for proportions\nExample 2 can almost be done in R. This does not specify the difference, just that the \\(H_A:p_s>p_i\\): the population average estimated by \\(\\hat{p_s}\\) input first (280/809) is greater than the population average estimated by \\(\\hat{p_i}\\) the fraction input second (197/646))\n\n## alternative: true mean is not equal to 2000\nprop.test(x = c(280, 197), n = c(809, 646), alternative = \"greater\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(280, 197) out of c(809, 646)\nX-squared = 2.5769, df = 1, p-value = 0.05422\nalternative hypothesis: greater\n95 percent confidence interval:\n -0.0007927079  1.0000000000\nsample estimates:\n   prop 1    prop 2 \n0.3461063 0.3049536 \n\n\n\n\ntwo sample \\(z\\) test for proportions\nexample 3. Again, the calculation in R is more accurate.\n\n## default is 95% confidence interval\nprop.test(x = c(280, 197), n = c(809, 646))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(280, 197) out of c(809, 646)\nX-squared = 2.5769, df = 1, p-value = 0.1084\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.008561667  0.090867154\nsample estimates:\n   prop 1    prop 2 \n0.3461063 0.3049536 \n\n\n\n\ntwo sample \\(t\\) test for means\n\nd.color <- diamonds %>% \n  filter(color == \"D\") %>% \n  select(carat)\n  \nj.color <- diamonds %>% \n  filter(color == \"J\") %>% \n  select(carat)\n\n## 2 - sided test\n## Alternative hypothesis: x != y\nt.test(x = d.color, y = j.color, \n       alternative = \"two.sided\")\n\n\n    Welch Two Sample t-test\n\ndata:  d.color and j.color\nt = -41.811, df = 3683.7, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5279915 -0.4806923\nsample estimates:\nmean of x mean of y \n0.6577948 1.1621368 \n\n## Alternative hypothesis: x < y by 0.1\nt.test(x = d.color, y = j.color, \n       alternative  = \"less\", mu = .1)\n\n\n    Welch Two Sample t-test\n\ndata:  d.color and j.color\nt = -50.101, df = 3683.7, p-value < 2.2e-16\nalternative hypothesis: true difference in means is less than 0.1\n95 percent confidence interval:\n       -Inf -0.4844961\nsample estimates:\nmean of x mean of y \n0.6577948 1.1621368 \n\n\n\n\nCI for the difference between two means\n\nt.test(x = d.color, y = j.color, conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  d.color and j.color\nt = -41.811, df = 3683.7, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5279915 -0.4806923\nsample estimates:\nmean of x mean of y \n0.6577948 1.1621368 \n\n\nexample 5 is already done in R\n\n\nPaired comparisons\n\n# Weight of mice before treatment\nbefore <-c(200.1, 190.9, 192.7, 213, 241.4, 196.9, 172.2, 185.5, 205.2, 193.7)\n# Weight of mice after treatment\nafter <-c(392.9, 393.2, 345.1, 393, 434, 427.9, 422, 383.9, 392.3, 352.2)\n\n# A tibble\nmice <- tibble(\n  group = rep(c(\"before\", \"after\"), each = 10),\n  weight = c(before, after)\n  )\n\nt.test(weight ~ group, data = mice, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  weight by group\nt = 20.883, df = 9, p-value = 6.2e-09\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 173.4219 215.5581\nsample estimates:\nmean difference \n         194.49"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html",
    "href": "supplemental/exam-2-studyguide.html",
    "title": "Study list for exam 2",
    "section": "",
    "text": "The R functions we have discussed in these weeks:"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#addition-rules",
    "href": "supplemental/exam-2-studyguide.html#addition-rules",
    "title": "Study list for exam 2",
    "section": "Addition rules",
    "text": "Addition rules\n-\\(E(X \\pm c) = E(X) \\pm c\\)\n-\\(SD(X \\pm c) = SD(X)\\)\n-\\(Var(X \\pm c) = Var(X)\\)"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#multiplication-rules",
    "href": "supplemental/exam-2-studyguide.html#multiplication-rules",
    "title": "Study list for exam 2",
    "section": "Multiplication rules",
    "text": "Multiplication rules\n-\\(E(cX) = cE(X)\\)\n-\\(SD(cX) = |c|SD(X)\\)\n-\\(Var(cX) = c^2Var(X)\\)\n-\\(E(cX \\pm a)=cE(X) \\pm a\\)\n- \\(Var(cX \\pm a)=c^2Var(X)\\)"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#which-variable-on-which-axis-and-what-are-the-names-for-each-variable",
    "href": "supplemental/exam-2-studyguide.html#which-variable-on-which-axis-and-what-are-the-names-for-each-variable",
    "title": "Study list for exam 2",
    "section": "Which variable on which axis and what are the names for each variable?",
    "text": "Which variable on which axis and what are the names for each variable?"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#describing-scatter-plots",
    "href": "supplemental/exam-2-studyguide.html#describing-scatter-plots",
    "title": "Study list for exam 2",
    "section": "Describing scatter plots",
    "text": "Describing scatter plots\n\nHomo and heteroskedastic"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#covariance-look-at-a-plot-and-know-if-positive-or-negative",
    "href": "supplemental/exam-2-studyguide.html#covariance-look-at-a-plot-and-know-if-positive-or-negative",
    "title": "Study list for exam 2",
    "section": "Covariance look at a plot and know if positive or negative",
    "text": "Covariance look at a plot and know if positive or negative\n\\(cov(x, y) = \\frac{(x_1 - \\bar{x})(y_1 - \\bar{y}) + (x_2 - \\bar{x})(y_2 - \\bar{y}) + \\ldots + (x_n - \\bar{x})(y_n - \\bar{y})}{n-1}\\)"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#correlation---look-at-a-plot-and-approximate",
    "href": "supplemental/exam-2-studyguide.html#correlation---look-at-a-plot-and-approximate",
    "title": "Study list for exam 2",
    "section": "Correlation - look at a plot and approximate",
    "text": "Correlation - look at a plot and approximate\n\\[corr(x, y) = \\frac{cov(x, y)}{s_x \\cdot s_y}\\]\n\nReferred to as \\(r\\)\nStrength of linear association\n\\(r\\) is always between \\(-1\\) and \\(+1\\), \\(-1 \\leq r \\leq 1\\).\n\\(r\\) does not have units"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#fitting-a-line",
    "href": "supplemental/exam-2-studyguide.html#fitting-a-line",
    "title": "Study list for exam 2",
    "section": "Fitting a line",
    "text": "Fitting a line\n\\(m = \\frac{r \\cdot s_y}{s_x}\\)\n\\(b = \\bar{y} - m \\bar{x}\\)"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#predicting",
    "href": "supplemental/exam-2-studyguide.html#predicting",
    "title": "Study list for exam 2",
    "section": "Predicting",
    "text": "Predicting\n\nlibrary(tidymodels)\n\n## Assign the least\n## squares line\nleast_squares_fit <- \n  linear_reg() %>% \n  set_engine(\"lm\") %>% \n  fit(price ~ carat, data = diamonds) \n## NOTE: outcome first, predictor second\n\n## Find the prediction\npredict(least_squares_fit, tibble(carat = c(1, 2, 2.5, 4)))\n\n# A tibble: 4 × 1\n   .pred\n   <dbl>\n1  5500.\n2 13256.\n3 17135.\n4 28769."
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#spurious-correlations",
    "href": "supplemental/exam-2-studyguide.html#spurious-correlations",
    "title": "Study list for exam 2",
    "section": "Spurious correlations",
    "text": "Spurious correlations\nbirth order associated with increased risk of downs syndrome, etc…"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#the-sharpe-ratio",
    "href": "supplemental/exam-2-studyguide.html#the-sharpe-ratio",
    "title": "Study list for exam 2",
    "section": "The Sharpe ratio",
    "text": "The Sharpe ratio\n\\(S(X) = \\frac{\\mu_X - r_f}{\\sigma_X}\\) - higher better - how to compute with a calculator from a pdf and also with just \\(\\mu\\) and \\(\\sigma\\) - computing in R"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#joint-pdfs",
    "href": "supplemental/exam-2-studyguide.html#joint-pdfs",
    "title": "Study list for exam 2",
    "section": "Joint pdfs",
    "text": "Joint pdfs\nFind probability given values Are they independent or not? What do increasing and decreasing joint pdfs look like?\n\\(E(X+Y) = E(X) + E(Y)\\) regardless of independence"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#rules-of-independence",
    "href": "supplemental/exam-2-studyguide.html#rules-of-independence",
    "title": "Study list for exam 2",
    "section": "3 Rules of independence",
    "text": "3 Rules of independence"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#covariance-of-rv",
    "href": "supplemental/exam-2-studyguide.html#covariance-of-rv",
    "title": "Study list for exam 2",
    "section": "Covariance of RV",
    "text": "Covariance of RV\n\\(Cov(X, Y) = E((X - \\mu_X)(Y - \\mu_Y))\\) \\(Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)\\) If \\(X\\),\\(Y\\) are independent, then \\(Cov(X, Y) = 0\\), opposite not true If \\(X\\), \\(Y\\) are independent, \\(Cov(X, Y) = 0\\), so \\(Var(X+ Y) = Var(X) + Var(Y)\\)"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#correlation",
    "href": "supplemental/exam-2-studyguide.html#correlation",
    "title": "Study list for exam 2",
    "section": "Correlation",
    "text": "Correlation\n\\(corr(x, y) = \\frac{cov(x, y)}{s_x \\cdot s_y}\\) \\[\\rho = Corr(X, Y) = \\frac{Cov(X, Y)}{\\sigma_X \\cdot \\sigma_Y}\\] ## IID variables \\(E(aX + bY + c) = aE(X) + bE(Y) + c\\) \\(Var(aX + bY + c) = a^2Var(X) + b^2Var(Y) + 2abCov(X, Y)\\)"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#bernoulli-trial",
    "href": "supplemental/exam-2-studyguide.html#bernoulli-trial",
    "title": "Study list for exam 2",
    "section": "Bernoulli trial",
    "text": "Bernoulli trial\nexpectation and variance"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#binomial",
    "href": "supplemental/exam-2-studyguide.html#binomial",
    "title": "Study list for exam 2",
    "section": "Binomial",
    "text": "Binomial\n\\(Y = B_1 + B_2 + ... + B_n\\), where \\(B_1, B_2, ..., B_n\\) Bernoulli trials FIST expectation and variance binomial pdf limiting of \\(p\\) approaches Poisson distribution use R to find these probabilities"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#poisson",
    "href": "supplemental/exam-2-studyguide.html#poisson",
    "title": "Study list for exam 2",
    "section": "Poisson",
    "text": "Poisson\nRIPS expectation and variance use R to find these probabilities"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#shifts-and-scales-of-normal-distribution",
    "href": "supplemental/exam-2-studyguide.html#shifts-and-scales-of-normal-distribution",
    "title": "Study list for exam 2",
    "section": "Shifts and scales of normal distribution",
    "text": "Shifts and scales of normal distribution\nZ score and standardization Use symmetry to find probabilities, finding them in R Percentiles to Z values, finding them in R"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#rule",
    "href": "supplemental/exam-2-studyguide.html#rule",
    "title": "Study list for exam 2",
    "section": "68 - 95 - 99.7 rule",
    "text": "68 - 95 - 99.7 rule"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#normality-tests-and-qqplots",
    "href": "supplemental/exam-2-studyguide.html#normality-tests-and-qqplots",
    "title": "Study list for exam 2",
    "section": "Normality tests and qqplots",
    "text": "Normality tests and qqplots"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#skewness-and-kurtosis",
    "href": "supplemental/exam-2-studyguide.html#skewness-and-kurtosis",
    "title": "Study list for exam 2",
    "section": "Skewness and Kurtosis",
    "text": "Skewness and Kurtosis"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#vocabulary-related-to-sampling",
    "href": "supplemental/exam-2-studyguide.html#vocabulary-related-to-sampling",
    "title": "Study list for exam 2",
    "section": "Vocabulary related to sampling",
    "text": "Vocabulary related to sampling\nsample, population, bias, representative, random, inference, cluster, strata, sampling frame, nonresponse rate, interviewer affects, survivor bias, …"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#sampling-methods",
    "href": "supplemental/exam-2-studyguide.html#sampling-methods",
    "title": "Study list for exam 2",
    "section": "Sampling methods",
    "text": "Sampling methods\nSRS, stratified, cluster, census, voluntary response, convenience benefits and drawbacks of all and where to apply each"
  },
  {
    "objectID": "supplemental/ae-13.html",
    "href": "supplemental/ae-13.html",
    "title": "Exam 3 revision - all possible questions",
    "section": "",
    "text": "Mark each statement True or False. If you believe that a statement is false, briefly explain why you think it is false.\n11. All other things the same, a 90% confidence interval is shorter than a 95% confidence interval.\n12. Ninety-five percent z-intervals have the form of a sta­tistic plus or minus 3 standard errors of the statistic.\n13. By increasing the sample size from n = 100 to n = 400, we can reduce-the margin of error by 50%.\n14. If we double the sample size from n = 50 to n = 100, the length of the confidence interval is reduced by half.\n15. If the 95% confidence interval for the average purchase of customers at a department store is $50 to $110, then $100 is a plausible value for the population mean at this level of confidence.\n16. If the 95% confidence interval for the number of moviegoers who purchase from the concession stand is 30% to 45%, then fewer than half of all moviegoers do not purchase from the concession stand.\n17. If zero lies inside the 95% confidence interval for \\(\\mu\\), then zero is also inside the 99% confidence interval for \\(\\mu\\).\n18. There is a 95% chance that the mean y of a second sample from the same population is in the range \\(\\bar{x} \\pm 1.96\\sigma/\\sqrt{n}.\\)\n19. To guarantee a margin of error of 0.05 for the population proportion \\(p\\), a survey needs to have at least 500 respondents.\n20. The 95% t-interval for \\(\\mu\\) works best if the sample data are normally distributed.\n\n\n\n21. Convert these confidence intervals.\n(a)  [11 pounds to 45 pounds] to kilograms (1 pound = 0.453 kilogram)\n(b)  [$2,300 to $4,400] to yen (Use the exchange rate of $1 = \\(\\yen\\) 116.3.)\n(c)  [$79.50 to $101.44] minus a fixed cost of $25\n(d)  [$465,000 to $729,000] for total revenue of 25 retail stores to a per-store revenue\n22. Convert these confidence intervals.\n(a)  [14.3 liters to 19.4 liters ] to gallons (1 gallon = 3.785 liters )\n(b)  [€234 to €520] to dollars ( Use the exchange rate 1 dollar = 0.821 euro. )\n(c)  5% of [ $23,564 to $45,637]\n(d)  250 items at a profit of [$23.4 to $32.8 ] each (Give the interval for the total.)\n23. What are the chances that \\(\\bar{X} > \\mu\\)\n24. What is the coverage of the confidence interval [\\(\\hat{p}\\) to 1]?\n25. Which is shorter, a 95% z-interval for \\(\\mu\\) or a 95% t-interval for \\(\\mu\\)? Is one of these always shorter, or does the outcome depend on the sample?\n26. Which is more likely to contain \\(\\mu\\), the z-interval \\(\\bar{X} \\pm 1.96\\sigma-\\sqrt{n}\\) or the t-interval \\(\\bar{X} \\pm t_{o.o25,n-1}S/\\sqrt{n}\\)?\n27. The clothing buyer for a department store wants to order the right mix of sizes. As part of a survey, she measured the height (in inches) of men who bought suits at this store. Her software reported the following confidence interval:\nWith 95.00% confidence, 70.8876 < \\(\\mu\\) < 74.4970\n(a) Explain carefully what the software output means.\n(b) What’s the margin of error for this interval?\n(c) How should the buyer round the endpoints of the interval to summarize the result in a report for store managers?\n(d) If the researcher had calculated a 99% confidence interval, would the output have shown a longer or shorter interval?\n28. Data collected at a company produced this confidence interval for the average age of MBAs hired during the past recruiting season.\nWith 95.00% confidence, 26.202 < \\(\\mu\\) < 28.844\n(a) Explain carefully what the software output means.\n(b) What is the margin of error for this interval?\n(c) Round the endpoints of the interval for a scale appropriate for a summary report to management.\n(d) If the researcher had calculated a 90% confidence interval, would the interval be longer or shorter?\n29. A summary of sales of a department store says that the average retail purchase was $125 with a margin of error equal to $15. What does the margin of error mean in this context?\n30. A news report summarizes a poll of voters and then adds that the margin of error is plus or minus 4%. Explain what that means.\n31. To prepare a report on the economy, analysts need to estimate the percentage of businesses that plan to hire additional employees in the next 60 days.\n(a) How many randomly selected employers must you contact in order to guarantee a margin of error of no more than 4%?\n(b) If analysts believe that at most 20% of firms are likely to be hiring, how many must they survey to obtain a margin of error 0.04?\n32. A political candidate is anxious about the outcome of the election.\n(a) To have his next survey result produce a 95% confidence interval with margin of error of no more than 0.025, how many eligible voters are needed in the sample?\n(b) If the candidate fears that he’s way behind and needs to save money, how many voters are needed if he expects that his percentage among voters p is near 0.25?\n33. The Basel II standards for banking specify procedures for estimating the exposure to risk. In particular, Basel II specifies how much cash banks must keep on hand to cover bad loans. One element of these standards is the following formula, which expresses the expected amount lost when a borrower defaults on a loan:\nExpected Loss = PD X EAD X LGD\nwhere PD is the probability of default on the loan, EAD is the exposure at default (the face value of the loan), and LGD is the loss given default (expressed as a percentage of the loan).\nFor a certain class of mortgages, 6% of the borrowers are expected to default. The face value of these mortgages averages $250,000. On average, the bank recovers 80% of the mortgaged amount if the borrower defaults by selling the property.\n(a) What is the expected loss on a mortgage?\n(b) Each stated characteristic is a sample estimate. The 95% confidence intervals are [0.05 to 0.07 for PD, [$220,000 to $290,000] for EAD, and [0.18 to 0.23] for LGD. What effect does this uncertainty have on the expected loss?\n(c) What can be said about the coverage of the range implied by combining these intervals?\n34. Catalog sales companies such as L.L. Bean mail seasonal catalogs to prior customers. The expected profit from each mailed catalog can be expressed as the product\nExpected Profit = p x D x S\nwhere p is the probability that the customer places an order, D is the dollar amount of the order, and S is the percentage profit earned on the total value of an order. Typically, 10% of customers who receive a catalog place orders that average $125, and 20% of that amount is profit.\n(a) What is the expected profit under these conditions?\n(b) The response rates and amounts are sample estimates. If it costs the company $2.00 to mail each catalog, how accurate does the estimate of p need to be in order to convince you that the expected profit from the next mailing is positive?\n\n\n\n35. Find the appropriate percentile from a t-distribution for constructing the following confidence intervals:\n(a) 90% t-interval with n = 12\n(b) 95% t-interval with n = 6\n(c) 99% t-interval with n = 15\n36. Find the appropriate percentile for the following:\n(a) 90% t-interval with n = 5\n(b) 95% t-interval with n = 20\n(c) 99% t-interval with n = 2\n37. Consider each situation described below. Identify the population and the sample, explain what the parameter p or \\(\\mu\\) represents, and tell whether the methods of this chapter can be used to create a confidence interval. If so, find the interval.\n(a) The service department at a new car dealer checks for small dents in cars brought in for scheduled maintenance. It finds that 22 of 87 cars have a dent that can be removed easily. The service department wants to estimate the percentage of all cars with these easily repaired dents.\n(b) A survey of customers at a supermarket asks whether they found shopping at this market more pleasing than at a nearby store. Of the 2,500 forms distributed to customers, 325 were filled in and 250 of these said that the experience was more pleasing.\n(c) A poll asks visitors to a Web site for the number of hours spent Web surfing daily. The poll gets 223 responses one day. The average response is three hours per day with s = 1.5.\n(d) A sample of 1,000 customers given loans during the past two years contains 2 who have defaulted.\n38. Consider each situation. Identify the population and the sample, explain what p or p, represents, and tell whether the methods of this chapter can be used to create a confidence interval. If so, indicate what the interval would say about the parameter.\n(a) A consumer group surveys 195 people who recently bought new kitchen appliances. Fifteen percent of them expressed dissatisfaction with the salesperson.\n(b) A catalog mail order firm finds that the number of days between orders for a sample of 250 cus­tomers averages 105 days with s = 55.\n(c) A questionnaire given to customers at a ware­house store finds that only 2 of the 50 who return the questionnaire live more than 25 miles away\n(d) A factory is considering requiring employees to wear uniforms. In a survey of all 1,245 employ­ees, 380 forms are returned, with 228 employees in favor of the change.\n39. Hoping to lure more shoppers downtown, a city builds a new public parking garage in the central business district. The city plans to pay for the structure through parking fees. During a two-month period (44 weekdays), daily fees collected averaged $1,264 with a standard deviation of $150.\n(a) What assumptions must you make in order to use these statistics for inference?\n(b) Write a 90% confidence interval for the mean daily income this parking garage will generate, rounded appropriately.\n(c) The consultant who advised the city on this project predicted that parking revenues would average $1,300 per day. On the basis of your confidence interval, do you think the consultant was correct? Why or why not?\n(d) Give a 90% confidence interval for the total revenue earned during five weekdays.\n40. Suppose that for planning purposes the city in Exercise 39 needs a better estimate of the mean daily income from parking fees.\n(a) Someone suggests that the city use its data to create a 95% confidence interval instead of the 90% interval first created. Would this interval be better for the city planners? (You need not actu­ally create the new interval.)\n(b) How would the 95% interval be worse for the city planners?\n(c) How could city planners achieve an interval esti­mate that would better serve their planning needs?\n(d) How many days’ worth of data must planners collect to have 95% confidence of estimating the true mean to within $10? Does this seem like a reasonable objective? (Use a z-interval to simplify the calculations.)\n41. A sample of 150 calls to a customer help line during one week found that callers were kept waiting on average for 16 minutes with s = 8.\n(a) Find the margin of error for this result if we use a 95% confidence interval for the length of time all customers during this period are kept waiting.\n(b) Interpret for management the margin of error.\n(c) If we only need to be 90% confident, does the confidence interval become longer or shorter?\n(d) Find the 90% confidence interval.\n42. A sample of 300 orders for take-out food at a local pizzeria found that the average cost of an order was $23 with s =$15.\n(a) Find the margin of error for the average cost of an order.\n(b) Interpret for management the margin of error.\n(c) If we need to be 99% confident, does the confidence interval become longer or shorter?\n(d) Find the 99% confidence interval for the average cost of an order.\n43. A book publisher monitors the size of shipments of its textbooks to university bookstores. For a sample of texts used at various schools, the 95% confidence interval for the size of the shipment was 250 ± 45 books. Which, if any, of the following interpretations of this interval is/are correct?\n(a) All shipments are between 205 and 295 books.\n(b) 95% of shipments are between 160 and 340 books.\n(c) The procedure that produced this interval gener­ates ranges that hold the population mean for 95% of samples.\n(d) If we get another sample, then we can be 95% sure that the mean of this second sample is be­tween 160 and 340.\n(e) We can be 95% confident that the range 160 to 340 holds the population mean.\n44. An online retailer promises to deliver orders placed on its Web site within three days. Follow-up calls to randomly selected customers show that a 95% confidence interval for the proportion of all orders that arrive on time is 88% ± 6%. What does this mean? Are the following conclusions correct? Explain.\n(a) Between 82% and 94% of all orders arrive on time.\n(b) 95% of all random samples of customers will show that 88% of orders arrived on time.\n(c) 95% of all random samples of customers will show that 82% to 94% of orders arrived on time.\n(d) We are 95% sure that between 82% and 94% of the orders placed by the customers in this sample arrived on time.\nOn a randomly chosen day, we can be 95% confident that between 82% and 94% of the large volume of orders will arrive on time.\n45. Find the 95% z-interval or t-interval for the indicated parameter.\n(a)   \\(\\mu\\)      \\(\\bar{x} = 152, *s* = 35, *n* = 60\\)\n(b)   \\(\\mu\\)     \\(\\bar{x} = 8, *s* = 75, *n* = 25\\)\n(c)   \\(\\mu\\)      \\(\\hat{p} = 0.5, n = 75\\)\n(d)   \\(\\mu\\)      \\(\\hat{p} = 0.3, n = 23\\)\n46. Show the 95% z-interval or t-interval for the indicated parameter.\n(a)   \\(\\mu\\)     \\(\\bar{x} = -45,s = 80, n = 33\\)\n(b)   \\(\\mu\\)    \\(\\bar{x}=255,s= 16, n = 21\\)\n(c)   \\(p\\)     \\(\\hat{p} = 0.25, n = 48\\)\n(d)   \\(p\\)     \\(\\hat{p} = 0.9, n = 52\\)\n47. Direct mail advertisers send solicitations (junk mail) to thousands of potential customers hoping that some will buy the product. The response rate is usually quite low. Suppose a company wants to test the response to a new flyer and sends it to 1,000 randomly selected people. The company gets orders from 123 of the recipients and decides to do a mass mailing to everyone on its mailing list of 200,000. Create a 95% confidence interval for the percentage of those people who will order something.\n48. Not all junk mail comes from businesses. Internet lore is full of familiar scams, such as the desperate foreigner who needs your help to transfer a large amount of money. The scammer sends out 100,000 messages and gets 15 replies. Can the scam artist make a 95% confidence interval for the proportion of victims out there, or is there a problem with the methods of this chapter?\n49. A package of light bulbs promises an average life of more than 750 hours per bulb. A consumer group did not believe the claim and tested a sample of 40 bulbs. The average lifetime of these 40 bulbs was 740 hours with s = 30 hours. The manufacturer responded that its claim was based on testing hundreds of bulbs.\n(a) If the consumer group and manufacturer both make 95% confidence intervals for the population’s average lifetime, whose will probably be shorter? Can you tell for certain?\n(b) Given the usual sampling assumptions, is there a 95% probability that 740 lies in the 95% confidence interval of the manufacturer?\n(c) Is the manufacturer’s confidence interval more likely to contain the population mean because it is based on a larger sample?\n50. In Fall 2011, the Wall Street Journal reported that 44% of smartphone purchases made by people with incomes between $35,000 to $50,000 were Android phones.16\n16 “Targets Shift in Phone Wars”, Wall Street Journal, October 10, 2011.\n(a) If the story is based on a sample of 200 purchases, then should we conclude that Android phones were getting less than half of the market at that time?\n(b) How large a sample is needed for the 95% confidence interval for the population proportion to exclude 50%?\n51. In a survey of employees, Watson-Wyatt reported that 51% had confidence in the actions of senior management.17 To be 95% confident that at least half of all employees have confidence in senior management, how many would have to be in the survey sample?\n17 The sample size is 12,703.\n52. Fireman’s Fund commissioned an online survey of 1,154 wealthy homeowners to find out what they knew about their insurance coverage.\n(a) When asked whether they knew the replacement value of their home, 63% replied yes. In this case, should Fireman’s Fund conclude that more than half of wealthy homeowners know the value of their homes?\n(b) Round the interval into a form suitable for presentation.\n(c) The results of the survey were accompanied by the statement, “In theory, with probability samples of this size, one could say with 95 percent certainty that the results have a statistical precision of plus or minus 3 percentage points. This online sample was not a probability sample.” What is the point of this comment?\n53. Click fraud has become a major concern as more\nand more companies advertise on the Internet. When Google places an ad for a company with its search results, the company pays a fee to Google each time someone clicks on the link. That’s fine when it’s a person who’s interested in buying a product or service, but not so good when it’s a computer program pretending to be a customer. An analysis of 1,200 clicks coming into a company’s site during a week identified 175 of these clicks as fraudulent. (18)\n18 “Google Faces the slickest Click Fraud Yet,” Forbes.com, January 12, 2010.\n(a) Under what conditions does it make sense to treat these 1,200 clicks as a sample? What would be the population?\n(b) Show the 95% confidence interval for the population proportion of fraudulent clicks in a form suitable for sharing with a nontechnical audience.\n(c) If a company pays Google $4.50 for each click, give a confidence interval (again, to presentation precision) for the mean cost due to fraud per click.\n54. Philanthropic organizations, such as the Gates Foundation, care about the return on their investment. For example, if a foundation spends $7,000 for preschool education of children who are at risk, does society get a reasonable return on this investment? A key component of the social benefit is the reduction in spending for crimes later in life. In the High/Scope Perry Preschool Project, 128 children were randomly assigned to a year of regular education or to an intensive preschool program that cost an additional $10,600 per pupil in 2005. By age 27, those in the preschool program averaged 2.3 fewer crimes (with standard error 0.85) than those.who were not in the preschool program.19\n19 “Going Beyond Head Start,” Business Week, October 23, 2006. Additional data were derived from The Economics of Investing in Universal Preschool Education in California, L. A. Karoly and I. H. Bigelow (2005), Rand.\n(a) Explain why it is important that the children in this study were randomly assigned to the preschool program.\n(b) Does the 95% confidence interval for the reduction in crime in the preschool group include zero? Is this important?\n(c) If the total cost per crime averages $25,000 (in costs for police, courts, and prisons), give a confidence interval for the savings produced by this preschool program using your interval in (b)."
  },
  {
    "objectID": "supplemental/ae-13.html#mix-and-match",
    "href": "supplemental/ae-13.html#mix-and-match",
    "title": "Exam 3 revision - all possible questions",
    "section": "Mix and Match",
    "text": "Mix and Match\nMatch the description of each concept with the correct symbol or term.\n\n\n\n\n\n\n\nConcept\nTerm/Symbol\n\n\n\n\n\nOne-sided null hypothesis\n\nt-statistic\n\n\n\nIdentifies the alternative hypothesis\n\nμ0\n\n\n\nMaximum tolerance for incorrectly rejecting H0\n\np-value\n\n\n\nNumber of standard errors that separate an observed statistic from the boundary of H0\n\np-value < α\n\n\n\nNumber of estimated standard errors that separate an observed statistic from the boundary of H0\n\nType I error\n\n\n\nLargest α-level for which a test rejects the null hypothesis\n\nz-statistic\n\n\n\nOccurs if the p-value is less than α when H0 is true\n\nType II error\n\n\n\nOccurs if the p-value is larger than α when H0 is false\n\nα-level\n\n\n\nSymbol for the largest or smallest mean specified by the null hypothesis\n\nH0: μ ≥ 0\n\n\n\nIndicates a statistically significant result\n\nHa,H1"
  },
  {
    "objectID": "supplemental/ae-13.html#true-or-false",
    "href": "supplemental/ae-13.html#true-or-false",
    "title": "Exam 3 revision - all possible questions",
    "section": "True or False",
    "text": "True or False\nMark each statement True or False. If you believe that a statement is false, briefly explain why you think it is false.\nExercises 11-18. A retailer maintains a Web site that it uses to attract shoppers. The average purchase amount is $80. The retailer is evaluating a new Web site intended to encourage shoppers to spend more. Let μ represent the average amount spent per customer at its redesigned Web site.\n\nThe appropriate null hypothesis for testing the profitability of the new design sets μ0 = $80.\nThe appropriate null hypothesis for testing the profitability of the new design is H0: μ ≤ μ0.\nIf the α-level of the test is α = 0.05, then there is at most a 5% chance of incorrectly rejecting H0.\nIf the p-value of the test of H0 is less than α, then the test has produced a Type II error.\nIf the test used by the retailer rejects H0 with the α-level set to α = 0.05, then it would also reject H0 with α = 0.01.\nThe larger the sample size used to evaluate the new design, the larger the chance for a Type II error.\nIf the standard deviation is estimated from the data, then a z-statistic determines the p-value.\nIf the t-statistic rejects H0, then we would also reject H0 had we obtained the p-value using a normal distribution rather than a t-distribution.\n\nExercises 19-26. An accounting firm is considering offering investment advice in addition to its current focus on tax planning. Its analysis of the costs and benefits of adding this service indicates that it will be profitable if 40% or more of its current customer base use it. The firm pans to survey its customer who will use the service if offered, and let p̂ denote the proportion who say in a survey that they will use this service. The firm does not want to invest in the expansion unless data show that it will be profitable.\n\nIf H0 holds, then p̂ in the sample will be less than 0.4.\nBy setting a small α-level, the accounting firm reduces the chance of a test indicating that it should add this new service even though it is not profitable.\nIf p̂ is larger than 0.4, a test will reject the appropriate null hypothesis for this context.\nThe larger the absolute value of the z-statistic that compares p̂ to p, the smaller the p-value.\nThe mean of the sampling distribution of p̂ that is used to determine whether a statistically significant result has been obtained in this example is 0.4.\nThe standard error of the sampling distribution of p̂ depends on an estimate determined from the survey results.\nThe p-value of the test of the null hypothesis in this example is the probability that the firm should add the investment service.\nLarger samples are more likely than smaller samples to produce a test that incorrectly rejects a true null hypothesis."
  },
  {
    "objectID": "supplemental/ae-13.html#think-about-it-1",
    "href": "supplemental/ae-13.html#think-about-it-1",
    "title": "Exam 3 revision - all possible questions",
    "section": "Think About It",
    "text": "Think About It\n\nA pharmaceutical company is testing a newly developed therapy. If the therapy lower the blood pressure of a patient by more than 10mm, it is deemed effective.16 What are the natural hypotheses to test in a clinical study of this new therapy?\nA chemical firm has been accused of polluting the local river system. State laws require the accuser to prove the polluting by a statistical analysis of water samples. Is the chemical firm worried about a Type I or a Type II error?\nThe research labs of a corporation occasionally produce breakthroughs that can lead to multibillion-dollar blockbuster products. Should the managers of the labs be more worried about Type I or Type II errors?\nModern combinatorial chemistry allows drug researchers to explore millions of products when searching for the net big pharmaceutical drug. An analysis can investigate hundreds of thousands of compounds. Suppose that none of 100,000 compounds in reality produces beneficial results. How many would a compounds-by-compound testing procedure with α = 0.05 nonetheless indicate were effective? Would this cause any problems?\nConsider the following test of whether a coin is fair. Toss the coin three times. If the coin lands either all heads or all tails, reject H0 : p = 1/2. (The p denotes the chance for the coin to land on heads.)\n(a) What is the probability of a Type I error for this procedure?\n(b) If p = 3/4, what is the probability of a Type II error for this procedure? (The null hypothesis remains the same.)\nA consumer interest group buys a brand-name kitchen appliance. During its first moth of use, the appliance breaks down. The manufacturer claims that 99% of its appliances work for a year without a problem.\n(a) State the appropriate null hypothesis that will facilitate a hypothesis test of this claim.\n(b) Do the data supply enough information to reject the null hypothesis? If so, at what α-level?\nA jury of 12 beings with the premise that the accused is innocent. Assume that there 12 jurors were chosen from a large population, such as voters. Unless the jury votes unanimously for conviction, the accused is set free.\n(a) Evidence in the trial of an innocent suspect is enough to convince half of all jurors in the population that the suspect is guilty. What is the probability that a jury convicts an innocent suspect?\n(b) What type of error (Type I or Type II) is committed by the jury in part (a)?\n(c) Evidence in the trial of guilty suspect is enough to convince 95% of all jurors in the population that the suspect is guilty. What is the probability that a jury fails to convict the guilty suspect?\n(d) What type of error is committed by the jury in part (c)?\nTo demonstrate that a planned commercial will be cost effective, at least 60% of those watching the programming need to see the commercial (rather than switching stations or using a digital recorder to skip the commercial). Typically, half of viewers watch the commercials. It is possible to measure the behavior of viewers in 1,000,000 households, but is such a large sample needed?\nThe biostatistician who designed a study for investigating the efficacy of a new medication was fired after the study. The tested null hypothesis states that the drug is no better than a placebo. The t-statistic that was obtained in the study of 400 subjects was t = 20, rejecting the null hypothesis and showing that the drug has a beneficial effect. Why was management upset?\nSuppose that 2% of the modifications proposed to improve browsing on a Web site actually do improve customers’ experience. The other 98% have no effect. Now imagine testing 100 newly proposed modifications. It is quick and easy to measure the shopping behavior of hundreds of customers on a busy Web site, so each test will use a large sample that allows the test to detect real improvements. The tests use independent samples, and the level of significance is α = 0.05.\n(a) Of the 100 tests, how many would you expect to reject the null hypothesis that claims the modifications provides no improvement?\n(b) If the tests that find significant improvement are carefully replicated, how many would you expect to again demonstrate a significant improvement?\n(c) Do these results suggest an explanation for why scientific discoveries often cannot be replicated?\nThe Human Resources (HR) group gives job applicants at a firm a personality test to assess how well they will fit into the firm and get along with colleagues. Historically, test scores have been normally distribution with mean μ and standard deviation σ = 25. The HR group wants to hire applicants whose true personality rating μ is greater than 200 points. (Test scores are an imperfect measure of true personality.)\n(a) Before seeing test result, should the HR group assert as the null hypothesis that μ for an applicants is greater than 200 or less than 200?\n(b) If the HR group chooses H0: μ ≤ 200, then for what test scores (approximately) will the HR group reject H0 if α = 2.5%?\n(c) What is the chance of a Type II error using the procedure in part (b) if the true score of an applicant is 225?\nA test of filtering software examined a sample of n = 100 messages. If the filtering software reduces the level of spam to 15%, this test only has a 33% chance of correctly rejecting H0: p ≥ 0.20. Suppose instead of using 100 messages, the test were to use n = 400.\n(a) In order to obtain a p-value of 0.05, what must be the percentage of spam that gets through the filtering software? (Hint: The z-statistic must be -1.645.)\n(b) With the larger sample size, does p̂ need to be as far below p0 = 0.20 as when n = 100? Explain what threshold moves closer to p0.\n(c) If in fact p = 0.15, what is the probability that the test with n = 400 correctly rejects H0?\n\n16 Historically, blood pressure was measured using a device that monitored the height of a column of mercury in a tube. That’s not used anymore, but the scale remains."
  },
  {
    "objectID": "supplemental/ae-13.html#you-do-it-1",
    "href": "supplemental/ae-13.html#you-do-it-1",
    "title": "Exam 3 revision - all possible questions",
    "section": "You Do It",
    "text": "You Do It\n\nAn appliance manufacturer stockpiles washers and dryers in a large warehouse for shipment to retail stores. Some appliances get damaged in handling. The long-term goal has been to keep the level of damaged machines below 2%. In a recent test, an inspector randomly checked 60 washers and discovered that 5 of them had scratches or dents. Test the null hypothesis H0: p ≤ 0.02 in which p represents the probability of a damaged washer.\n(a) Do these data supply enough evidence to reject H0? Use a binomial model from Chapter 11 to obtain the p-value.\n(b) What assumption is necessary in order to use the binomial model for the count of the number of damaged washers?\n(c) Test H0 by using a normal model for the sampling distribution of p̂. Does this test reject H0?\n(d) Which test procedure should be used to test H0? Explain your choice.\nThe electronic components used to assemble a cellular phone have been exceptionally reliable, with more than 99.9% working correctly. The head of procurement believes that the current supplier meets this standard, but he tests components just the same. A test of a sample of 100 components yielded no defects. Do these data prove beyond reasonable doubt that the components continue to exceed the 99.9% target?\n(a) Identify the relevant population parameter and null hypothesis.\n(b) Explain why it is not appropriate to use a normal approximation for the sampling distribution of the proportion in this situation.\n(c) Determine if the data supply enough evidence to reject the null hypothesis. Identify any assumptions you need for the calculation.\nA company that stocks shelves in supermarkets is considering expanding the supply that it delivers. Items that are not sold must be discarded at the end of the day, so it only wants to schedule additional deliveries if stores regularly sell out. A break-even analysis indicates that an additional delivery cycle will be profitable if items are selling out in more than 60% of markets. A survey during the last week of 45 markets found the shelves bare in 35.\n(a) State the null and alternative hypotheses.\n(b) Describe a Type I error and a Type II error in this context.\n(c) Find the p-value of the test. Do the data supply enough evidence to reject the null hypothesis if the α-level is 0.05?\nField test of a low-calorie sport drink found that 80 of the 100 who tasted the beverage preferred it to the regular higher-calorie drink. A break-even analysis indicates that the launch of this product will be profitable if the beverage is preferred by more than 75% of all customers.\n(a) State the null and alternative hypotheses.\n(b) Describe a Type I error and a Type II error in this context.\n(c) Find the p-value for a test of the null hypothesis. If α = 0.10, does the test reject H0?\nThe management of a chain of hotels avoids intervening in the local management of its franchises unless problems become far too common to ignore. Management believes that solving the problems is better left to the local staff unless the measure of satisfaction drops below 33%. A survey of 80 guests who recently stayed in the franchise in St.Louis found that only 20% of the guests indicated that they would return to that hotel when next visiting the city. Should management intervene in the franchise in St.Louis?\n(a) State the null and alternative hypotheses.\n(b) Describe Type I and Type II errors in the context.\n(c) Find the p-value of the test. Do the data supply enough evidence to reject the null hypothesis if α = 0.025?\nAn importer of electronic goods is considered packaging a new, easy-to-read instruction booklet with DVD players. It wants to package this booklet if it helps customers more than the current booklet. Previous tests found that 30% of customers were able to program their DVD player. An experiment using the new booklet found that 16 out of 60 customers were able to program their DVD player.\n(a) State the null and alternative hypotheses.\n(b) Describe Type I and Type II errors in the context.\n(c) Find the p-value of the test. Do the data supply enough evidence to reject the null hypothesis if α = 0.05?\nA variety of stores offer loyalty programs. Participating shoppers swipe a bar-coded tag at the register when checking out and receive discounts on certain purchases. Stores benefit by gleaning information about shopping habits and hope to encourage shoppers to spend more. A typical Saturday morning shopper who does not participate in the program spends $120 on her or his order. In a sample of 80 shoppers participating in the loyalty program, each shopper spent $130 on average during a recent Saturday, with standard deviation s = $40. (See the histogram below.) Is this statistical proof that the shoppers participating in the loyalty program spend more on average than typical shoppers? (Assume that the data meet the sample size condition.)\n(a) State the null and alternative hypotheses. Describe the parameters.\n(b) Describe Type I and Type II errors in the context.\n(c) How large could the kurtosis be without violating the CLT condition?\n(d) Find the p-value of the test. Do the data supply enough evidence to reject the null hypothesis if α = 0.05?\nBrand managers become concerned if they discover that customers are again and moving out of the high-spending age groups. For example, the average Cadillac buyer is older than 60, past the prime middle years that typically are associated with more spending. Part of the importance to Cadillac of the success of the Escalade model has been its ability to draw in younger customers. If a sample of 50 Escalade purchasers has average age 45 (with standard deviation 25), is this compelling evidence that Escalade buyers are younger on average than the typical Cadillac buyer? (Assume that the data meet the sample size condition.)\n(a) State the null and alternative hypotheses. Describe the parameters.\n(b) Describe Type I (false positive) and Type II (false negative) errors in the context.\n(c) Find the p-value of the test using a normal model for the sampling distribution. Do the data supply enough evidence to reject the null hypothesis if α = 0.025?\nRefer to the analysis of shopper in Exercise 45.\n(a) If several of those participating in the loyalty program are members of the same family, would this cause you to question the assumptions that underlie the test in this question?\n(b) Several outliers were observed in the data for the loyalty program. Should these high-volume purchases be excluded?\nRefer to the analysis of car buyers in Exercise 46.\n(a) Suppose the distribution of the ages of the buyers of the Cadillac Escalade is skewed. Does this affect the use of a normal model as the sampling distribution? How skewed can the data become without preventing used of the t-test?\n(b) The distribution of the ages of these buyers is in fact bimodal, with one group hovering near 30 and the other at 60+. How does the observation affect the test?\nRefer to the analysis of shoppers in Exercise 45. If the population mean spending amount for shoppers in the loyalty program is $135 (with σ = $40), then what is the probability that the test procedure used in this question will fail to reject H0?\nRefer to the analysis of car buyers in Exercise 46. If the population mean age for purchasers of the Cadillac Escalade is 50 years (with σ = 25 years), then what is the probability that the test procedure used in this question will fail to reject H0?\nBanks frequently compete by adding special services that distinguish them from rivals. These services can be expensive to provide. The bank hopes to retain customers who keep high balances in accounts that do not pay large interest rates. Typical customers at this bank keep an average balance of $3,500 in savings accounts that pay 2% monthly interest annually. The bank loans this money to other customers at an average rate of 6%, earning 4% profit on the balance. A sample of 65 customers was offered a special personalized account. After three months, the average balance in savings from these customers was $5,000 (s = $3,000). If the service costs the bank $50 per customer per year, is this going to be profitable to roll out on a larger scale?\n(a) State the null and alternative hypotheses. Describe the parameters.\n(b) Describe Type I and Type II errors in the context.\n(c) What is necessary for the sample size to be adequate for using a t-test?\n(d) Find the p-value of the test. Do the data supply enough evidence to reject the null hypothesis if α = 0.05? (Assume that the data meet the sample size condition.)\nHeadhunters locate candidates to fill vacant senior positions in companies. These placement companies are typically paid a percentage of the salary of the filled position. A placement company that specializes in biostatistics is considering a move into information technology (IT). It earns a fee of 15% of the starting salary for each person it places. Its numerous placements in biostatistics had an average starting salary of $125,000. Its first 50 placements in IT had an average starting salary of $140,000 (s = @$20,000) but produced higher costs at the agency. If each placement in IT has cost the placement company $1,200 more than each placement in biostatistics, should the firm continue its push into the IT industry?\n(a) State the null and alternative hypotheses. Describe the parameters.\n(b) Describe Type I and Type II errors in the context.\n(d) Find the p-value of the test. Do the data supply enough evidence to reject the null hypothesis if α = 0.10? (Assume that the data meet the sample size condition.)"
  },
  {
    "objectID": "supplemental/ae-13.html#mix-and-match-1",
    "href": "supplemental/ae-13.html#mix-and-match-1",
    "title": "Exam 3 revision - all possible questions",
    "section": "Mix and Match",
    "text": "Mix and Match\nMatch each definition on thee left with a mathematical expression or term on the right.\n\nPlot used for visual comparison of results in two (or more) groups (a) t = -4.6\nDifference between the averages in two samples (b) t = 1.3\nDifference between the averages in two populations (c) \\(\\mu_1 - \\mu_2\\)\nName given to the variable that specifies the treatments in an experiment (d) \\(\\sqrt{s_1^2/n_1+s_2^2/n_2}\\)\nEstimate of the standard error of the difference between two sample means (e) n - 1\nAvoids confounding in a two-sample comparison (f) \\(\\bar{x}_1 - \\bar{x}_2\\)\nTest statistic indicating a statistically significant result if \\(\\alpha\\) = 0.05 and \\(H_0\\):\\(\\mu_1 - \\mu_2 \\geq 0\\) (g) Confounding\nTest statistic indicating that a mean difference is not statistically significant if \\(\\alpha\\) = 0.05 (h) Randomization\nThe number of degrees of freedom in a paired t-test (i) Factor\nMultiple factors explain the difference between two samples (j) Comparison boxplot"
  },
  {
    "objectID": "supplemental/ae-13.html#truefalse-1",
    "href": "supplemental/ae-13.html#truefalse-1",
    "title": "Exam 3 revision - all possible questions",
    "section": "True/False",
    "text": "True/False\nMark each statement True or False. If you believe that a statement is false, briefly explain why you think it is false.\n\nThe null hypothesis from a break-even analysis includes the possibility that the methods are equally profitable.\nA one-sided test of \\(H_0\\):\\(\\mu_1 - \\mu_2 \\leq 10\\) rejects \\(H_0\\) if 10 lies outside of the 95% confidence interval for \\(\\mu_1 - \\mu_2\\).\nIf the standard two-sample t-test rejects \\(H_0\\):\\(\\mu_1 - \\mu_2 \\leq\\) $100, then \\(\\mu_1\\) is more than $100 larger than \\(\\mu_2\\).\nIf we double the size of both samples, we reduce the chance that we falsely reject the null hypothesis when using the two-sample t-test.\nIf the two-sample confidence interval for \\(\\mu_1-\\mu_2\\) includes 0, then we should conclude that \\(\\mu_1 = \\mu_2\\).\nThe t-statistic in a two-sample test does not depend on the units of the comparison. (We could, for example, measure the data in dollars or cents.)\nIf the boxplots of the data for the two groups overlap, then the two means are not significantly different.\nIf the confidence interval for \\(\\mu_1\\) does not overlap the confidence interval for \\(\\mu_2\\), then the two means are statistically significantly different.\nIf the two-sample standard deviations are essentially the same (\\(s^2_1 \\approx s^2_2\\)), then the pooled two-sample t-test agrees with the regular two-sample t-test for the difference in the means.\nPooling two samples to estimate a common variance \\(\\sigma^2\\) avoids complications due to confounding."
  },
  {
    "objectID": "supplemental/ae-13.html#think-about-it-2",
    "href": "supplemental/ae-13.html#think-about-it-2",
    "title": "Exam 3 revision - all possible questions",
    "section": "Think About It",
    "text": "Think About It\n\nA busy commuter is concerned about the time she spends in traffic getting to the office. She times the drive for a couple of weeks and finds that it averages 40 minutes. The next day, she tries public transit and it takes 45 minutes. The next day, she’s back on the roads, convinced that driving is quicker. Does her decision make sense?\nIf the busy commuter in Exercise 21 decides to do more testing, how should she decide on the mode of transportation? Should she, for example, drive for a week and then take public transit for a week? What advice would you offer?\nA business offers its employees free membership in a local fitness center. Rather than ask the employees if they like this benefit, the company developed a measure of productivity for each employee based on the number of claims handled. To assess the program, managers measured productivity of staff members both before and after the introduction of this program. How should these data be analyzed in order to judge the effect of the exercise program?\nDoctors tested a new type of contact lens. Volunteers who normally wear contact lenses were given a standard type of lens for one eye and a lens made of the new material for the other. After a month of wear, the volunteers rated the level of perceived comfort for each eye.\n(a) Should the new lens be used for the left or right eye for every patient?\n(b) How should the data on comfort be analyzed?\nMembers of a sales force were randomly assigned to two management groups. Each group employed a different technique for motivating and supporting the sales team. Let’s label these groups A and B, and let \\(\\mu_A\\) and \\(\\mu_A\\) denote the mean weekly sales generated by members of the two groups. The 95% confidence interval for \\(\\mu_A - \\mu_B\\) was found to be [$500, $2,200].\n(a) If profits are 40% of sales, what’s the 95% confidence interval for the difference in profits generated by the two methods?\n(b) Assuming the usual conditions, should we conclude that the approach taken by Group A sells more than that taken by Group B, or can we dismiss the observed difference as due to chance?\n(c) The manager responsible for Group B complained that his group had been assigned the “poor performers” and that the results were not his fault. How would you respond?\nManagement of a company divided its sales force into two groups. The situation is precisely that of the previous exercise, only now a manager decided the assignment of the groups. The 95% confidence interval for \\(\\mu_A - \\mu_B\\) was found to be [$500, $2,200].\n(a) Does the lack of randomized assignment of the two groups suggest that we should be more careful in checking the condition of similar variances?\n(b) The manager responsible for Group B complained that her group had been assigned the poor performers and that the results were not her fault. How would you respond?\nMany advocates for daylight savings time claim that it saves money by reducing energy consumption. Generally, it would be hard to run an experiment to test this claim, but as fortune would have it, counties in Indiana provided an opportunity. Until 2006, only 15 of the 92 counties in Indiana used daylight savings time. The rest had remained on standard time. Now all are on daylight savings time. The local energy provider has access to utility records in counties that recently moved to daylight savings time and counties that did not. How can it exploit these data to test the benefits of daylight savings time?\nManagers of a national retail chain want to test a new advertising program intended to increase the total sales in each store. The new advertising requires moving some display items and making changes in lighting and decoration. The changes can be done overnight in a store. How would you recommend they choose the stores in which to place the advertising? Would data from stores that did not get the new advertising be useful as well?\nWine. The recommendations of respected wine critics such as Robert M. Parker Jr. have a substantial effect on the price of wine. Vintages that earn higher ratings command higher prices and spark surges in demand. These data are a sample of ratings of wines selected from an online Web site from the 2000 and 2001 vintages.\n(a) Do the ratings meet the conditions needed for a two-sample confidence interval?\n(b) Find the 95% confidence interval for the difference in average ratings for these two vintages. Does one of the years look better than the other?\n(c) Suggest one or two possible sources of confounding that might distort the confidence interval reported in part (b).\n(d) Round the endpoints of the interval as needed for presenting it as part of a message.\n(e) Given a choice between a 2000 and 2001 vintage at the same price, which produces higher ratings? Does it matter?\nDexterity. A factory hiring people for tasks on its assembly line gives applicants a test of manual dexterity. This test counts how many oddly shaped parts the applicant can install on a model engine in a one-minute period. Assume that these tested applicants represent simple random samples of men and women who apply for these jobs.\n(a) Find 95% confidence intervals for the expected number of parts that men and women can install during a one-minute period\n(b) These data are counts, and hence cannot be negative or fractions. How can we use the normal model in this situation?\n(c) Your intervals in part (a) should overlap. What does it mean that the intervals overlap?\n(d) Find the 95% confidence interval for the difference \\(\\mu_{men} - \\mu_{women}\\).\n(e) Does the interval found in part (d) suggest a different conclusion about \\(\\mu_{men} - \\mu_{women}\\) than the use of two separate intervals?\n(f) Which procedure is the right one to use if we’re interested in making an inference about \\(\\mu_{men} - \\mu_{women}\\)?\nUsed Cars. These data indicate the prices of 155 used BMW cars. Some have four-wheel drive (the model identified by the Xi type) and others two-wheel drive (the model denoted simply by the letter i).\n(a) If we treat the data as samples of the typical selling prices of these models, what do you conclude? Do four-wheel drive models command a higher price as used cars, or are differences in average price between these samples typical of sampling variation?\n(b) These cars were not randomized to the two groups. We also know that newer cars sell for more than older cars. Has this effect distorted through confounding the confidence interval in part (a)?\nRetail Sales. These data give the sales volume (in dollars per square foot) for 86 retail outlets specializing in women’s clothing in 2015 and 2016. Did sales change by a statistically significant amount from 2015 to 2016?\nBased on data collected in 2010, 23% of women wanted to buy a Google Android smart phone or tablet. Among men, 33% were interested in an Android purchase next. The survey contacted 240 women and 265 men.(a) Might the comparison between these two samples be confounded with a hidden factor? If so, identify such a factor.\n(b) Find the 95% confidence interval for the difference between the proportion of men and the proportion women interested in buying an Android device.\n(c) What are the implications of the confidence interval for sales executives at Google?\nInternet search engines report many hits, but getting the link to your site on the first page is critical for a business. A study surveyed 1,232 people from a U.S. online consumer panel in 2002 and independently surveyed a second panel of 1,137 in 2009. Analysts found that 62% of those surveyed in 2009 clicked on a result on the first page, up from 48% in 2002.\n(a) Might the comparison between these two samples be confounded with a hidden factor? If so, identify such a factor.\n(b) Find the 95% confidence interval for the differ ence between the proportion of users who click on a link on the first page of the search results in the two surveys.\n(c) What do the results tell businesses about the importance of improving their search rank?\nDecoy. A retail chain is considering installing devices that resemble cameras to deter shoplifting. The devices only look like cameras, saving the expense of wiring and recording video. To test the benefit of this decoy system, it picked 40 stores, with half to get the decoy and the other half to serve as comparison group (control group). Stores were matched based on typical levels of sales, local market size, and demographics. The comparison lasted for 3 months during the summer. At the end of the period, the retailer used its inventory system to compute the amounts lost to theft in the stores.\n(a) Why is it important that all of the stores measure theft during the same time period?\n(b) Compute separate 95% confidence intervals for the amount lost to theft with and without the decoy cameras. Is there evidence of a statistically significant difference?\n(c) Perform the appropriate comparison of the decoy system versus stores without this potential deterrent. Is there a statistically significant difference in the average amount lost to theft? Be sure to check the conditions for the method used.\nDistance. A golf equipment manufacturer would like to convince members of a club that its golf balls travel farther than those of a competitor, even for weekend golfers. For the comparison, 12 golfers were randomly selected from the club to each drive a ball from this manufacturer and from the rival. The results show the distance traveled, in yards.\n(a) Should every golfer hit the rival ball first and then the manufacturer’s ball second?\n(b) Is there a statistically significant difference in the average distance traveled between the two types of balls? Be sure to check the conditions for the method used.\n(c) Rather than have each golfer hit one ball of each type, the manufacturer could have easily gotten 12 more golfers and allowed all 24 to hit one ball. Should the manufacturer run the study as it did, or with a larger number of golfers hitting just one ball?\nDamage. A cosmetics firm uses two different shipping companies. Shipper A is more expensive, but managers believe that fewer shipments get damaged by Shipper A than with the firm’s the current shipper, Shipper B. To compare the shippers, the company devised the following comparison. The next 450 shipments to outlet stores were randomly assigned to Shipper A or to Shipper B. Upon receipt, an agent at the store reported whether the shipment had suffered noticeable damages, Because shipping with Shipper A is more expensive, a financial calculation showed that Shipper A needs to have at least 10% fewer damaged shipments than Shipper B for Shipper A to improve profits.\n(a) How would you randomly assign the shipments to Shipper A or to Shipper B?\n(b) Do the data show that shipping with Shipper A exceeds the needed threshold by a statistically significant amount?\n(c) Is there a statistically significant difference between the damage rates?\nRetention. High turnover of employees is expensive for firms. The firm not only loses experienced employees, it must also hire and train replacements. A firm is considering several ways to improve its retention (the proportion of employees who continue with the firm after 2 years). The currently favored approach is to offer more vacation days. Improved health benefits are a second alternative, but the high cost of health benefits implies that to be effective this benefit must increase retention by at least 0.05 above that associated with offering increased vacation days. To choose between these, a sample of 125 employees in the Midwest was given increased health benefits, and a sample of 140 on the East Coast was offered increased vacation time.\n(a) What are potential confounding effects in this comparison?\n(b) Do the data indicate that offering health benefits has statistically significantly higher retention to compensate for switching to health benefits?\n(c) Is there a statistically significant difference in retention rates between the benefit plans?"
  },
  {
    "objectID": "supplemental/ae-13.html#truefalse-2",
    "href": "supplemental/ae-13.html#truefalse-2",
    "title": "Exam 3 revision - all possible questions",
    "section": "True/False",
    "text": "True/False\nIf you believe that a statement is false, briefly explain why you think it is false.\n\nThe chi-squared test of independence only detects linear association between the variables in a contingency table.\nA statistically significant \\(\\chi^2\\) in the test of independence implies a causal relationship between the variables that define a contingency table.\nThe expected size of the chi-squared statistic \\(\\chi^2\\) increases with the number of observations \\(n\\) in the table.\nTables with more rows and columns on average produce larger values of \\(\\chi^2\\) than tables with fewer rows and columns when the null hypothesis of independence is true.\nThe smallest possible value of \\(\\chi^2\\) is zero.\nSmall p-values of the chi-squared test of independence indicate that the data have small amounts of dependence.\nSmall values of \\(\\chi^2\\) imply that the null hypothesis in a chi-squared test is true.\nThe chi-squared test of independence requires at least 5 to 10 observations in each cell of the contingency table.\nThe chi-squared test of goodness of fit finds the expected cell counts based on the distribution of a random variable.\nSimilar proportions within each column of a plot suggest a small p-value in the chi-squared test of independence."
  },
  {
    "objectID": "supplemental/ae-13.html#think-about-it-3",
    "href": "supplemental/ae-13.html#think-about-it-3",
    "title": "Exam 3 revision - all possible questions",
    "section": "Think About It",
    "text": "Think About It\n\nWithout doing all the calculations, which of these 2 × 2 tables has the largest value of chi-squared? The smallest? (Each table has n = 100.)\n\n\n\n\n(c)\n\n\n\n\n\n30\n20\n\n\n20\n30\n\n\n\n\n\n\n(b)\n\n\n\n\n\n50\n0\n\n\n0\n50\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n25\n25\n\n\n25\n25\n\n\n\n\nWithout doing all the calculations, which of these 3 × 2 tables has the largest value of chi-squared? The smallest? (Each table has n = 150.)\n\n\n\n\n(a)\n\n\n\n\n\n8\n42\n\n\n37\n13\n\n\n18\n32\n\n\n\n\n\n\n(b)\n\n\n\n\n\n23\n27\n\n\n27\n23\n\n\n25\n25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n26\n24\n\n\n19\n31\n\n\n31\n19\n\n\n\n\na. Does this stacked bar chart suggest that the chi-squared test of independence will be statistically significant? Explain why or why not.\n\nb. What are the degrees of freedom in the chi-squared test of independence for these data?\nc. What would a mosaic plot show you that you cannot tell from the stacked bar chart?\na. Does this stacked bar chart suggest that the chi-squared test of independence will be statistically significant? Explain why or why not.\n\nb. What are the degrees of freedom in the chi-squared test of independence for these data?\nc. Should seeing a mosaic plot of these data possibly change your answer to “a”?\nThis is a bar chart of the number of defaults on mortgages in a collection of loans during the credit crisis that began in 2007–2008. The bar chart identifies the type of income documentation used when the mortgage was obtained. Before we test whether loans with full documentation are riskier than the others, what else do we need to know?\n\nThe following bar chart shows the number of major and minor league baseball players that were suspended for using steroids during the 2008 season. Can we use these counts to test whether the rate of steroid use is higher in some positions than others?\n\nA stock market analyst recorded the number of stocks that went up or went down each day for 5 consecutive days, producing a contingency table with two rows (up or down) and five columns (Monday through Friday). Are these data suitable for applying the chi-squared test of independence ?\nA market research firm gathered a random sample of 150 customers. The firm randomly assigned 50 of these to rate the current product and assigned 50 each to rate new versions of the product. The firm then formed a contingency table of Version (old, new A, new B) versus Rating (like, dislike, ambivalent). Are these data suitable for using the chi-squared test of independence?\nA bank makes loans to many types of customers. Some of these customers default on their loans. How could analysts at the bank use \\(\\chi^2\\) to identify characteristics associated with customers who default on loans?\nInventory counts are seldom perfect because of misplaced items or misfiled sales reports. How could managers use \\(\\chi^2\\) to decide whether expected inventory counts differ substantially from the actual counts?\nManagers in the human resources department suspect that sick-day absentee rates are higher on some weekdays than others. What test can they use to investigate this claim?\nA manager in the previous question thinks that the absentee rate is the same on Monday and Friday, but different from the rate on Tuesday through Thursday. What method should she use to test her suspicion?\nThe following contingency table breaks down a month of customer complaints received by a retailer. Some of those complaining are long-term customers, whereas the rest are recent. There are four types of complaints.\n\n\na\\. What would it mean to the retailer if customer status and complaint type were dependent?\n\nb\\. If many of these complaints come from the same irate customer, do the assumptions needed for a chi-squared test hold?\n\nc\\. Does it appear from a brief inspection that customer status and the type of complaint are independent or dependent? Don't calculate $\\chi^2$ ; just skim the table.\n\nd\\. If every count in the table increased by a factor of 2, how would the value of $\\chi^2$ change (for example, if the first cell were to hold 84 rather than 42)? As in (b), think rather than calculate.\n\nA mobile phone service provider randomly samples customers each year to measure current satisfaction with the service provided. The following table summarizes a portion of the survey, with 100 customers sampled each year. Customers are labelled “very satisfied” if they rate their service as 8, 9, or 10 on a 10-point scale. Those who rate the service 5, 6, or 7 are labelled “satisfied.\n\n a. The rest are labelled “unsatisfied.”Would the phone provider prefer these counts to be dependent or independent?\nb\\. The survey includes 100 customers each year, fixing the column totals. Do such fixed margins violate the assumptions of the chi-squared test?\n\nc\\. Does it appear that the level of satisfaction and year of the survey are independent or dependent? Don't calculate $\\chi^2$; just skim the table.\n\nd\\. Suppose that the values shown in the table were column percentages rather than counts, with 250 surveyed each year. How would $\\chi^2$ change?"
  },
  {
    "objectID": "supplemental/ae-13.html#you-do-it-2",
    "href": "supplemental/ae-13.html#you-do-it-2",
    "title": "Exam 3 revision - all possible questions",
    "section": "You Do It",
    "text": "You Do It\nIn each of the next six exercises, answer the question about the small contingency table shown with each exercise.\n\nThese data count the number of male and female shoppers who accept or reject a discounted offer in return for supplying a retailer with an email address. Consider using these data to test the null hypothesis that gender and acceptance are independent (H0). Find the\n\n\na\\. Degrees of freedom of $\\chi^2$.\n\nb\\. Expected number of women who reject if H~0~ holds\n\nc\\. Value of $\\chi^2$ for testing H~0~.\n\nd\\. p-value for testing H~0~.\n\nThese data count the types of meals ordered by customers at two restaurants in a national chain. Consider using these data to test the null hypothesis that store and choice are independent (H0). Find the\n\n\na\\. Degrees of freedom of $\\chi^2$.\n\nb\\. Expected number of customers who order beef at restaurant 1, assuming H~0~ holds\n\nc\\. Value of $\\chi^2$ for testing H~0~.\n\nd\\. p-value for testing H~0~.\n\nThe following table shows counts of the number of days that employees were absent from a small manufacturing center. The null hypothesis is that the counts are uniformly distributed over the days of the week. Find the\n\n\na\\. Degrees of freedom of $\\chi^2$.\n\nb\\. Expected number of days absent on Wednesday, if H~0~ holds\n\nc\\. Value of $\\chi^2$ for testing H~0~.\n\nd\\. p-value for testing H~0~.\n\nThe table shown with this question counts the number of calls that arrive at a telephone help desk during the hours of 1 to 3 p.m. on 5 weekdays. The company uses the same number of employees to staff the center for each of these days. Test the null hypothesis that the calls are uniformly distributed over these days, agreeing with the staffing policy. Find the\n\n\n a. Degrees of freedom of \\(\\chi^2\\).\nb\\. Expected number of calls that arrive on Friday, if H~0~ holds\n\nc\\. $\\chi^2$\n\nd\\. p-value for testing H~0~.\n\nThe human resources group regularly interviews prospective clerical employees and tests their skill at data entry. The following table shows the number of errors made by 60 prospective clerks when entering a form with 80 numbers. (Five clerks made four errors.) Test the null hypothesis that the number of errors follows a Poisson distribution. Find the\n\n\n\nerrors\nClerks\n\n\n\n\n0\n12\n\n\n1\n20\n\n\n2\n9\n\n\n3\n14\n\n\n4 or more\n5\n\n\n\na. Rate (or mean) of the Poisson distribution\nb. Expected number of employees who do not make an error\nc. Degrees of freedom of \\(\\chi^2\\)\nd. \\(\\chi^2\\)\ne. p-value for testing H0.\nA type of new car is offered for sale with four option packages. A customer can buy any number of these, from none to all four. A manager proposes the null hypothesis that customers pick packages at random, implying the number of packages bought by a customer should be binomial with n = 4. This table shows the number of packages chosen by 400 customers. Find the:\n\n\na\\. Binomial parameter p needed to compute the expected counts\n\nb\\. Estimated probability that a customer picks one option.\n\nc\\. Degrees of freedom of $\\chi^2$\n\nd\\. $\\chi^2$\n\ne\\. p-value for testing H~0~.\n\nRefer to the data shown in Question 33.\na. Compute the value of x2 and the p-value for the test of the null hypothesis of independence. b. Summarize the results of this test for the retailer’s sales manager.\nRefer to the data shown in Question 34.\na. Compute the value of x2 and the p-value for the test of the null hypothesis of independence.\nb. Summarize the results of this test for the mobile phone service provider.\nWe routinely use the normal quantile plot to check for normality. One can also use a chi-squared test. For that, we have to group the data into bins, converting numerical data into categorical data. The following figure shows the normal quantile plot of daily stock returns in 2010 on the value-weighted total U.S. market index. The following table counts the number of returns falling into eight intervals. The table includes the count expected under the assumption that these data are normally distributed, using the sample mean x¯=0.0009874 with SD = 0.0151.\n\n a. What does the normal quantile plot indicate about the distribution of returns?\n\n\nThe table groups all returns that are less than − 0.03 and more than 0.03. Why not use more categories to separate very high or low returns? Compute the chi-squared test of goodness of fit and its p-value, noting that we have to estimate two parameters from the data in order to find the expected counts.\nDoes the chi-squared test agree with the normal quantile plot?\nWhat’s the advantage of using a normal quantile plot to check for normality? The advantage of using the chi-squared test?\n\n\n\nThe following table summarizes whether the stock market went up or down during each trading day of 2010.\n\n\na\\. Use a chi-squared test to determine if these data indicate that trading on some days is better or worse (more or less likely to earn positive returns) than any other.\n\nb\\. How does the test used in (a) differ from comparing the proportion positive for each day with 0.5?\n\nc\\. These data only indicate the direction of the market. How does that limit the conclusions we might draw?\n\nThe following table from the U.S. Golfing Association summarizes the performance of players in the 2015 U.S. Open golf tournament. The table shows the number of holes in each round completed in fewer than the allowed number of strokes (birdies, which includes eagles), at the allowed number of strokes (par), with one extra stroke (bogey), or with two extra (double bogey). For example, in the first round players scored par on 1,718 holes. Only players who score among the lower half after two rounds participate in the final two rounds.\n\n\na\\. What would it mean to find a statistically significant value of x^2^ for this table? Interpret your answer in the context of the tournament.\n\nb\\. What is the impact of dropping the higher scoring players from the later rounds on the assumptions and interpretation of x^2^?\n\nc\\. Do these data meet the conditions required by the chi-squared test of independence?\n\nd\\. Regardless of your answer to (c) compute x^2^ with a p-value. How would you interpret these values, given your answer to (c)?"
  },
  {
    "objectID": "supplemental/ae-13.html#mix-and-match-2",
    "href": "supplemental/ae-13.html#mix-and-match-2",
    "title": "Exam 3 revision - all possible questions",
    "section": "Mix and Match",
    "text": "Mix and Match\nMatch the description of each concept with the correct symbol or term.\n\n\n\n\n\n\n\nConcept\nTerm/Symbol\n\n\n\n\n\nSymbol for the explanatory variable in a regression\n\n\\(r^2\\)\n\n\n\nSymbol for the response in a regression\n\nb0\n\n\n\nFitted value from an estimated regression equation\n\nȳ\n\n\n\nResidsual from an estimated regression equation\n\nb1\n\n\n\nIdentifies the intercept in a fitted line\n\nX\n\n\n\nIdentifies the slope in a fitted line\n\nŷ\n\n\n\nPercentage variation described by a fitted line\n\nb0 + b1\n\n\n\nSymbol for the standard deviation of the residuals\n\nY\n\n\n\nPrediction from a fitted line is x = x̄\n\ny - ŷ\n\n\n\nPrediction from a fitted line is x = 1\n\nse"
  },
  {
    "objectID": "supplemental/ae-13.html#true-or-false-1",
    "href": "supplemental/ae-13.html#true-or-false-1",
    "title": "Exam 3 revision - all possible questions",
    "section": "True or False",
    "text": "True or False\nMark each statement True or False. If you believe that a statement is false, briefly explain why you think it is false.\n\n\nIn a scatterplot, the response is shown on the horizontal axis with the explanatory variable on the vertical axis.\nRegression analysis requires several values of the response for each value of the predictor so that we can calculate averages for each \\(x\\).\nIf all of the data lie along a single line with nonzero slope, then the \\(r^2\\) of the regression is 1. (Assume the values of the explanatory variable are not identical.)\nIf the correlation between the explanatory variable and the response is zero, then the slope will also be zero.\nThe use of a linear equation to describe the association between price and sales implies that we expect equal differences in sales when comparing periods with prices $10 and $11 and periods with prices $20 and $21.\nThe linear equation (estimated from a sequence of daily observations)\nEstimated Shipments = b0 + 0.9 Orders Processed\nimplies that we expect twice as many shipments when the number of orders processed doubles only if b0 = 0.\nThe intercept estimates how much the response changes on average with changes in the predictor.\nThe estimated value \\(\\hat{y} = b_0 + b_1\\) approximates the average value of the response when the explanatory variable equals \\(x\\).\nThe horizontal distance between \\(y\\) and \\(\\hat{y}\\) is known as the residual and so takes its scale from the predictor.\nThe sum of the fitted value \\(\\hat{y}\\) plus the residual \\(e\\) is equal to the original data value \\(y\\).\nThe plot of the residuals on the predictor should show a linear pattern, with the data packed along a diagonal line.\nRegression predictions become less reliable as we extrapolate farther from the observed data."
  },
  {
    "objectID": "supplemental/ae-13.html#think-about-it-4",
    "href": "supplemental/ae-13.html#think-about-it-4",
    "title": "Exam 3 revision - all possible questions",
    "section": "Think About It",
    "text": "Think About It\n\nIf the correlation between X and Y is r = 0.5, do X and Y share half of their variation in common?\nThe value of \\(r^2 = 1\\) if data lie along a simple line. Is it possible to fit a linear regression for which \\(r^2\\) is exactly equal to zero?\nIn general, is the linear least squares regression equation of Y on X the same as the equation for regressing X on Y?\nIn what special case does the regression of Y on X match the regression of X on Y, so that if two fitted lines were drawn on the same plot, the lines would coincide?\nFrom looking at this plot of the residuals from a linear equation, estimate the value of \\(s_e\\).\nThis histogram summarizes residuals from a fit that regresses the number of items produced by 50 employees during a shift on the number of years with the company. Estimate \\(s_e\\) from this plot. \nA package delivery service uses a regression equation to estimate the fuel costs of its trucks based on the number of mile driven. The equation is of the form \\(\\text{Estimated Dollars} = b_0 + b_1\\) Miles. If gasoline prices go up, how would you expect the fit of this equation to change?\nA customized milling operation uses the equation $200 plus $150 per hours to give price estimates to customers. If it pays a fixed fee to ship these orders, how should it change this equation if the cost of shipping goes up?\nIf the standard deviation of X matches the standard deviation of Y, then what is the relationship between the slope in a least squares regression of Y on X and the correlation between X and Y?\nIf the correlation between X and Y is 0.8 and the slope in the regression of Y on X is 1.5, then which of X or Y has larger variation?\nShoppers at a local supermarket spend, on average, $85 during each shopping trip. Imagine the scatterplot that shows the total amount spent each day in the store on the y-axis versus the number of shoppers each day on the x-axis.\n(a) Would you expect a linear equation to describe these data?\n(b) What would you expect for the intercept of the linear model?\n(c) What would you expect for the slope?\n(d) Do you expect patterns in the variation around the equation?\nCosts for building a new elementary school in the United States average about $100 per square foot. In a review of school construction projects in Arizona, the head of the Department of Education examined a scatterplot of the cost of recently completed schools (Y) versus the size of the school (in square feet, X).\n(a) Would you expect a linear equation to describe these data?\n(b) What would you expect for the intercept of the linear model?\n(c) What would you expect for the slope?\n(d) Do you expect patterns in the variation around the equation?\nA division of a multinational retail company prepared a presentation to give at the home office in Paris, France. The presentation includes a scatterplot that shows the relationship between square footage and annual sales in retail outlets owned by the chain. The units in the plot show the size in thousands of square feet and the response in thousands of dollars. A fitted line in the plot is \\(y = 47 + 650\\cdot x\\).\n(a) Interpret the slope and intercept in the fitted line.\n(b) To present the model in Europe, the plot must be shown with sales denominated in euros rather than dollars ( use the exchange rate $1 = €0.82) and size given in square meters rather than square feet ( 1 square foot = 0.093 square meter). Find the slope and intercept in these new units.\n(c) Would the \\(r^2\\) summary attached to the regression model change along with the slope and intercept when the data are changed into euros and meters?\n(d) Would \\(s_e\\) change with the new scales?\nAn assembly plant tracks the daily productivity of the workers on the line. Each day, for every employee, the plant records the number of hours put in (Hours) and the number of completed packages assembled by the employee (Count). A scatterplot of the data for one day shows a linear trend. A fitted line with the equation\nEstimated Count = - 2 + 15 Hours\nsummarizes this trend\n(a) Interpret the slope and intercept in the fitted line.\n(b) A carton holds 12 packages. A working day at this plant has 8 hours. Describe the regression line if the data were converted to cartons produced (Y) and days (or fraction of a day) worked.\n(c) Would the \\(r^2\\) summary attached to the regression model change along with the slope and intercept when the data are converted to·cartons and days?\n(d) What about the value of \\(s_e\\)? Would it change with the new scales?"
  },
  {
    "objectID": "supplemental/ae-13.html#you-do-it-3",
    "href": "supplemental/ae-13.html#you-do-it-3",
    "title": "Exam 3 revision - all possible questions",
    "section": "You Do It",
    "text": "You Do It\nThe name shown with each question identifies the data table to be used for the problem.\n\nDiamond Rings This data table contains the listed prices and weights of the diamonds in 48 rings offered for sale in The Singapore Times. The prices are in Singapore dollars, with the weights in carats.\n(a) Scatterplot the listed prices of the rings on the weights of the rings. Does the trend in the average price seem linear?\n(b)Estimate the linear equation using least squares. Interpret the fitted intercept and slope. Be sure to include their units. Note if either estimate represents a large extrapolation and is consequently not reliable.\n(c) Interpret the summary values \\(r^2\\) and \\(s_e\\) associated with the fitted equation. Attach units to these summary statistics as appropriate.\n(d) What is the estimated difference in price (on average) between diamond rings with diamonds that weigh 0.25 and 0.35 carat?\n(e) The slope in this regression is a lot larger than the slope for the emerald diamonds discussed in this chapter. Or is it? Notice that one Singapore dollar is currently worth about $0.65 and convert the slope to an analysis in U.S. dollars.\n(f) These are rings, not diamonds. How would you expect the cost of the setting to affect the linear equation between weight and price?\n(g) A ring with a 0.18-carat diamond lists for $;325 Singapore. Is this a bargain?\n(h) Plot the residuals from this regression. If appropriate, summarize these by giving the mean and standard deviation of the collection of residuals. What does the standard deviation of the residuals tell you about the fit of this equation?\nConvenience Shopping It’s rare that you’ll find a gas station these days that only sells gas. It’s become more common to find a convenience store that also sells gas. These data describe the sales over time at a franchise outlet of a major U.S. oil company. Each row summarizes sales for one day. This particular station sells gas, and it also has a convenience store and a car wash. The column labeled Sales gives the dollar sales of the convenience store, and the column Volume gives the number of gallons of gas sold\n(a) Scatterplot Sales on Volume. Does there appear to be a linear pattern that relates these two sequences?\n(b) Estimate the linear equation using least squares. Interpret the fitted intercept and slope. Be sure to include their units. Note if either estimate represents a large extrapolation and is consequently not reliable.\n(c) Interpret the summary values \\(r^2\\) and \\(s_e\\) associated with the fitted equation. Attach units to these summary statistics as appropriate.\n(d) Estimate the difference ’in sales at the convenience store (on average) between a day with 3,500 gallons sold and a day with 4,000 gallons sold.\n(e) This company also operates franchises in Canada. At those operations, gas sales are tracked in liters and sales in Canadian dollars. What would your equation look like if measured in these other units? (Note: 1 gallon = 3.7854 liters, and use the exchange rate $1 = $1.1 Canadian.) Include \\(r^2\\) and \\(s_e\\) as well as the slope and intercept.\n(f) The form of the equation suggests that selling more gas produces increases in sales at the associated store. Does this mean that customers come to the station to buy gas and then happen to buy something at the convenience store, or might the causation work in the other direction?\n(g) On one day, the station sold 4,165 gallons of gas and had sales of $1,744 at the attached convenience store. Find the residual for this case. Are these sales higher or lower than you would expect?\n(h) Plot the residuals from this regression. If appropriate, summarize these by giving the mean and SD of the collection of residuals. What does the SD of the residuals tell you about the fit of this equation?\nDownload Before taking the plunge into videoconferencing, a company ran tests of its current internal computer network. The goal of the tests was to measure how rapidly data moved through the network given the current demand on the network. Eighty files ranging in size from 20 to 100 megabytes (MB) were transmitted over the network at various times of day, and the time to send the files (in seconds) recorded.\n(a) Create a scatterplot of Transfer Time on File Size. Does a line seem to you to be a good summary of the association between these variables?\n(b) Estimate the least squares linear equation for Transfer Time on File Size. Interpret the fitted intercept and slope. Be sure to include their units. Note if either estimate represents a large extrapolation and is consequently not reliable.\n(c) Interpret the summary values \\(r^2\\) and \\(s_e\\) associated with the fitted equation. Attach units to these summary statistics as appropriate.\n(d) To make the system look more impressive (i.e., have smaller slope and intercept), a colleague changed the units of Y to minutes and the units of X to kilobytes ( 1 MB = 1,024 kilobytes). What does the new equation look like? Does it fit the data any better than the equation obtained in part (b)?\n(e) Plot the residuals from the regression fit in part (b) on the sizes of the files. Does this plot suggest that the residuals reveal patterns in the residual variation? ·\n(f) Given a goal of getting data transferred in no more than 15 seconds, how many data do you think can typically be transmitted in this length of time? Would the equation provided in part (b) be useful, or can you offer a better approach?\nProduction Costs A manufacturer produces custom metal blanks that are used by its customers for computer-aided machining. The customer sends a design via computer (a 3-D blueprint), and the manufacturer comes up with an estimated cost per unit, which is then used to determine a price for the customer. This analysis considers the factors that affect the cost to manufacture these blanks. The data for the analysis were sampled from the accounting records of 195 previous orders that were filled during the last 3 months.\n(a) Create a scatterplot for the average cost per item on the material cost per item. Do you find a linear pattern?\n(b) Estimate the linear equation using least squares. Interpret the fitted intercept and slope. Be sure to include their units. Note if either estimate represents a large extrapolation and is consequently not reliable.\n(c) Interpret the summary values \\(r^2\\) and \\(s_e\\) associated with the fitted equation. Attach units to these summary statistics as appropriate.\n(d) What is the estimated increase in the average cost per finished item if the material cost per unit goes up by $3?\n(e) One can argue that the slope in this regression should be 1, but it’s not. Explain the difference.\n(f) The average cost of an order in these data was $61.16 per unit with material costs of $4.18 per unit. Is this a relatively expensive order given the material costs?\n(g) Plot the residuals from this regression. If appropriate, summarize these by giving the mean and standard deviation of the collection of residuals. What does the standard deviation of the residuals tell about the fit of this equation?\nSeattle Homes This data table contains the listed prices and the number of square feet for 112 homes listed by an on-line realtor in the Seattle area.\n(a) Create a scatterplot for the price of the home on the number of square feet. Does the trend in the average price seem linear?\n(b) Estimate the linear equation using least squares. Interpret the fitted intercept and slope. Be sure to include their units. Note if either estimate represents a large extrapolation and is consequently not reliable.\n(c) Interpret the summary values \\(r^2\\) and \\(s_e\\) associated with the fitted equation. Attach units to these summary statistics as appropriate.\n(d) If a homeowner adds an extra room with 500 square feet to her home, can we use this model to estimate the increase in the value of the home?\n(e) A home with 910 square feet lists for $635,000. What is the residual for this case? Is it a good deal?\n(f) Do the residuals from this regression show patterns? Does it make sense-to interprets, as the standard deviation of the errors of the fit? Use the plot of the residuals on the number of square feet to help decide.\nLeases This data table includes the annual prices of 223 commercial leases. All of these leases provide office space in a Midwestern city in the United States.\n(a) Create a scatterplot for the annual cost of the leases on the number of square feet of leased space. Does the pattern in the plot seem linear? Does the White Space Rule hint of possible problems?8\n(b) Estimate the linear equation using least squares. Interpret the fitted intercept and slope. Be sure to include theirunits. Note if either estimate represents a large extrapolation and is consequently not reliable.\n(c) Interpret the summary values \\(r^2\\) and \\(s_e\\) associated with the fitted equation. Attach units to these summary statistics as appropriate.\n(d) If a business decides to expand and wants to lease an additional 1,000 square feet beyond its current lease, explain how it can use the equation obtained in part (b) to estimate the increment in the cost of its lease. Would this estimate be reliable?\n(e) A row in the data table describes a lease for 32,303 square feet. The annual cost for this lease is $496,409. What is the residual for this case? Is it a good deal?\n(f) Do the residuals from this regression show any patterns? Does it make sense to interpret \\(s_e\\), as the standard deviation of the errors of the fit? Use the plot of the residuals on the number of square feet to decide.\n8The White Space Rule was covered in Chapter 4. A plot that is mostly white space doesn’t reveal much about the data. A good plot uses its space to show data, not empty space.\nR&D Expenses This data file describes 409 companies operating in the semiconductor industry in 2014. One column gives the expenses on research and development (R&D), and another gives the total assets of the companies. Both columns are reported in millions of dollars.\n(a) Scatterplot R&D Expense on Assets. Does a line seem to you to be a good summary of the relationship between these two variables? Describe the outlying companies.\n(b) Estimate the least squares linear equation for R&D Expense on Assets. Interpret the fitted intercept and slope. Be sure to include their units. Is either estimate a large extrapolation and consequently not reliable?\n(c) Interpret the summary values \\(r^2\\) and \\(s_e\\), associated with the fitted equation. Attach units to these summary statistics as appropriate. Does the value of \\(r^2\\) seem fair to you as a characterization of how well the equation summarizes the association?\n(d) Inspect the histograms of the \\(x\\) and y-variables in this regression. Do the shapes of these histograms anticipate some aspects of the scatterplot and the linear relationship between these variables?\n(e) Plot the residuals from this regression. Does this plot reveal patterns in the residuals? Does \\(s_e\\) provide an adequate summary of the residual variation?\nCars The cases that make up this data set are types of cars. The data include the engine size (in liters) and horsepower (HP) of 311 vehicles sold in the United States in 2016. These measurements were produced by the U.S. Environmental Protection Agency as part of its mandate to monitor vehicle fuel efficiency.\n(a) Create a scatterplot of the horsepower on the engine displacement of the car. Describe the association between these variables.\n(b) Estimate the linear equation using least squares. Interpret the fitted intercept and slope. Be sure to include their units. Note if either estimate represents a large extrapolation and is consequently not reliable.\n(c) Interpret \\(r^2\\) and \\(s_e\\), associated with the fitted equation. Attach units to these summary statistics as appropriate.\n(d) If a manufacturer increases the size of the engine by 0.5 liter, should it use 0.5b1 to get a sense of how much more power the engine will generate?\n(e) A vehicle with a 3-liter engine among these produces 340 horsepower. What is the residual for this case? Does the.data point representing this car lie above or below the fitted line?\n(f) How would you describe cars with positive residuals? Those with negative residuals?\n(g) Do you find patterns in the residuals from this regression? Does it make sense to interpret \\(s_e\\) as the standard deviation of the errors of the fit? Use the plot of the residuals on the predictor to help decide.\nOECD The Organization for Economic Cooperation and Development (OECD) tracks various summary statistics of the member economies. Two variables of interest are GDP (gross domestic product per capita, a measure of the overall production in an economy per citizen) and trade balances (measured as a percentage of GDP). Exporting countries tend to have large positive trade balances. Importers have negative balances. These data were extracted from online databases provided by the OECD in 2016.\n(a) Describe the association in the scatterplot of GDP on Trade Balance.\n(b) Estimate the least squares linear equation for GDP on Trade Balance. Interpret the fitted intercept and slope. Be sure to include their units. Note if either estimate represents a large extrapolation and is consequently not reliable.\n(c) Interpret \\(r^2\\) and \\(s_e\\), associated with the fitted equation. Attach units to these summary statistics as appropriate.\n(d) Plot the residuals from this regression. After considering this plot, does \\(s_e\\) provide an adequate summary of the residual variation?\n(e) Which country has the largest values of each variable? Is it the country that you expected?\n(f) Locate the United States in the scatterplot and find the residual for the United States. Interpret the value of the residual for the United States.\nHiring A firm that operates a large, direct-to-consumer sales force would like to implement a system to monitor the progress of new agents. A key task for agents is to open new accounts; an account is a new customer to the business. The goal is to identify “superstar agents” as rapidly as possible, offer them incentives, and keep them with the company. To build such a system, the firm has been monitoring sales of new agents over the past 2 years. The response of interest is the profit to the firm (in dollars) of contracts sold by agents over their first year. Among the possible predictors of this performance is the number of new accounts developed by the agent during the first 3 months of work.\n(a) Create a scatterplot for Profit from Sales on Number of Accounts. Does a line seem to be a good summary of the association between these variables?\n(b) Estimate the least squares linear equation for Profit from Sales on Number of Accounts. Interpret the fitted intercept and slope; be sure to include their units. Note if either estimate represents a large extrapolation and is consequently not reliable.\n(c) Interpret \\(r^2\\) and \\(s_e\\), associated with the fitted equation. Attach units to these summary statistics as appropriate.\n(d) Based on the equation fit in part (b), what is the gain in profit to the firm of getting agents to open 100 additional accounts in the first 3 months? Do you think that this is a reasonable estimate?\n(e) Plot the residuals from the regression fit in part (b) on the number of accounts. Does this plot show random variation?\n(f) Exclude the data for agents who open 75 or fewer accounts in the first 3 months. Does the fit of the least squares line change much? Should it?\nPromotion These data describe spending by a major pharmaceutical company for promoting a cholesterol lowering drug. The data cover 39 consecutive weeks and isolate the area around Boston. The variables in this collection are shares. Marketing research often uses the notion of voice to describe the level of promotion for a product. In place of the absolute spending for advertising, voice is the share of a type of advertising devoted to a specific product. Voice puts this spending in context; $10 million might seem like a lot for advertising unless everyone else is spending $200 million.\nThe column Market Share is sales of this product divided by total sales for such drugs in th Boston area. The column Detail Voice is the ratio of detailing for this drug to the amount of detailing for all cholesterol-lowering drugs in Boston. Detailing counts the number of promotional visits made by representatives of a pharmaceutical company to doctors’ offices.\n(a) Do timeplots of Market Share and Detail Voice suggest an association between these series? Does either series show simple variation?\n(b) Create a scatterplot for Market Share on Detail Voice. Are the variables associated? Does a line summarize any association?\n(c) Estimate the least squares linear equation for the regression of Market Share on Detail Voice. Interpret the intercept and slope. Be sure to include the units for each. Note if either estimate represents a large extrapolation and is consequently not reliable.\n(d) Interpret \\(r^2\\) and \\(s_e\\), associated with the fitted equation. Attach units to these summary statistics as appropriate.\n(e) According to this equation, how does the average share for this product change if the detail voice rises from 0.04 to 0.14 ( 4% to 14%)?\n(f) Plot the residuals from the regression fit in part (c) on the level of detail voice. Does this plot suggest that the residuals possess simple variation?\nApple This data set tracks the monthly performance of stock in Apple Computer since January 1990 through the end of 2015. The data include 312 monthly returns on Apple Computer, as well as returns on the entire stock market, returns on Treasury Bills (short term, 30-day loans to Uncle Sam), and inflation.\n(The column Market Return is the return on a value weighted portfolio that purchases stock in proportion to the size of the company rather than one from each company.)\n(a) Begin by inspecting timeplots of the variables Apple Return and Market Return. Do the timeplots show trends that would be obscured in the scatterplot of Apple Return and Market Return?\n(b) Create a scatterplot for Apple Return on Market Return. Does a line seem to be a good summary of the association between these variables?\n(c) Estimate the least squares linear equation for Apple Return on Market Return. Interpret the fitted intercept and slope. Be sure to include their units. Note if either estimate represents a large extrapolation and is consequently not reliable.\n(d) Interpret \\(r^2\\) and \\(s_e\\), associated with the fitted equation. Attach units to these summary statistics as appropriate.\n(e) If months in which the market went down by 2% were compared to months in which the market went up by 2%, how would this equation suggest Apple stock would differ between these periods?\n(f) Plot the residuals from the regression fit in part (b) on Market Return. Does this plot suggest that the residuals possess simple variation? Do you recognize the dates of any of the outliers?\n(g) Careful analyses of stock prices often subtract the so-called risk-free rate from the returns on the stock. After the risk-free rate has been subtracted, the returns are sometimes called “excess returns” to distinguish them. The risk-free rate is the interest rate returned by a very safe investment, one with no (or at least almost no) chance of default. The return on short-term Treasury Bills is typically used as the risk-free rate. Subtract the risk-free rate from returns on Apple stock and the market, and then refit the equation using these excess returns. Does the equation change from the previous estimate? Explain why it’s similar or different."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\tag{1}\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Equation 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation 1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\tag{2}\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Equation 2, i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\tag{3}\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\tag{4}\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Equation 4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike’s Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\tag{5}\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\tag{6}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#tips",
    "href": "project-tips-resources.html#tips",
    "title": "Project tips + resources",
    "section": "Tips",
    "text": "Tips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that’s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-tips-resources.html#formatting-communication-tips",
    "href": "project-tips-resources.html#formatting-communication-tips",
    "title": "Project tips + resources",
    "section": "Formatting + communication tips",
    "text": "Formatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\n\nHeaders\n\nUse headers to clearly label each section.\nInspect the document outline to review your headers and sub-headers.\n\n\n\nReferences\n\nInclude all references in a section called “References” at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called “Appendix”.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\nResize plots and figures, so you have more space for the narrative.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf you’re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\n\n❌ NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n✅ YES! Names are readable\n\nggplot(data = mpg, aes(y = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg %>%\n  count(manufacturer) %>%\n  mutate(manufacturer = str_to_title(manufacturer)) %>%\n  ggplot(aes(y = fct_reorder(manufacturer,n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_minimal() \n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel <- lm(mpg ~ hp, data = mtcars)\ntidy(model) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STAT 1010.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document."
  },
  {
    "objectID": "project-tips-resources.html#additional-resources",
    "href": "project-tips-resources.html#additional-resources",
    "title": "Project tips + resources",
    "section": "Additional resources",
    "text": "Additional resources\n\nR for Data Science\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "STA 1010: Introduction to Business Statistics",
    "section": "",
    "text": "This course introduces students to the discipline of statistics as a science of understanding and analyzing data. Throughout the semester, students will learn how to effectively make use\nof data in the face of uncertainty: how to collect data, how to analyze data, and how to use data to make inferences and conclusions about real world phenomena.\n\nThe course goals are as follows:\n\nRecognize the importance of data collection, identify limitations in data collection methods, and determine how they affect the scope of inference.\nUse statistical software to summarize data numerically and visually, and to perform data analysis.\nHave a conceptual understanding of the unified nature of statistical inference.\nApply estimation and testing methods to analyze single variables or the relationship between two variables in order to understand natural phenomena and make data-based\ndecisions.\nModel numerical response variables using a single or multiple explanatory variables.\nInterpret results correctly, effectively, and in context without relying on statistical jargon.\nCritique data-based claims and evaluate data-based decisions.\nComplete a research project demonstrating mastery of statistical data analysis from exploratory analysis to inference to modeling."
  },
  {
    "objectID": "project-description.html#data",
    "href": "project-description.html#data",
    "title": "Showcase your inner data scientist",
    "section": "Data",
    "text": "Data\nIn order for you to have the greatest chance of success with this project it is important that you choose a manageable dataset. This means that the data should be readily accessible and large enough that multiple relationships can be explored. As such, your dataset must have at least 50 observations and between 10 to 20 variables (exceptions can be made but you must speak with me first). The dataset’s variables should include categorical variables, discrete numerical variables, and continuous numerical variables.\nIf you are using a dataset that comes in a format that we haven’t encountered in class, make sure that you are able to load it into R as this can be tricky depending on the source. If you are having trouble ask for help before it is too late.\nNote on reusing datasets from class: Do not reuse datasets used in examples, homework assignments, or labs in the class.\nBelow are a list of data repositories that might be of interest to browse. You’re not limited to these resources, and in fact you’re encouraged to venture beyond them. But you might find something interesting there:\n\nTidyTuesday\nNHS Scotland Open Data\nEdinburgh Open Data\nOpen access to Scotland’s official statistics\nBikeshare data portal\nUK Gov Data\nKaggle datasets\nOpenIntro datasets\nAwesome public datasets\nYouth Risk Behavior Surveillance System (YRBSS)\nPRISM Data Archive Project\nHarvard Dataverse\nIf you know of others, let me know, and we’ll add here…"
  },
  {
    "objectID": "project-description.html#deliverables",
    "href": "project-description.html#deliverables",
    "title": "Showcase your inner data scientist",
    "section": "Deliverables",
    "text": "Deliverables\n\nProposal - due 26 October\nPresentation - due 5 & 7 Dec\nExecutive summary - due 9 Dec\n\n\nProposal\nThis is a draft of the introduction section of your project as well as a data analysis plan and your dataset.\n\nSection 1 - Introduction: The introduction should introduce your general research question and your data (where it came from, how it was collected, what are the cases, what are the variables, etc.).\nSection 2 - Data: Place your data in the `/data` folder, and add dimensions and codebook to the README in that folder. Then print out the output of and codebook to the README in that folder. Then print out the output of glimpse() or skim() of your data frame. Add the README for the data, the codebook, and the output of glimpse() or skim() to the end of your proposal.\nSection 3 - Data analysis plan:\n\nAny outcomes (response, Y) and predictor (explanatory, X) variables you will use to answer your question.\nThe comparison groups you will use, if applicable.\nVery preliminary exploratory data analysis, including some summary statistics and visualizations, along with some explanation on how they help you learn more about your data. (You can add to these later as you work on your project.)\nThe method(s) that you believe will be useful in answering your question(s). (You can update these later as you work on your project.)\nWhat results from these specific statistical methods are needed to support your hypothesized answer?\n\n\nEach section should be no more than 1 page (excluding figures). You can check a print preview to confirm length.\nThe grading scheme for the project proposal is as follows. Note that after you receive feedback for your proposal you can improve it based on the feedback and re-submit it. If you re-submit, your final score for the proposal will be the average of two scores you receive (first and second submission).\n\n\n\nTotal\n10 pts\n\n\n\n\nData\n3 pts\n\n\nProposal\n5 pts\n\n\nWorkflow, organization, code quality\n1 pt\n\n\nTeamwork\n1 pt\n\n\n\n\n\nPresentation\n5 minutes maximum, and each team member should say something substantial. You can either present live during your workshop or pre-record and submit your video to be played during the workshop.\nPrepare a slide deck using the template I will give to you. This template uses quarto, and allows you to make presentation slides using R Markdown syntax. There isn’t a limit to how many slides you can use, just a time limit (5 minutes total). Each team member should get a chance to speak during the presentation. Your presentation should not just be an account of everything you tried (“then we did this, then we did this, etc.”), instead it should convey what choices you made, and why, and what you found.\nBefore you finalize your presentation, make sure your chunks are turned off with echo = FALSE.\nPresentations will take place on 5 and 7 December. You can choose to do your presentation live or pre-record it. During your workshop you will watch presentations from other teams in your workshop and provide feedback in the form of peer evaluations. The presentation line-up will be generated randomly.\nThe grading scheme for the presentation is as follows:\n\n\n\n\n\n\n\nTotal\n50 pts\n\n\n\n\nTime management: Did the team divide the time well amongst themselves or got cut off going over time?\n4 pts\n\n\nContent: Is the research question well designed and is the data being used relevant to the research question?\n5 pts\n\n\nProfessionalism: How well did the team present? Does the presentation appear to be well practiced? Did everyone get a chance to say something meaningful about the project?\n5 pts\n\n\nTeamwork: Did the team present a unified story, or did it seem like independent pieces of work patched together?\n6 pts\n\n\nContent: Did the team use appropriate statistical procedures and interpretations of results accurately?\n10 pts\n\n\nCreativity and Critical Thought: Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n10 pts\n\n\nSlides: Are the slides well organized, readable, not full of text, featuring figures with legible labels, legends, etc.?\n10 pts\n\n\n\n\n\nExecutive summary\nAlong with your presentation slides, we want you to provide a brief summary of your project.\nThis executive summary should provide information on the dataset you’re using, your research question(s), your methodology, and your findings.\nThe executive summary is worth 15 points and will be evaluated based on whether it follows guidance and whether it’s concise but detailed enough.\n\n\nProject organization\nThe following folders and files in your project. Please download the template here:\n\npresentation.qmd + presentation.html: Your presentation slides\nREADME.Rmd + README.md: Your write-up\n/data: Your dataset in CSV or RDS format and your data dictionary\n/proposal: Your project proposal\n\nStyle and format does count for this assignment, so please take the time to make sure everything looks good and your data and code are properly formatted."
  },
  {
    "objectID": "project-description.html#tips",
    "href": "project-description.html#tips",
    "title": "Showcase your inner data scientist",
    "section": "Tips",
    "text": "Tips\n\nYou’re working in the same google doc as your teammates now, so make sure that the coding is in the correct order.\nReview the marking guidelines below and ask questions if any of the expectations are unclear.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution.\nSet aside time to work together and apart (physically).\nWhen you’re done, review the documents to make sure you’re happy with the final state of your work. Then go get some rest!\nCode: In your presentation your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your quarto file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcomed to show that portion.\n\nTeamwork: You are to complete the assignment as a team. All team members are expected to contribute equally to the completion of this assignment and team evaluations will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-description.html#marking",
    "href": "project-description.html#marking",
    "title": "Showcase your inner data scientist",
    "section": "Marking",
    "text": "Marking\n\n\n\nTotal\n100 pts\n\n\n\n\nProposal\n10 pts\n\n\nPresentation\n50 pts\n\n\nExecutive summary\n15 pts\n\n\nReproducibility and organization\n10 pts\n\n\nTeam peer evaluation\n10 pts\n\n\nClassmates’ evaluation\n5 pts\n\n\n\n\nCriteria\nYour project will be assessed on the following criteria:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100% - Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89% - Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79% - Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69% - Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60% - Student is not making a sufficient effort.\n\n\n\nTeam peer evaluation\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member out of 10 points. You will additionally report a contribution percentage for each team member. Filling out the survey is a prerequisite for getting credit on the team member evaluation.If you are suggesting that an individual did less than 20% of the work, please provide some explanation. If any individual gets an average peer score indicating that they did less than 10% of the work, this person will receive half the grade of the rest of the group.\n\n\nLate work policy\n\nThere is no late submission / make up for the presentation. You must be in class on the day of the presentation to get credit for it or pre-record and submit your presentation by 9am in the morning of the presentations.\nThe late work policy for the write-up is 5% of the maximum obtainable mark per calendar day up to seven calendar days after the deadline. If you intend to submit work late for the project, you must notify the course organizer before the original deadline as well as soon as the completed work is submitted."
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download a PDF copy of the syllabus."
  },
  {
    "objectID": "course-syllabus.html#course-info",
    "href": "course-syllabus.html#course-info",
    "title": "Syllabus",
    "section": "Course info",
    "text": "Course info\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nSection 1\nMon & Wed\n8:30 am - 10:00 am\nSHDH 1206\n\n\nSection 2\nMon & Wed\n10:15 am - 11:45 am\nSHDH 1206\n\n\nSection 3\nMon & Wed\n1:45 pm - 3:15 pm\nSHDH 1206"
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the semester, you will be able to…\n\nunderstand data manipulation, and find basic summaries\nreason under uncertainity\nmake predictions\nunderstand probability\ncommunicate results from statistical analyses to a general audience."
  },
  {
    "objectID": "course-syllabus.html#community",
    "href": "course-syllabus.html#community",
    "title": "Syllabus",
    "section": "Community",
    "text": "Community\n\nWharton’s Code of Conduct\nAs a student in this course, you have agreed to uphold the Wharton’s Student Code of Conduct as well as the practices specific to this course.\n\n\nInclusive community\nMy goal as your lecturer is to help you accomplish or discover your passion and find your spark. You all belong here. It is also my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Diversity, Inclusion and Belonging at the Wharton School. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups. Please feel free to leave anonymous comments in my mailbox on the 4th floor of the Academic Research Building.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean is an excellent resource.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Weingarten Center is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the the Weingarten Center to request or update accommodations under these circumstances; it can take up to 4 weeks to review documents and get approval.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website at stats1010-f22.github.io/website.\nI will regularly send course announcements via email and canvas, make sure to check one or the other of these regularly. If an announcement is sent Monday through Thursday, I will assume that you have read the announcement by the next day. If an announcement is sent on a Friday or over the weekend, I will assume that you have read it by Monday.\n\n\nWhere to get help\n\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. Please use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the Ed discussion forum. There is a chance another student has already asked a similar question, so please check the other posts in Conversations before adding a new question. If you know the answer to a question posted in the discussion forum, I encourage you to respond!\nEmails should be reserved for questions not appropriate for the public forum. If you email me, please include “STAT 1010” in the subject line and ONLY STAT 1010. Barring extenuating circumstances, I will respond to STAT 1010 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.\n\nCheck out the Support page for more resources."
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nWhile there is no official textbook for the course, we will be assigning readings from the following textbooks.\n\nR for Data Science by Garret Grolemund and Hadley Wickham\nIntroduction to Modern Statistics by Mine Çetinkaya-Rundel and Johanna Hardin\nStatistics for Business by Robert Stine and Dean Foster"
  },
  {
    "objectID": "course-syllabus.html#lectures",
    "href": "course-syllabus.html#lectures",
    "title": "Syllabus",
    "section": "Lectures",
    "text": "Lectures\nThe goal of the lectures is for them to be as interactive as possible. My role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. You are expected to attend all lecture and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature application exercises that will be graded. In addition to application exercises will be periodic activities to help build a learning community. These will be short, fun activities that will help everyone in the class connect throughout the semester.\nYou are expected to bring a laptop to each class so that you can take part in the in-class exercises. Please make sure your laptop is fully charged before you come to class as the number of outlets in the classroom will not be sufficient to accommodate everyone. If you are in need of a loaner laptop please seek support here."
  },
  {
    "objectID": "course-syllabus.html#teams",
    "href": "course-syllabus.html#teams",
    "title": "Syllabus",
    "section": "Teams",
    "text": "Teams\nYou will be assigned to a team at the beginning of the semester. You are encouraged to sit with your teammates in lecture. All team members are expected to contribute equally to the completion of the labs and project and you will be asked to evaluate your team members throughout the semester. Failure to adequately contribute to an assignment will result in a penalty to your mark relative to the team’s overall mark."
  },
  {
    "objectID": "course-syllabus.html#assessment",
    "href": "course-syllabus.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\nAssessment for the course is comprised of five components: application exercises, homework assignments, exams, projects, and teamwork.\n\nApplication exercises\nParts of some lectures will be dedicated to working on Application Exercises (AEs). These exercises give you an opportunity to apply the statistical concepts and code introduced in the readings and lectures. Due dates for AEs will be announced as they are assigned.\nBecause these AEs are for practice, they will be graded based on completion, i.e., a good-faith effort has been made in attempting all parts. Successful on-time completion of at least 80% of AEs will result in full credit for AEs in the final course grade.\n\n\nHomework\nIn homework, you will apply what you’ve learned during lecture and lab to complete data analysis tasks. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. Similar to lab assignments, homework must be typed up using Quarto and submitted as a PDF in Canvas.\nOne homework assignment will be dedicated to a statistics experience. The statistics experience is an opportunity to engage with statistics and data science outside of the classroom through podcasts, books, seminars, data analysis competitions, and other activities. As you complete these experiences, the goal is to consider how the material you’re learning in the course connects with society more broadly.\nThe lowest homework grade will be dropped at the end of the semester.\n\n\nExams\nThere will be four exams consisting of multiple choice questions. Through these exams you have the opportunity to demonstrate what you’ve learned in the course thus far. The exams will focus on the conceptual understanding of the content. The content of the exam will be related to the content in the prepare, practice, and perform assignments. More detail about the exams will be given during the semester.\nThe lowest exam grade will be dropped at the end of the semester.\n\n\nProject\nThe purpose of the project is to apply what you’ve learned throughout the semester to analyze an interesting, data-driven research question. The project will be completed with your teams, and each team will present their work. More information about the project will be provided during the semester."
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nApplication exercises\n7%\n\n\nHomework\n35% (7 x 5%)\n\n\nProject\n15%\n\n\nExam 01\n10%\n\n\nExam 02\n10%\n\n\nExam 03\n10%\n\n\nTeamwork\n13%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n>= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n< 60"
  },
  {
    "objectID": "course-syllabus.html#five-tips-for-success",
    "href": "course-syllabus.html#five-tips-for-success",
    "title": "Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success on this course depends very much on you and the effort you put into it. The course has been organized so that the burden of learning is on you. Your TA and I will help you be providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask the TA, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask us why. If you’re not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings.\nDo the homework and lab.The earlier you start, the better. It’s not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example.)\nDon’t procrastinate. If something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, and eventually you won’t know where to begin asking questions. Don’t let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours, and let me help you identify a good (re)starting point."
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic integrity\nTL;DR: Don’t cheat!\nAll students must adhere to Wharton’s Code of Academic Integrity: Wharton is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nRegardless of course delivery format, it is your responsibility to understand and follow Wharton policies regarding academic integrity, including doing one’s own work, following proper citation of sources, and adhering to guidance around group work projects. Ignoring these requirements is a violation of the Wharton Community Standard. If you have any questions about how to follow these requirements, please contact Julie Nettleton, Director of the Center for Community Standards and Accountability (CSA).\n\n\nCollaboration policy\nOnly work that is clearly assigned as team work should be completed collaboratively.\n\nThe homework assignments must be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what’s the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to homework questions (including any code) with anyone other than myself and the teaching assistants.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\n\n\n\nPolicy on sharing and reusing code\nI am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course’s policy is that you may make use of any online resources (e.g. RStudio Community, StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. On individual assignments you may not directly share code with another student in this class, and on team assignments you may not directly share code with another team in this class.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework and lab assignment will be dropped to accommodate such circumstances.\n\nHomework and labs may be submitted up to 3 days late. There will be a 5% deduction for each 24-hour period the assignment is late.\nThere is no late work accepted for application exercises, since these are designed to help you prepare for labs and homework.\nThe late work policy for the project will be provided with the project instructions.\n\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a lab or homework assignment by the stated due date, you may email dr. sturdevant before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used for once in the semester, so only use it for a truly extenuating circumstance.\nIf there are circumstances that are having a longer-term impact on your academic performance, please let your academic dean know, as they can be a resource. Please let dr. sturdevant know if you need help contacting your academic dean.\n\n\nRegrade request policy\nRegrade requests must be submitted on Gradescope within a week of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the final project presentations.\n\n\nAttendance policy\nResponsibility for class attendance rests with individual students. Since regular and punctual class attendance is expected, students must accept the consequences of failure to attend. More details on Wharton attendance policies are available here.\nHowever, there may be many reasons why you cannot be in class on a given day, particularly with possible extra personal and academic stress and health concerns this semester. All course lectures will be recorded and available to enrolled students after class. If you miss a lecture, make sure to watch the recording and review the material before the next class session. Given the technologies we use in the course, this is straightforward to do asynchronously. If you know you’re going to miss a class and you’re feeling well enough to do so, notify your teammates ahead of time. Additionally, please fill out a Course Absence Notice (CAN) so that I am notified in case of an attendance quiz. Overall these policies are put in place to ensure communication between team members, respect for each others’ time, and also to give you a safety net in the case of illness or other reasons that keep you away from attending class.\n\n\nAttendance policy related to COVID symptoms, exposure, or infection\nStudent health, safety, and well-being are the university’s top priorities. To help ensure your well-being and the well-being of those around you, please do not come to class if you have symptoms related to COVID-19, have had a known exposure to COVID-19, or have tested positive for COVID-19. If any of these situations apply to you, you must follow university guidance related to the ongoing COVID-19 pandemic and current health and safety protocols. If you are experiencing any COVID-19 symptoms, contact student health at 215-898-0300. To keep the university community as safe and healthy as possible, you will be expected to follow these guidelines. Please reach out to me and your academic dean as soon as possible if you need to quarantine or isolate so that we can discuss arrangements for your continued participation in class. For the safety of all students, I require that students wear masks in class and prefer N95 or KN95 masks.\n\n\nInclement weather policy\nIn the event of inclement weather or other connectivity-related events that prohibit class attendance, I will notify you how we will make up missed course content and work. This might entail holding the class on Zoom synchronously or watching a recording of the class.\n\n\nPolicy on video recording course content\nLectures will be recorded and available on Canvas, so students should not need to create their own recordings of lectures. If you feel that you need record the lectures yourself, you must get written permission from me ahead of time and these recordings should be used for personal study only, not for distribution. The full policy on recording of lectures falls under the University of Pennsylvania’s V.L. Policy on Unauthorized Copying of Copyrighted Media, available at https://catalog.upenn.edu/faculty-handbook/v/v-l/. Unauthorized distribution may result in a civil suite, criminal charges, and/or penalties and fines."
  },
  {
    "objectID": "course-syllabus.html#learning-during-a-pandemic",
    "href": "course-syllabus.html#learning-during-a-pandemic",
    "title": "Syllabus",
    "section": "Learning during a pandemic",
    "text": "Learning during a pandemic\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don’t hesitate to ask.\n\nYou never owe me personal information about your health (mental or physical) but you’re always welcome to talk to me. If I can’t help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded during this crisis.\n\nNote: If you’ve read this far in the syllabus, send me an email with a picture of your pet if you have one or your favorite memes with STAT1010 in the subject line!"
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nAugust 30: First day of classes\nSeptember 5: Labor Day holiday, no classes are held\nSeptember 13: Course Selection Period ends\nOctober 6 - 9: Fall term break, no classes are held\nOctober 10: Drop period ends\nOctober 10: Indigenous People’s Day (classes in session)\nOctober 28: Grade Type Change Deadline\nNovember 7: Last day to withdraw from a course\nNovember 22-23: Thur-Fri class schedule on Tue-Wed\nNovember 24-27: Thanksgiving Break\nDecember 12: Last day of classes\nDecember 13-14: Reading Days\nDecember 15 - 22: Final exams\nDecember 22: Fall term ends\n\nClick here for the full University of Pennsylvania academic calendar."
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "Discussion forum\n🔗 on Ed discussion\n\n\n\n\nLecture streaming and recordings (email for permission)\n🔗 on Canvas\n\n\nSubmit AE\n🔗 on Canvas\n\n\nSubmit HWK\n🔗 on Gradescope\n\n\nGradebook\n🔗 on Canvas"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "dr. gwynn sturdevant (she/her/ella) is a Lecturer at Wharton. gwynn’s work focuses on revamping introductory statistics and data science curriculum, connecting recent advances in human computer interaction to statistical programming languages, and community-led research involving those facing entrenched inequalities.\n\n\n\nOffice hours\nLocation\n\n\n\n\nTues 11:15 - 12:15\nARB 327\n\n\nFri 11:00 - 12:00 & Tues 6 - 7pm\nZoom\n\n\n\nIf these times don’t work for you or you’d like to schedule a one-on-one meeting, you can do so by emailing gwynnc@wharton.upenn.edu. Please put STATS 1010 in the subject line."
  },
  {
    "objectID": "exams/exam-1.html",
    "href": "exams/exam-1.html",
    "title": "Exam 1",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 4 at 9 am ET and must be completed by Mon, Feb 7 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-1- repo to complete Part 3 of your exam. Add your answers to the exam-1.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 7 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀\n\n\n\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am).\n\n\n\n\n\nPart 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "ae/ae-8.html",
    "href": "ae/ae-8.html",
    "title": "Random variables AE-8",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 14 Oct at 2:00pm.\n\n\n\nlibrary(tidyverse)\nlibrary(praise)\n\n\nGiven that the random variable \\(X\\) has mean \\(\\mu = 120\\) and SD \\(\\sigma = 15\\), find the mean and SD of each of these random variables that are defined by \\(X\\).\n\n\\(X/3\\)\n\\(2X - 100\\)\n\\(X+2\\)\n\\(X-X\\)\n\nAn investor buys the stock of two companies, investing \\(\\$10000\\) in each. The stock of each company either goes up by \\(80%\\) after a month (rising to \\(\\$18000\\)) with probability \\(\\frac{1}{2}\\) or drops by \\(60%\\) (falling to \\(\\$4000\\)) with probability \\(\\frac{1}{2}\\). Assume that the changes in each are independent. Let the random variable \\(X\\) denote the value of the amounted invested after one month.\n\nFind the probability distribution of \\(X\\).\nFind the mean value of \\(X\\)\nDoe the mean value represent the experience of the typical investor?\n\n\n\npraise()\n\n\nA law firm takes cases on a contingent fee basis. If the case goes to trial, the firm expects to earn \\(\\$25000\\) as part of the settlement if it wins and nothing if it does not. The firm wins one-third of the cases that go to trial. If the case does not go to trial, the firm earns nothing. Half of the cases do not go to trial.\n\nDefine a random variable to model the earning of taking a case of this type.\nWhat is the expected value of such a case to the firm?\nWhat is the standard deviation of the earnings?\n\nThe maintenance staff of a large office building regularly replaces fluorescent ceiling lights that have gone out. During a visit to a typical floor, the staff may have to replace several lights. The manager of this staff has given the following probabilities to the number of lights (identified by the random variable \\(Y\\)) that need to be replaced on the floor:\n\n\n\n\\(Y\\)\n0\n1\n2\n3\n4\n\n\n\\(P(Y = y)\\)\n0.2\n0.15\n0.2\n0.3\n0.15\n\n\n\n\nHow many lights should the manager expect to replace on a floor?\nWhat is the standard deviation of the number of lights on a floor that are replaced?\nIf a crew takes six lights to a floor, how many should it expect to have left after replacing those that are out?\nIf a crew takes six lights to a floor, find the standard deviation of the number of lights that remain after replacing those that are out on a floor.\nIf it takes 10 minutes to replace each light, how long should the manager expect the crew to take when replacing the lights on a floor.\n\nSuppose that you’ve just bought a \\(\\$4000\\) TV. Should you also buy the \\(\\$50\\) surge protector that guarantees to protect your TV from electric surges caused by lightning?\n\nLet \\(p\\) denote the probability that your home is hit by lightning during the time that you own this TV, say five years. In order for the purchase of the surge protector to have positive long-term values, what must the chance of being hit by lightning be?\nThere are about 100 million households in the United States, and fewer than 10,000 get hit by lightening each year. Do you think the surge protector is a good deal on average? Be sure to note any assumptions that you make.\n\n\n\npraise()\n\n\nThe ATM at a local convenience store allows customers to make withdrawals of \\(\\$10\\), \\(\\$20\\), \\(\\$50\\), or \\(\\$100\\). Let \\(X\\) denote a random variable that indicates the amount withdrawn by a customer. The probability distribution of \\(X\\) is:\n\\(p(10) = 0.2\\)\n\\(p(20) = 0.5\\)\n\\(p(50) = 0.2\\)\n\\(p(100) = 0.1\\)\n\nPlot the probability distribution of \\(X\\) in R.\nWhat is the probability that a customer withdraws more than \\(\\$20\\)?\nWhat is the expected amount of money withdrawn by a customer?\nThe expected value is not a possible value value of the amount withdrawn. Interpret the expected value for a manager.\nFind the variance and standard deviation of \\(X\\)."
  },
  {
    "objectID": "ae/ae-4.html",
    "href": "ae/ae-4.html",
    "title": "Introduction to probability (AE-4)",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 23 Sept at 2:00pm.\n\n\n\nOne ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan?\nWhat is the probability that the ball will not be cyan?\nInstead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\nNow repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling with replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\nTwo events A and B are independent if Pr(A and B)=Pr(A)P(B). Under which situation are the draws independent?\n\nYou don’t replace the draw.\nYou replace the draw.\nNeither\nBoth\n\nSay you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one is yellow?\nIf you roll a 6-sided die six times, what is the probability of not seeing a 6?"
  },
  {
    "objectID": "ae/ae-11.html",
    "href": "ae/ae-11.html",
    "title": "Confidence intervals: AE - 10",
    "section": "",
    "text": "Match each item on the left with its correct description on the right.\n1.\\(y \\pm 2se(\\bar{y})\\) (a)  Sampling distribution of X\n2. \\(\\hat{p} \\pm se(\\hat{p})\\) (b)  Margin of error\n3. \\(2se (\\bar{X})\\) (c)  100% confidence interval for p\n4. \\(N(\\mu,\\sigma^2/n)\\) (d)  Estimated standard error of \\(\\bar{Y}\\)\n5. \\(s/\\sqrt{n}\\) (e)  Estimated standard error of \\(\\hat{p}\\)\n6. \\(\\sigma/\\sqrt{n}\\) (f)  An interval with about 95% coverage\n7. \\(1/(0.05)^2\\) (g)  Actual standard error of \\(\\bar{Y}\\)\n8. [0, 1] (h) About 2 for moderate sample sizes\n9. \\(\\sqrt{\\hat{p}(1 -\\hat{p})/n}\\) (i)   An interval with 68% coverage\n10. \\(t_{0.025, n-1}\\) (j)   Sample size needed for 0.05 margin of error\n11. True or False: By increasing the sample size from \\(n = 100\\) to \\(n = 400\\), we can reduce-the margin of error by \\(50\\%\\).\n12. If the 95% confidence interval for the average purchase of customers at a department store is $50 to $110, then $100 is a plausible value for the population mean at this level of confidence.\n13. If zero lies inside the 95% confidence interval for \\(\\mu\\), then zero is also inside the 99% confidence interval for \\(\\mu\\).\n14. The clothing buyer for a department store wants to order the right mix of sizes. As part of a survey, she measured the height (in inches) of men who bought suits at this store. Her software reported the following confidence interval:\nWith 95.00% confidence, 70.8876 < \\(\\mu\\) < 74.4970\n(a) Explain carefully what the software output means.\n(b) What’s the margin of error for this interval?\n(c) How should the buyer round the endpoints of the interval to summarize the result in a report for store managers?\n(d) If the researcher had calculated a 99% confidence interval, would the output have shown a longer or shorter interval?\n15. Hoping to lure more shoppers downtown, a city builds a new public parking garage in the central business district. The city plans to pay for the structure through parking fees. During a two-month period (44 weekdays), daily fees collected averaged $1,264 with a standard deviation of $150.\n(a) What assumptions must you make in order to use these statistics for inference?\n(b) Write a 90% confidence interval for the mean daily income this parking garage will generate, rounded appropriately.\n(c) The consultant who advised the city on this project predicted that parking revenues would average $1,300 per day. On the basis of your confidence interval, do you think the consultant was correct? Why or why not?\n(d) Give a 90% confidence interval for the total revenue earned during five weekdays."
  },
  {
    "objectID": "ae/ae-15.html",
    "href": "ae/ae-15.html",
    "title": "Active listening tool - 1",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 8 Sept at 2:00pm.\n\n\n\n\n\nGroup\nTitle\nComments ((I notice…, I wonder…))"
  },
  {
    "objectID": "ae/ae-0-first_line_of_code.html",
    "href": "ae/ae-0-first_line_of_code.html",
    "title": "Introduction to the diamonds dataset",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due at noon on 6 Sept at 4:59pm.\n\n\n\nScenario\nYou own a jewelry business and are working to price your diamonds. To facilitate, you explore the diamond dataset in R and pay special attention to the cost of the diamond. Let’s explore together.\nTo accomplish this, please paste each line of code into your RStudio instance, and answer the accompanying questions.\n\n\nCoding basics\nGood coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.\n\nWhen writing code, load all libraries first.\n“##” is used to write comments in R and should be used to justify next steps and anything unusual or unexpected steps\nComments are really important and are notes to your future self to remind you about steps that you took and decisions that you made.\n\n\n## loading library\nlibrary(tidyverse) # for data analysis and visualisation\n\n\n\nData\nThe diamonds dataset includes basic information about 53940 diamonds including price and other characteristics.\n\n## View the dataset in a separate tab\nglimpse(diamonds)\n\nThe ___ dataset has ___ observations and ___ variables.\n\n## What are the variables in this dataset?\n?diamonds\n\n\nWhat do the “4 c’s” of every diamond mean? What kind of variables are they? (hint: This video may help, copy this into your notes under the title data types)\n\nggplot is an R package that is used to create data visualizations. We will be using it throughout this course. The basic format for every ggplot command is:\n\n## DO NOT COPY INTO YOUR QUARTO DOCUMENT\n## THIS IS TO SHOW YOU THE BASIC FORMAT OF ALL ggplots\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\n\n\n## What is the distribution of price?\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = price), binwidth = 1) \n\n\nWhat kind of variable is price? What happens if you draw a histogram with another kind of variable?\n\n\n## What happens if you draw a histogram with a different type of variable?\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = OTHER_VAR), binwidth = 1) \n\n\nWhat does the binwidth in the geom_histogram() function do? (hint: draw multiple plots changing this value) What binwidth value is most appropriate for this data?\n\n\n## Find an appropriate binwidth\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = price), binwidth = OTHER_VALUE) \n\nThe distribution of diamond prices is right skewed (long right tail).\n\nIn your notes, copy these histograms and the words used to describe them.\n\nOne interesting feature of this graph is the dip in diamonds priced at about $1000. Let’s explore this a little further and look at some basic dplyr commands.\nThe pipe operator “%>%” tells R to take the output from one function and sends it to the input in the next function.\n\n## Filtering on price\ndiamonds %>% \n  filter(price > 750 & price < 1250) %>% \n  View()\n\n\nWhat does the filter function do?\n\n\ndiamonds %>% \n  filter(price > 750 & price < 1250) %>% ## Filtering on price\n  mutate(price_hundred = price %/% 100) %>% ## Finding price_hundred\n  View()\n\n\nWhat does this “%/%” operator do? (hint: run 10 %/% 3, 10 %/% 5, 7 %/% 3, 7 %/% 5 in the console)\nWhat does the mutate function do? (hint: pay special attention to the dimensions of the data)\n\n\ndiamonds %>% \n  filter(price > 750 & price < 1250) %>% ## Filtering on price\n  mutate(price_hundred = price %/% 100) %>% ## Finding price_hundred\n  select(price, price_hundred) %>% ## Select vars\n  View()\n\n\nWhat does the select function do?\n\n\ndiamonds %>% \n  filter(price > 750 & price < 1250) %>% ## Filtering on price\n  mutate(price_hundred = price %/% 100) %>% ## Finding price_hundred\n  select(price, price_hundred) %>% ## Select vars\n  count(price_hundred) %>% ## Count\n  View()\n\n\nCan you identify the values where the dip is using the information above? Why or why not?\n\n\ndiamonds %>% \n  filter(price > 750 & price < 1250) %>% ## Filtering on price\n  mutate(price_hundred = price %/% 100) %>% ## Finding price_hundred\n  select(price, price_hundred) %>% ## Select vars\n  mutate(price_remainder = price %% 100) %>% ## Finding price_remainder\n  count(price_hundred, price_remainder) %>% ## Count\n  View()\n\n10. What does the “%%” operator do?\n\nCan you identify the values where the dip is using the information above? Why or why not?\n\nCut may impact upon price\n\n## Cut and price\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = price, color = cut, fill = cut),\n                 binwidth = 100) \n\n\nWhich cut do you suspect is the most common in the dataset? How many diamonds with that cut are in the dataset?\n\n\n## Most common cute\ndiamonds %>% \n  INSERT_FUNCTION_HERE(cut)\n\n\nThe variables color or clarity might impact upon the price. Draw a histogram and color it using one of them.\n\n\n## Impact of color and clarity\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = price, color = VAR_NAME, \n                               fill = VAR_NAME), \n                 binwidth = YOUR_BIN_WIDTH) \n\n\nWhat do the fill and color inputs do in the above coding?\nWhat kind of variables should we use for the fill and color inputs? What happens if another type of variable is used?\n\n\n## Color and fill variables\nggplot(data = diamonds) +\n      geom_histogram(mapping = aes(x = price, color = VAR_NAME, \n                                   fill = VAR_NAME), \n                     binwidth = YOUR_BIN_WIDTH) \n\nSo far we have been looking at only 1 continuous variable. To look at two continous variables, we draw scatterplots.\n\n## Color and fill variables\nggplot(data = diamonds) +\n      geom_point(mapping = aes(x = price, y = depth)) \n\n\nWhat kinds of variables do you think you could use to color this plot? Please use an appropriate variable from diamonds dataset and do so.\n\n\n## Color and fill variables\nggplot(data = diamonds) +\n      geom_point(mapping = aes(x = price, y = depth))"
  },
  {
    "objectID": "ae/ae-17.html",
    "href": "ae/ae-17.html",
    "title": "Revision for last exam",
    "section": "",
    "text": "These materials may be useful:\nRevision 1 for exam 1\nRevision 2 for exam 1\nExam 1 study guide\nExam 2 revison\nExam 2 study guide\nExam 3 revison - short\nExam 3 revison - comprehensive\nExam 3 studyguide"
  },
  {
    "objectID": "ae/ae-16.html",
    "href": "ae/ae-16.html",
    "title": "Active listening tool - 2",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 8 Dec at 2:00pm.\n\n\n\n\n\nGroup\nTitle\nComments (I notice…, I wonder…)"
  },
  {
    "objectID": "ae/ae-3.html",
    "href": "ae/ae-3.html",
    "title": "Exploring numeric data (AE-3)",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 20 Sept at 2:00pm."
  },
  {
    "objectID": "ae/ae-3.html#dotplot",
    "href": "ae/ae-3.html#dotplot",
    "title": "Exploring numeric data (AE-3)",
    "section": "Dotplot",
    "text": "Dotplot\n\nLook at the coding below and one at a time remove each of the dotsize, alpha, and stackdir from the code. What does each of these inputs do? How does each input impact upon the plot?\n\n\nggplot(data = diamonds) + # the data\n  geom_dotplot(mapping = aes(x = price), # plot of price\n               dotsize = 0.005, # make the dots tiny\n               alpha = 0.3, # less opaque\n               stackdir = \"center\") #"
  },
  {
    "objectID": "ae/ae-3.html#dotplot-vs-boxplot",
    "href": "ae/ae-3.html#dotplot-vs-boxplot",
    "title": "Exploring numeric data (AE-3)",
    "section": "Dotplot vs boxplot",
    "text": "Dotplot vs boxplot\nA boxplot provides summary information for the distribution of a variable. Here are some explanations (using different data) showing the purpose of the box portion of the boxplot.\nThe left line of the box is the quartile 1 (\\(Q_1\\)) or Q1 \n\npraise()\n\nThe middle line of the box is the median (\\(Q_2\\)) \nThe right line of the box is the quartile 3 (\\(Q_3\\)) \n\nLooking at the above plots, count the percentage of yellow dots in each of the dotplots. Is there anything interesting about these values?\nCompare and contrast a dotplot and a boxplot? What are the advantages and disadvantages of each? Think carefully about bimodal data and which plot you would use to show that aspect of a variable. Also think about outliers."
  },
  {
    "objectID": "ae/ae-3.html#boxplots",
    "href": "ae/ae-3.html#boxplots",
    "title": "Exploring numeric data (AE-3)",
    "section": "Boxplots",
    "text": "Boxplots\n\nDraw a boxplot of the price data from the diamonds dataset.\n\n\nggplot(data = INSERT) + # the data\n  INSERT(aes(x = INSERT)) # the variable\n\n\nBoxplot descriptions\n\n\nUse the information above to describe the boxplot of price.\n\n\n\nInterquartile range (IQR)\nThe IQR of a variable is the length of the box in a boxplot. \\[ IQR = Q_3 - Q_1\\]\n\npraise()"
  },
  {
    "objectID": "ae/ae-3.html#density-plot",
    "href": "ae/ae-3.html#density-plot",
    "title": "Exploring numeric data (AE-3)",
    "section": "Density plot",
    "text": "Density plot\n\n ggplot(data = diamonds) + # the data\n       geom_density(aes(x = price), # plot of price\n                    bw = 1) # plot of price\n\n\nWhat does the “bw” input in the geom_density function do? What is an appropriate value of “bw” for this data?\n\n\nggplot(data = diamonds) + # the data\n  geom_density(aes(x = price), # plot of price\n               bw = INSERT) + # binwidth\n  facet_wrap(~clarity) # faceting\n\n\nWhat does the above plot suggest about the prices of diamonds for all levels of the variable clarity?\n\n\npraise()\n\n\nDraw a density plot for each value of cut. Describe the distribution of diamond price for each value of cut.\n\n\nggplot(data = diamonds) + # the data\n  geom_density(aes(x = price), # plot of price\n               bw = INSERT) + # binwidth\n  facet_wrap(~INSERT) # faceting"
  },
  {
    "objectID": "ae/ae-3.html#measures-of-center",
    "href": "ae/ae-3.html#measures-of-center",
    "title": "Exploring numeric data (AE-3)",
    "section": "Measures of center",
    "text": "Measures of center\nWe use the summarise() function when we expect the result to be one number.\n\nMean\n\ndiamonds %>% # the dataset\n  summarise(mean = mean(price)) # function that is used to compute mean of price\n\n9, Now compute the means of the variables \\(x\\), \\(y\\), \\(z\\), and \\(carat\\). (hint: You will need to insert a code chunk for each of these variables.)\n\npraise()\n\nLast class we learned about the group_by() and used it to compute conditional probabilities. Today we will use it to compare the means of different subgroups.\n\ndiamonds %>% # the data\n  group_by(cut) %>% # for each value of cut\n  summarise(mean = mean(price), # find the mean\n            median = median(price)) # and the median\n\n\nCompare the average values of price for each level of the cut variable. Considering the distributions of price for each level of cut in question 8, is the grouped mean a good measure of center for each level of cut? Why or why not?\n\n\n\nMedian\n\nHow would you find the overall median of price?"
  },
  {
    "objectID": "ae/ae-3.html#measures-of-spread",
    "href": "ae/ae-3.html#measures-of-spread",
    "title": "Exploring numeric data (AE-3)",
    "section": "Measures of spread",
    "text": "Measures of spread\n\nVariance\nR also computes measures of spread.\n\ndiamonds %>% # the data\n  summarise(var = var(price)) # find the variance\n\nBesides the direct method above, we can also do this computationally, by using the definition and finding the mean, subtracting, and squaring, then dividing.\n\ndiamonds %>% # the data\n  mutate(\n    mean_price = mean(price), # mean of price\n    deviation = price - mean_price, # subtraction from notes\n    deviation_sq = deviation^2) %>% # then square it\n  summarise(\n    mean_sq_deviation = sum(deviation_sq)/ # sum them\n      (nrow(diamonds) - 1)) # divide by n - 1\n\n\nRun each line of code above and ensure that you understand how this computation was developed. This shows us how to find \\(s_{price}^2\\) directly.\nFind \\(s_{x}^2\\), \\(s_{y}^2\\), and \\(s_{z}^2\\) both directly, and computationally: having R do the computation. (hint: you will need to insert 6 code chunks)\n\n\npraise()\n\n\n\nStandard deviation\n\nR also computes the standard deviation \\(s\\) directly:\n\n\n    diamonds %>% # the data\n      summarise(sd = sd(price)) # find the sd\n\nCan you use the computational approach (finding the mean_sq_deviation) from above to find the standard deviation of price \\(s_{price}\\)? (hint: you will need to use the sqrt() operator)\n\n\nIQR\n\nThe \\(IQR\\) can also be computed directly or indirectly:\n\n\ndiamonds %>% # the data\n  summarise(\n    q1 = quantile(price, 0.25), # find Q1\n    q3 = quantile(price, 0.75), # find Q3\n    iqr = q3 - q1 # now the IQR\n  )\n\n\n\nRange\n\ndiamonds %>% # the data\n  summarise(\n    min = min(price), # find min\n    max = max(price), # find max\n    range = max - min # and range\n  )\n\n\nOf all of the measures of spread, which do you think are sensitive to skewed data and outliers? Why?"
  },
  {
    "objectID": "ae/ae-7.html",
    "href": "ae/ae-7.html",
    "title": "Revision 2 for exam 1",
    "section": "",
    "text": "Find the matching item from the second column\n\n\n\n\n\n\n\n1. Independent events\na. \\(P(A \\cap B) \\neq P(A) \\times P(B)\\)\n\n\n\n\n\nDisjoint events\n\nb. \\(0\\leq p \\leq 1\\)\n\n\n\nUnion\n\nc. \\(A, B\\) disjoint \\(P(A\\cup B) = P(A) +P(B)\\)\n\n\n\nIntersection\n\nd. \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\n\nComplement of A\n\ne. \\(P(A \\cup B) \\leq P(A) + P(B)\\)\n\n\n\nSample space\n\nf. \\(A \\cap B\\)\n\n\n\nAddition rule\n\ng. \\(P(A \\cap B) = 0\\)\n\n\n\nComplement rule\n\nh. \\(P(S) = 1\\)\n\n\n\nBoole’s inequality\n\ni. \\(P(A^c) = 1 - P(A)\\)\n\n\n\nDependent events\n\nj. \\(P(A \\cap B) = P(A) \\times P(B)\\)\n\n\n\nKolmogorov’s axiom\n\nk. \\(A \\cup B\\)\n\n\n\nl. \\(A^c\\)\n\n\n\nm. \\(S\\)\n\n\n\n\n\n\nWhen \\(A \\subset B\\), then \\(A \\cup B = A\\)\n\nTrue\nFalse\n\nWhen \\(A \\subset B\\), then \\(A \\cap B = A\\)\n\nTrue\nFalse\n\nOne ball will be drawn at random from a box containing: 4 cyan balls, 3 magenta balls, and 5 yellow balls. What is the probability that the ball will be cyan?\nWhat is the probability that the ball will not be cyan?\nInstead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\nNow repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling with replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\nTwo events A and B are independent if Pr(A and B)=Pr(A)P(B). Under which situation are the draws independent?\n\nYou don’t replace the draw.\nYou replace the draw.\nNeither\nBoth\n\nSay you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one is yellow?\nIf you roll a 6-sided die six times, what is the probability of seeing neither a 5 or 6?\nPlease match the following venn diagrams with the corresponding set theory notation.\n\n\n\n22.\na.\\(B \\subset A\\)\n\n\n\n\n23.\nb. \\(A\\) & \\(B\\) disjoint\n\n\n24.\nc. \\(A\\)\n\n\n25.\nd. \\(A \\cup B\\)\n\n\n26.\ne. \\(A \\cap B\\)\n\n\n27.\nf. \\(A^c\\)\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n\n1\nj\n\n\n\n\n2\ng\n\n\n3\nk\n\n\n4\nf\n\n\n5\nl\n\n\n6\nm\n\n\n7\nd\n\n\n8\ni\n\n\n9\ne\n\n\n10\na\n\n\n11\nh, c, b\n\n\n12\nFalse\n\n\n13\nTrue\n\n\n14\n\\(\\frac{4}{4+3+5} = \\frac{4}{12}\\)\n\\(= \\frac{1}{3}\\)\n\n\n15\n\\(1 - \\frac{1}{3} = \\frac{2}{3}\\)\n\n\n16\nLet \\(C_1 = \\{1st\\:draw \\: is \\:cyan\\}\\) and\n\\(C_2 = \\{2nd\\: draw\\: is\\: cyan\\}\\)\nWe want :\\(P(C_1 \\cap C_2^c) = P(C_1) \\times P(C_2^c \\vert C_1)\\)\nThis is \\(\\frac{1}{3} \\times (1 -\\frac{3}{11}) = \\frac{8}{33}\\)\n\n\n17\nBecause they are independent:\\(P(C_1 \\cap C_2^c) = P(C_1) \\times P(C_2^c)\\)\n= \\(\\frac{1}{3} \\times \\frac{2}{3} = \\frac{2}{9}\\)\n\n\n18\nb\n\n\n19\n\\(\\frac{5}{12}\\)\n\n\n20\nLet \\(A = \\{5 \\: or \\: 6 \\: pips\\: facing\\: up\\}\\)\nWe want:\\(P(A^c \\cap A^c \\cap A^c \\cap A^c \\cap A^c \\cap A^c) = P(A^c)^6\\) because of independence of rolling a die.\n\\(= (\\frac{2}{3})^6 = \\frac{64}{729}\\)\n\n\n21\nNo question\n\n\n22\nc\n\n\n23\nf\n\n\n24\nb\n\n\n25\ne\n\n\n26\nd\n\n\n27\na"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html",
    "href": "ae/ae-4-exam-1-review.html",
    "title": "AE 4: Exam 1 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-4-exam-1-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#packages",
    "href": "ae/ae-4-exam-1-review.html#packages",
    "title": "AE 4: Exam 1 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggfortify)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "href": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "title": "AE 4: Exam 1 Review",
    "section": "Restaurant tips",
    "text": "Restaurant tips\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St. Olaf who worked at a local restaurant.1\nThe variables we’ll focus on for this analysis are\n\nTip: amount of the tip\nParty: number of people in the party\n\nView the data set to see the remaining variables.\n\ntips <- read_csv(\"data/tip-data.csv\")"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "href": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "title": "AE 4: Exam 1 Review",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\n\nVisualize, summarize, and describe the relationship between Party and Tip.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#modeling",
    "href": "ae/ae-4-exam-1-review.html#modeling",
    "title": "AE 4: Exam 1 Review",
    "section": "Modeling",
    "text": "Modeling\nLet’s start by fitting a model using Party to predict the Tip at this restaurant.\n\nWrite the statistical model.\nFit the regression line and write the regression equation. Name the model tips_fit and display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret the slope.\nDoes it make sense to interpret the intercept? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#inference",
    "href": "ae/ae-4-exam-1-review.html#inference",
    "title": "AE 4: Exam 1 Review",
    "section": "Inference",
    "text": "Inference\n\nInference for the slope\n\nThe following code can be used to create a bootstrap distribution for the slope (and the intercept, though we’ll focus primarily on the slope in our inference). Describe what each line of code does, supplemented by any visualizations that might help with your description.\n\n\nset.seed(1234)\n\nboot_dist <- tips %>%\n  specify(Tip ~ Party) %>%\n  generate(reps = 100, type = \"bootstrap\") %>%\n  fit()\n\n\nUse the bootstrap distribution created in Exercise 6, boot_dist, to construct a 90% confidence interval for the slope using bootstrapping and the percentile method and interpret it in context of the data.\n\n\n# add your code here\n\n\nConduct a hypothesis test at the equivalent significance level using permutation. State the hypotheses and the significance level you’re using explicitly. Also include a visualization of the null distribution of the slope with the observed slope marked as a vertical line.\n\n\n# add your code here\n\n\nCheck the relevant conditions for Exercises 7 and 8. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\nNow repeat Exercises 7 and 8 using approaches based on mathematical models.\n\n\n# add your code here\n\n\nCheck the relevant conditions for Exercise 9. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\n\nInference for a prediction\n\nBased on your model, predict the tip for a party of 4.\n\n\n# add your code here\n\n\nSuppose you’re asked to construct a confidence and a prediction interval for your finding in Exercise 11. Which one would you expect to be wider and why? In your answer clearly state the difference between these intervals.\nNow construct the intervals from Exercise 12 and comment on whether your guess is confirmed.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#model-diagnostics",
    "href": "ae/ae-4-exam-1-review.html#model-diagnostics",
    "title": "AE 4: Exam 1 Review",
    "section": "Model diagnostics",
    "text": "Model diagnostics\n\nLeverage (Outliers in x direction)\n\nWhat is the threshold used to identify observations with high leverage? Calculate the threshold and save the value as leverage_threshold.\n\n\n# add your code here\n\n\nMake a plot of the standardized residuals vs. leverage (you can do this with ggplot() or with autoplot(which = 5)). Use geom_vline() to add a vertical line to help identify points with high leverage.\n\n\n# add your code here\n\n\nLet’s dig into the data further. Which observations have high leverage? Why do these points have high leverage?\n\n\n# add your code here\n\n\n\nIdentifying outliers (outliers in y direction)\n\nMake a plot of the residuals vs. fitted values and a plot of the square root of the absolute value of standardized residuals vs. fitted (You can use autoplot(which = c(1, 3)) to display the plots side-by-side).\n\n\nHow are the plots similar? How do they differ?\nWhat is an advantage of using the plot of the residuals vs. fitted to check conditions and model diagnostics?\nWhat is an advantage of using the plot of the \\(\\sqrt{|\\text{standardized residuals}|}\\) vs. fitted to check conditions and model diagnostics?\n\n\n# add your code here\n\n\nAre there any observations that are outliers?\n\n\n# add your code here\n\n\n\nCook’s distance\n\nMake a plot to check Cook’s distance (autoplot(which = 4)). Based on this plot, are there any points that have a strong influence on the model coefficients?\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#adding-another-variable",
    "href": "ae/ae-4-exam-1-review.html#adding-another-variable",
    "title": "AE 4: Exam 1 Review",
    "section": "Adding another variable",
    "text": "Adding another variable\n\nAdd another variable, Alcohol, to your exploratory visualization. Describe any patterns that emerge.\n\n\n# add your code here\n\n\nFit a multiple linear regression model predicting Tip from Party and Alcohol. Display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret each of the slopes.\nDoes it make sense to interpret the intercept? Explain your reasoning.\nAccording to this model, is the rate of change in tip amount the same for various sizes of parties regardless of alcohol consumption or are they different? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-10.html",
    "href": "ae/ae-10.html",
    "title": "Exam 2 revision",
    "section": "",
    "text": "Section A:\n\n\n\n\n\n\n\n\nExpected value of \\(X\\)\n\na. \\(E(X - \\mu)\\)\n\n\n\nVariance of \\(X\\)\n\nb. \\(10X\\)\n\n\n\nStandard deviation of \\(X\\)\n\nc. \\(\\frac{(X-0.04)}{\\sigma}\\)\n\n\n\nShorthand notation for \\(P(X=x)\\)\n\nd. \\(X+10\\)\n\n\n\nHas 10 times the standard deviation of \\(X\\)\n\ne. \\(E(X-\\mu)^2\\)\n\n\n\nIs always equal to zero\n\nf. \\(\\sqrt{VarX}\\)\n\n\n\nIncreases the mean of \\(X\\) by 10\n\ng. \\(\\mu\\)\n\n\n\nHas standard deviation 1\n\nh. \\(p(x)\\)\n\n\n\nPlease review AE-8, the solutions are on the Canvas page.\nSection B:\n\n\n\n\n\n\n\n\nConsequence of covariance\n\na. \\(p(x,y)\\)\n\n\n\nCovariance between \\(X\\) and \\(Y\\)\n\nb. \\(\\rho\\)\n\n\n\nProperty of uncorrelated random variables\n\nc. \\(p(x, y) = p(x)p(y)\\)\n\n\n\nWeighted sum of two random variables\n\nd. \\(\\rho\\sigma_x\\sigma_y\\)\n\n\n\nSharpe ratio of random variable\n\ne. \\(Var(X+Y) >Var(X) + Var(Y)\\)\n\n\n\nImplies \\(X\\) and \\(Y\\) are independent random variables\n\nf. \\(p(x) = p(y)\\)\n\n\n\nImplies \\(X\\) and \\(Y\\) are identically distributed\n\ng. \\(X_1, X_2, X_3\\)\n\n\n\nSymbol for the correlation between random variables\n\nh. \\(Var(X+Y) = Var(X) + Var(Y)\\)\n\n\n\nSymbol for joint probability distribution\n\ni. \\(S(Y)\\)\n\n\n\nSequence of iid random variables\n\nj. \\(3X-2Y\\)\n\n\n\nPlease review AE-9, the solutions are on the Canvas page.\nSection C:\n\\(Y\\) is a binomial random variable with parameters \\(n\\) and \\(p\\), \\(X\\) is a Poisson random variable with parameter \\(\\lambda\\) and \\(B_1\\) and \\(B_2\\) are Bernoulli trials with probability of success \\(p\\).\n\n\n\n\n\n\n\n1. Expresssion for the variance of \\(Y\\)\na. \\(np\\)\n\n\n\n\n\nProbability that the first trial \\(B_1\\) fails\n\nb. \\(e^{-\\lambda}\\)\n\n\n\nExpression for the mean of \\(X\\)\n\nc. \\(e^{-\\lambda} \\frac{\\lambda^2}{2}\\)\n\n\n\nProbability that \\(Y\\) is zero\n\nd. \\(p\\)\n\n\n\nProbability that \\(X\\) is 2\n\ne. \\(0\\)\n\n\n\nCovariance between \\(B_1\\) and \\(B_2\\)\n\nf. \\(p^n\\)\n\n\n\nExpression for the expected value of \\(X\\)\n\ng. \\(1-p\\)\n\n\n\nThe expected value of \\(B_1\\)\n\nh. \\((1-p)^n\\)\n\n\n\nProbability that \\(Y\\) is \\(n\\)\n\ni. \\(np(1-p)\\)\n\n\n\nProbability that \\(X\\) is \\(0\\).\n\nj. \\(\\lambda\\)\n\n\n\n\nAn auditor inspects 25 transactions processed by the business office of a company. The auditor selects these transactions at random from the thousands that were processed in the most recent three months. In the past, the auditor has found 10% of transactions at this type of company to have been processed incorrectly. True or False: A binomial model would be more appropriate for this problem if the auditor picked the first 25 transactions during the three-month period.\nIs the binomial model suited to these applications?\n\nThe next five cars that enter a gasoline filling stations get a fill-up.\nA poll of the 15 members of the board of directors indicates that 6 are in favor of a proposal to change the salary of the CEO.\nA company realizes that 10% of its packages are not being sealed properly. When examining a case of 24, it counts the number that are unsealed.\n\nEvery now and then even a good diamonds cutter has a problem and the diamond shatters when being cut. For one cutter, the chance of such errors is 0.1%.\n\nWhat is the probability model seems well suited to this problem? Why?\nIf this cutter works on 75 stones, what is the probability that he breaks 2 or more?\n\nA dairy farmer accidentally allowed some of his cows to graze in a pasture containing weeds that would contaminate the milk from his herd. The farmer estimates that there’s a 10% chance of a cow grazing on some of the flavorful weeds.\n\nUnder these conditions, what is the probability that none of the 12 animals in this herd ate the weeds.\nDoes the poisson model give a good estimate of the probability that no animal ate the weeds?\n\n\nSection D:\n\\(X\\) denotes a normally distributed random variable: \\(X \\sim N(\\mu, \\sigma^2)\\). A googol, the namesake of Google, is \\(10^{100}\\). The random variable \\(Z\\) denotes a standard normal random variable, \\(Z \\sim N(0,1)\\)\n\n\n\n\n\n\n\n\nMean of \\(X\\)\n\na. \\(\\frac{1}{2}\\)\n\n\n\nVariance of \\(X\\)\n\nb. \\(P(Z<1)\\)\n\n\n\nProbability of \\(X\\) being less than its mean\n\nc. \\(0.05\\)\n\n\n\nProbability of \\(X\\) being less than \\(\\mu +\\sigma\\)\n\nd. \\(\\frac{2}{3}\\)\n\n\n\nStandard deviation of \\(Z\\)\n\ne. \\(\\frac{1}{1 googol}\\)\n\n\n\nProbability that a \\(z-score\\) based on \\(X\\) is less than \\(1\\) in magnitude\n\nf. \\(\\mu\\)\n\n\n\nProportion of a normal distribution that is more than \\(20\\sigma\\) from \\(\\mu\\)\n\ng. \\(\\sigma^2\\)\n\n\n\nDifference between value of \\(P(Z<-x)\\) and value of \\(P(Z>x)\\)\n\nh. \\(1\\)\n\n\n\nDistribution of the random variable \\(\\mu +\\sigma Z\\)\n\ni. \\(0\\)\n\n\n\nProbability that \\(Z>1.96\\) plus the probability that \\(Z<-1.96\\)\n\nj. \\(N(\\mu, \\sigma^2)\\)\n\n\n\n\nThe currently age (in years) of 400 clerical employees at an insurance claims processing center is normally distributed with mean 38 and SD 6. True or False: A training program for employees under the age of 30 at the center would be expected to attract about 36 employees.\nIf \\(X_1 \\sim N(\\mu, \\sigma)\\) and \\(X_2 \\sim N(\\mu, \\sigma)\\) are iid, then what is the distribution of \\(\\frac{X_1-X_2}{\\sqrt{2}\\sigma}\\)\nA contractor built 30 similar homes in a suburban development. The homes have comparable size and amenities, but each has features that customize the appearance, landscape, and interior. The contractor expects the homes to sell for $450,000. He expects that one-third of the homes will sell either for less than $400,000 or more than $500,000.\n\nWould a normal model be appropriate to describe the distribution of sale prices?\nWhat data would help you decide if a normal model is appropriate? (These homes are unsold, their prices are unavailable)\nWhat normal model has properties that are consistent with the intuition of the contractor?\n\nA hurricane bond pays the holder a face amount, say $1 million, if a hurricane causes major damage in the United States. Suppose that the chance for such a storm is 5% per year.\n\nIf a financial firm sells these bonds for $60,000, what is the chance that the firm loses money if it only sells one?\nIf the firm sells 1,000 of these policies, each for $60,000, what is the probability that it loses money.\nHow does the difference between the probabilities of parts a and b compare to the situation of an insurance company that writes coverage to homeowners who have accidents independently of one another?\n\n\nSection E:\n\n\n\n\n\n\n\n\nSample\n\na. A complete collection of items desired to be studied\n\n\n\nCensus\n\nb. A list of all the items in the population\n\n\n\nTarget population\n\nc. A subset of a larger collection of items\n\n\n\nStatistic\n\nd. A homogeous subset of the population\n\n\n\nParameter\n\ne. A characteristic of a sample\n\n\n\nSampling frame\n\nf. Occurs if a sampling method distorts a property of the population\n\n\n\nSimple random sample\n\ng. A comprehensive study of every item of the population\n\n\n\nStratum\n\nh. The result if a respondent chooses not to answer a question\n\n\n\nBias\n\ni. A characteristic of a population\n\n\n\nNonresponse\n\nj. Sample chosen so that all subset of size \\(n\\) are equally likely.\n\n\n\n\nTrue or False: Bias due to the wording of questions causes different samples to present different impressions of the population.\nA school district has requested a survey be conducted on the socioeconomic status of their students. Their budget only allows them to conduct the survey in some of the schools, hence they need to first sample a few schools. Students living in this district generally attend a school in their neighbourhood. The district is broken into many distinct and unique neighbourhoods, some including large single-family homes and others with only low-income housing. What kind of sampling did they employ?\n\nSection F:\n\nWhich of the following \\(X\\)-bar charts indicate a process that is out of control?\n\nWhich of the following \\(X\\)-bar charts show that a process went out of control?\n\n\nWhich, if any, of these combinations of an \\(X\\)-bar and an \\(S\\)-chart suggest a problem? If there’s a problem, in which chart did you find the problem?\n\nWhich, if any, of these combinations of an \\(X\\)-bar and an \\(S\\)-chart suggest a problem? If there’s a problem, in which chart did you find the problem?\n\nThe manager of a warehouse monitors shipments. A random sample of 25 packages is selected and weighed every day. The mean weight should be \\(\\mu = 22\\) pounds, and \\(\\sigma = 5\\) pounds. True or False: An \\(X\\)-bar chart with control limits 12 pounds and 32 pounds has a 5% chance of a Type I error.\nRather than stop the production when a mean crosses the control limits in the \\(X\\)-bar chart, a manager has decided to wait until two consecutive means lie outside the control limits before stopping the process. The control limits are \\(\\mu \\pm 2\\sigma/ \\sqrt{n}\\).\n\nBy waiting for two consecutive sample means to lie outside the control limits, has the manager increased or decreased the chance for a Type I error?\nWhat is the probability of a Type I error if this procedure is used?\n\nWhere should the control limits for an \\(X\\)-bar chart be placed if the design of the process sets \\(\\alpha = 0.0027\\) with the following parameters (assume that the sample size condition for control charts has been verified)?\n\n\\(\\mu = 10, \\sigma = 5,\\) and \\(n = 18\\) cases per batch\n\\(\\mu = -4, \\sigma = 2,\\) and \\(n = 12\\) cases per batch"
  },
  {
    "objectID": "ae/ae-6.html",
    "href": "ae/ae-6.html",
    "title": "Revision 1 for exam 1",
    "section": "",
    "text": "What tidyverse function would you use to extract rows?\na. filter()\n\n\n\ncount()\nselect()\ndistinct()\n\n\n\nWhat tidyverse function would you use to extract colums?\n\n\n\nfilter()\ncount()\nc. select()\ndistinct()\n\n\n\nTo find the maximum number of times that a categorical variable appears which function should I use?\n\n\n\nfilter()\nb. count()\nselect()\ndistinct()\n\n\n\nWhat does the “%>%” operator do?\n\na. used in tidyverse between layers of dplyr coding\nb. used in ggplot to add more layers of coding\n\n\nWill this code run without an error? diamonds %>% mutate(price_hundred = price %/% 100)\n\n\n\nIt will produce an error\nb. It will not produce an error\n\n\n\nWhat does the binwidth input in ggplot do?\n\n\n\nIt controls the opacity of the plot\nIt can be used to update the labels\nc. It controls the smoothness of a density plot\nIt controls the size of dots\n\n\n\nWhat punctuation must be placed before a function or dataset to access more information?\n\n\n\n“.”\n“+”\nc. “?”\n“!”\n\n\n\nWhere should the variable names go in the following line of code: ggplot(data = A) + B(mapping = aes(C))\n\n\n\nA\nB\nc. C\n\n\n\nWhat does the here package do?\n\n\n\nfor data manipulation and display\nb. to organize files\nnone of the above\n\n\n\nWhich of the following variables are discrete?\na. number of employees in a company.\n\n\n\ndistance traveled to and from work\ntaxes paid in 2019\npurchases from 2018\n\n\n\nWhat is the difference between distinct() and count()?\n\n\n\nthey are the same\nb. distinct() gives the levels in a variable, count() gives the marginal distribution\ndistinct() gives the marginal distribution, count() gives the levels in a variable\n\n\n\nWhat operator is used in ggplot?\n\n\n\n“.”\nb. “+”\n“?”\n“!”\n\nCategorical variables\n\nFind the expected count for clarity VS1 and color I for a \\(\\chi^2\\) distribution assuming independence.\n\n\\(8171\\times 5422/53940\\)\n\n\n# A tibble: 8 × 10\n  clarity     D     E     F     G     H     I     J marginal_clarity marginal_…¹\n  <ord>   <int> <int> <int> <int> <int> <int> <int>            <dbl>       <dbl>\n1 I1         42   102   143   150   162    92    50              741        6775\n2 SI2      1370  1713  1609  1548  1563   912   479             9194        9797\n3 SI1      2083  2426  2131  1976  2275  1424   750            13065        9542\n4 VS2      1697  2470  2201  2347  1643  1169   731            12258       11292\n5 VS1       705  1281  1364  2148  1169   962   542             8171        8304\n6 VVS2      553   991   975  1443   608   365   131             5066        5422\n7 VVS1      252   656   734   999   585   355    74             3655        2808\n8 IF         73   158   385   681   299   143    51             1790       53940\n# … with abbreviated variable name ¹​marginal_color\n\n\n[1] 821.3415\n\n\n\nWhat proportion of the very good cut diamonds are colored I?\n0.0997\n\n\n\n# A tibble: 5 × 8\n  cut            D      E      F      G      H      I      J\n  <ord>      <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 Fair      0.0241 0.0229 0.0327 0.0278 0.0365 0.0323 0.0424\n2 Good      0.0977 0.0952 0.0953 0.0771 0.0845 0.0963 0.109 \n3 Very Good 0.223  0.245  0.227  0.204  0.220  0.222  0.241 \n4 Premium   0.237  0.239  0.244  0.259  0.284  0.263  0.288 \n5 Ideal     0.418  0.398  0.401  0.433  0.375  0.386  0.319 \n\n\n\n\n# A tibble: 5 × 8\n# Groups:   cut [5]\n  cut           D     E     F     G     H      I      J\n  <ord>     <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl>\n1 Fair      0.101 0.139 0.194 0.195 0.188 0.109  0.0739\n2 Good      0.135 0.190 0.185 0.178 0.143 0.106  0.0626\n3 Very Good 0.125 0.199 0.179 0.190 0.151 0.0997 0.0561\n4 Premium   0.116 0.169 0.169 0.212 0.171 0.104  0.0586\n5 Ideal     0.132 0.181 0.178 0.227 0.145 0.0971 0.0416\n\n\n\nWhich of the lines below is the mean, median, or mode?\n\n\n\n\n\n\n\nred is median, green is mean\nred is mean, green is mode\nred is mode, green is mean\nd. red is mean, green is median\n\n\n\nIs the mean or median a better measure of center in this distribution.\n\nThe mean is a better estimate in a a symmetric distribution, median in a skewed distribution\n\n\n\n\n\n\nWhich plot is best at displaying outliers:\n\n\n\nhistogram\ndotplot\nplot of proportions\nd. boxplot"
  },
  {
    "objectID": "ae/ae-5.html",
    "href": "ae/ae-5.html",
    "title": "The birthday simulation (AE-5)",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 14 Oct at 2:00pm.\n\n\nComputers let you assemble, manipulate, and visualize data sets, all at speeds that would have wowed yesterday’s scientists. In short, computers give you superpowers! But if you wish to use them, you’ll need to pick up some programming skills. Steve Job said that “computers are bicycles for our minds” because the efficiency rating of humans on bicycles is so incredible, and computers give us similar powers.\nOne reason computers are so incredible is that they allow us to simulate a multitude of events. Today we will simulate the probability that two of you in this section of Stat1010 have the same birthdays.\n\nlibrary(tidyverse) # for data manipulation\nlibrary(vctrs) # to find the length of a tibble\n\nSuppose you are in a classroom with 100 people. If we assume this is a randomly selected group of 100 people, what is the chance that at least two people have the same birthday? Although it is somewhat advanced, we can deduce this mathematically. We will do this later. Here we use a Monte Carlo simulation. For simplicity, we assume nobody was born on February 29. This actually doesn’t change the answer much.\nFirst, note that birthdays can be represented as numbers between 1 and 365, so a sample of 100 birthdays can be obtained like this:\n\nall_bdays <- as_tibble(1:365) # data from where we can sample\n\nbdays_class <- \n  all_bdays %>% # data from where to sample\n  slice_sample(n = 100, replace = TRUE) # sample of size 100 with replacement\n\nTo check if in this particular set of 100 people we have at least two with the same birthday, we can use the functions group_by and filter, which returns a tibble with a vector of duplicated dates. Here is an example:\n\nbdays_class %>% \n  group_by(value) %>% # for each birthday\n  count() %>% # count them\n  filter(n > 1) # those have more than 1\n\nTo estimate the probability of a shared birthday in the group, we repeat this experiment by sampling sets of 100 birthdays over and over. Prior to replicating, we need to write this as a function.\n\nbdays_dups <- \n  all_bdays %>% # data from where to sample\n  slice_sample(n = 100, replace = TRUE) %>% # take a sample of n 100\n  group_by(value) %>% # for each birthday\n  count() %>% # count them\n  filter(n > 1) %>% # those have more than 1\n  vec_n(.) # the number that are duplicated\n\nA function has an input (in this case \\(n\\))\n\nWhat does \\(n\\) represent in this coding?\n\n\nnum_of_same_birthdays <- function(n){\n  all_bdays %>% # data from where to sample\n  slice_sample(n = n, replace = TRUE) %>% # take a sample\n  group_by(value) %>% # for each birthday\n  count() %>% # count them\n  filter(n > 1) %>% # those have more than 1\n  vec_size(.)  # the number that are duplicated\n}\n\nTo run this function, we do this:\n\nnum_of_same_birthdays(10) # run the function\n\n\nHow many students are there in class today?\nRun the coding above but include the number of students in class today.\n\nThe law of large numbers says that if we do this many times we should have an estimate of the number of people that have the same birthdays in class today. The function replicate can be used to run functions multiple times.\n\nB <- 500 # the number of times to run\nresults <- replicate(B, # replicate this number of times\n          ifelse( # if there are some duplicated birthdays\n            num_of_same_birthdays(50) >= 1, # our function\n            1, # give me a 1\n            0)) # if not, give me a 0\n\nmean(results) # take the mean\n\nWere you expecting the probability to be this high?\nPeople tend to underestimate these probabilities. To get an intuition as to why it is so high, think about what happens when the group size is close to 365. At this stage, we run out of days and the probability is one.\nSay we want to use this knowledge to bet with friends about two people having the same birthday in a group of people. When are the chances larger than 50%? Larger than 75%?\nLet’s create a look-up table. We can create a function to compute this for any group size. This function runs our function \\(500\\) times for every value of $n$. Statisticians will usually do this \\(10000\\) times, but this will take a very very very very looooooooooonnnnnggggg time, so we are simplifying.\n\ncompute_prob <- # name the function\n  function(n, B = 500){ # function inputs\n  results <- # store results\n    replicate(B, # run num_of_same_birthdays B times \n              ifelse( # if there are some duplicated birthdays\n            num_of_same_birthdays(n) >= 1, # our function\n            1, # give me a 1\n            0)) # if not, give me a 0\n  \n  mean(results) # find the mean\n}\n\nUsing the function map_dbl, we can perform element-wise operations on any function. Note that this may take awhile to run.\n\nn <- seq(1, 60) # which values of n are important\nprob <- # save the results as prob\n  map_dbl(n,  # for each value of n \n          compute_prob) # run the function\n\nWe can now make a plot of the estimated probabilities of two people having the same birthday in a group of size n:\n\nas_tibble(n, prob) %>% # the format for ggplot\n  ggplot() + # draw a graph\n  geom_point(aes(x = n, y = prob)) # of points\n\nNow let’s compute the exact probabilities rather than use Monte Carlo approximations. Not only do we get the exact answer using math, but the computations are much faster since we don’t have to generate experiments.\nTo make the math simpler, instead of computing the probability of it happening, we will compute the probability of it not happening, or the compliment. For this, we use the multiplication rule.\nLet’s start with the first person. The probability that person 1 has a unique birthday is 1. The probability that person 2 has a unique birthday, given that person 1 already took one, is 364/365. Then, given that the first two people have unique birthdays, person 3 is left with 363 days to choose from. We continue this way and find the chances of all \\(n\\) people having a unique birthday is:\n\\[1×\\frac{364}{365}×\\frac{363}{365}…\\frac{365−n+1}{365}\\] We can write a function that does this for any number:\n\nexact_prob <- function(n){\n  1 - \n    prod(365:(365-n+1))/ # the product from the numerator above\n    365^(n) # the denominator from above\n}\n\n\nRun this coding for the number of people that are in class today.\n\nNow, we run this on multiple values of \\(n\\)\n\neprob <- map_dbl(n, exact_prob) # run on multiple values of n\n\nNext we plot the results.\n\nas_tibble(n, eprob) %>% # the format for ggplot\n  ggplot() + # draw a graph\n  geom_point(aes(x = n, y = eprob)) # of points\n\nOn mercury, the year is only 88 days. Update the coding above to simulate the expected number of people with the same birthdays on Mercury.\nAssuming we have the same number of people, would you expect the number of people with the same birthdays to be higher or lower than those on earth?"
  },
  {
    "objectID": "ae/ae-9.html",
    "href": "ae/ae-9.html",
    "title": "Associations AE-9",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 21 Oct at 2:00pm.\n\n\n\nlibrary(tidyverse) # for data manipulation and plots\nlibrary(praise) # for good vibes\n\nChapter 6: Association between quantitative variables\n\nIf the covariance between \\(x\\) and \\(y\\) is \\(0\\), then the correlation between \\(x\\) and \\(y\\) is \\(0\\) as well\n\nTrue\nFalse\n\nA retailer calculated the correlation line between the price of an item (\\(x\\)) and the amount sold (\\(y\\)). The correlation line is the same if the \\(x\\) and \\(y\\) variables are exchanged.\n\nTrue\nFalse\n\n\n\npraise()\n\n\nIf the correlation between number of customers and sales in dollars in retail stores is \\(r=0.6\\), then what would be the correlation if the sales were measured in thousands of dollars? In euros? (1 euro is worth about \\(\\$US0.97\\))\n\nChapter 10: Association between random variables\n\nMix and match\n\n\n\n\n\n\n\n1. Consequence of positive covariance\na. \\(p(x, y)\\)\n\n\n\n\n\nCovariance between X and Y\n\nb. \\(\\rho\\)\n\n\n\nProperty of uncorrelated random variables\n\nc. \\(p(x, y) = p(x)p(y)\\)\n\n\n\nWeighted sum of two random variables\n\nd. \\(\\rho \\sigma_X \\sigma_Y\\)\n\n\n\nSharpe ratio of a random variable\n\ne. \\(Var(X+Y) > Var(X) +Var(Y)\\)\n\n\n\nImplies \\(X\\) and \\(Y\\) are independent random variables\n\nf. \\(p(x) = p(y)\\)\n\n\n\nImplies \\(X\\) and \\(Y\\) are identically distributed\n\ng. \\(X_1, X_2, X_3\\)\n\n\n\nSymbol for correlation between random variables\n\nh. \\(Var(X, Y) = Var(X) +Var(Y)\\)\n\n\n\nSymbol for a joint probability distribution\n\ni. \\(S(Y)\\)\n\n\n\nSequence of iid random variables\n\nj. \\(3X - 2Y\\)\n\n\n\nIndependent random variables \\(X\\) and \\(Y\\) have the means and standard deviations as given in the following table. Use these parameters to find the expected value and SD of the following random variables that are derived from \\(X\\) and \\(Y\\)\n\n\\(2X - 100\\)\n\\(0.5Y\\)\n\\(X+Y\\)\n\\(X-Y\\)\n\n\n\n\nMean\nSD\n\n\n\\(X\\)\n1000\n200\n\n\n\\(Y\\)\n2000\n600\n\n\n\n\n\n\npraise()\n\n\nRepeat the calculations above but now \\(X\\) and \\(Y\\) are not independent and have \\(Cov(X, Y) = 12,500\\).\nWhat’s the covariance between a random variable \\(X\\) and a constant?\nA student budgets $60 weekly for gas and quick meals off-campus. Let \\(X\\) denote the amount spent for gas and \\(Y\\) the amount spent for quick meals in a typical week. Assume the student sticks to the budget.\n\nCan we model \\(X\\) and \\(Y\\) as independent random variables? Why or why not?\nSuppose we assume \\(X\\) and \\(Y\\) are dependent. What is the effect of this dependence on the variance of \\(X+Y\\)?"
  },
  {
    "objectID": "ae/ae-14.html",
    "href": "ae/ae-14.html",
    "title": "Downloading plots from R",
    "section": "",
    "text": "Load packages, data\n\n\n\nMake a plot\n\nggplot(data = diamonds, \n       mapping = aes(x = carat, \n                     y = price, \n                     color = cut)) +\n  geom_point() + \n  theme_minimal() # add a theme of your choice!\n\n\n\n\nExport to add to your presentation."
  },
  {
    "objectID": "ae/ae-12.html",
    "href": "ae/ae-12.html",
    "title": "Chi-squared test: AE - 11",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 21 Nov at 5:00pm.\n\n\nMatch each item on the left with its correct description on the right.\n\nNumber of degrees of freedom in the chi-squared test of independence in a 2 × 2 table\nNumber of constraints on frequencies in the chi-squared test of goodness of fit of a binomial distribution\nP-value if \\(\\chi^2\\) = 9.488 when testing for independence in a 3 × 3 table\nP-value if \\(\\chi^2\\) = 16.812 when testing the null hypothesis that a categorical variable with 7 levels has a uniform distribution\nSmallest possible value of \\(\\chi^2\\)\nImpossible value for \\(\\chi^2\\)\nReject the null hypothesis of independence in a 3 × 4 contingency table if \\(\\chi^2\\) is larger than this value and \\(\\alpha = 0.05\\)\nReject the null hypothesis of independence in a 6 × 3 contingency table if \\(\\chi^2\\) is larger than this value and \\(\\alpha = 0.01\\)\nThe expected cell count in a table should be larger than this number when using \\(\\chi^2\\)\nNumber of degrees of freedom if using chi-squared to test whether four proportions are the same\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na. 0\nb. 0.01\nc. 23.209\nd. 10\ne. 1\nf. 0.05\ng. 3\nh. 2\ni. 12.592\nj. -1\n\n\n\nTrue/False\n\nThe chi-squared test of independence only detects linear association between the variables in a contingency table.\nA statistically significant \\(\\chi^2\\) in the test of independence implies a causal relationship between the variables that define a contingency table.\nThe expected size of the chi-squared statistic \\(\\chi^2\\) increases with the number of observations \\(n\\) in the table.\nA stock market analyst recorded the number of stocks that went up or went down each day for 5 consecutive days, producing a contingency table with two rows (up or down) and five columns (Monday through Friday). Are these data suitable for applying the chi-squared test of independence?\nThe human resources group regularly interviews prospective clerical employees and tests their skill at data entry. The following table shows the number of errors made by 60 prospective clerks when entering a form with 80 numbers. (Five clerks made four errors.) Test the null hypothesis that the number of errors follows a Poisson distribution. Find the\n\n\n\nerrors\nClerks\n\n\n\n\n0\n12\n\n\n1\n20\n\n\n2\n9\n\n\n3\n14\n\n\n4 or more\n5\n\n\n\na. Rate (or mean) of the Poisson distribution\nb. Expected number of employees who do not make an error\nc. Degrees of freedom of \\(\\chi^2\\)\nd. \\(\\chi^2\\)\ne. p-value for testing H0."
  },
  {
    "objectID": "ae/ae-2-exploring-categorical-vars.html",
    "href": "ae/ae-2-exploring-categorical-vars.html",
    "title": "Exploring categorical data AE-2",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 16 Sept at 2:00pm.\n\n\n\nLoad packages, data\n\n## loading libraries\nlibrary(tidyverse) # for data analysis and visualisation\nlibrary(here) # to organize files\nlibrary(praise) # for ocassional good vibes!\n\nLet’s look at the data on the web prior to importing it and also to ensure that we know what it should look like when it is imported into R.\nNow press “raw”\n\nThen copy the url:\n\nDownload the file by inserting the URL below:\n\nfs::dir_create(here(\"data\")) # create a data folder \ndownload.file(\n  url = URL_HERE, # url of file to download\n  destfile = here(\"data/comics.csv\") # directory/name_of_file\n)\n\nThen click on the data folder in the “Files” panel.\n\nDouble click on “comics.csv” and click on “Import Dataset…”\n\nCheck the data in the “Data Preview” pane and ensure that it looks as it should.\n\nNext, click on the clipboard icon to copy the code from the “Code Preview” pane, then click “Import”.\n\n#PASTE THE CODE FROM \"Code Preview\" HERE\n\n\n\nData\nThe comics dataset includes basic information scraped from the superheroDb for a kaggle competition.\n\n## An overview of the dataset and type of variables\nglimpse(comics)\n\nThe first superhero is named “A-Bomb” he is male, with yellow eyes, human, with no hair. Marvel Comics created him and his skin color is not listed, but his height and weight are.\n\nAre there any differences between the data on the website above and the one in R? What are they? (Hint: which function can you use to see the whole dataset?)\n\npraise()\n\nThe ___ dataset has ___ observations and ___ variables.\nClassify all variables in the dataset as either: quantitative discrete, quantitative continuous, qualitative nominal, qualitative ordinal. Which of these variables is also categorical?\n\n\n\nCounts\n\nHow many publishers are there in this dataset?\n\ncomics %>% \n  distinct(Publisher) # finds the levels of publisher\n\nHow many levels of the variable Alignment are there in this dataset?\n\ncomics %>% \n  INSERT_FUNCTION(INSERT_VARIABLE)\n\n\nA common way to represent the number of cases that fall into each combination of levels of two categorical variables, such as these, is with what’s called a “contingency table.” Note that each superhero appears in exactly one cell, statisticians call this mutually exclusive.\n\npraise()\n\n\nWhich is the most common category in this contingency table? How many superheros are in that category? (note: please look at both of the next two blocks of code.)\n\ncomics %>% \n  count(Publisher, Alignment) %>% # counts all combinations\n  View()\n\n\nThe long format above is difficult to see clearly. The format we normally use for contingency tables is the wide format.\nThe third line of coding below takes a long table and makes it wide by moving the Alignment variable to the columns, and filling the values of the table with the count variable (n). The names_from argument tells R where the names of the new columns are coming from (i.e. what variable), and the values_from argument tells R where the values in the table are coming from. Here, the values we want in our table are stored as a variable labeled n in our table.\n\ncomics %>% \n  count(Publisher, Alignment) %>%  # counts all combinations\n  pivot_wider(names_from = Alignment, \n              values_from = n) # pivots from long to wide \n\n\nThere are about ___ good superheros for each bad superhero.\n\nggplot(data = comics) + # the dataset\n  geom_bar(mapping = aes(x = Alignment)) # a bar chart of Alignment\n\nLook at the next two plots and find the difference between them. What would make you use one plot over the other? Which plot shows that females are much more likely to be good superheros than bad? Which one shows that good superheros are more likely to be missing gender? And which that females are more likely to be good superheros than males? All of these mean that the variables gender and alignment are associated, the value of one impacts upon the value of the other.\n\nggplot(data = comics) + # add the data\n  geom_bar(mapping = aes(x = Gender, # bar chart of gender\n                         fill = Alignment)) +  # colored by alignment\n  labs(title = \"Gender colored with alignment\") # add title\n\n\nggplot(data = comics) + # add the data\n  geom_bar(mapping = aes(x = Alignment, # bar chart of alignment\n                         fill = Gender)) + # colored by gender\n  labs(title = \"Alignment colored with gender\")  # add title\n\n\nWe note that the commonly held belief that gender is binary is no longer the scientific consensus and that gender is a complex spectrum.\n\nStatisticians want to discuss the strength of association of variables. They want to ensure that this association is not the result of random noise, but instead is a function of the underlying population of interest (all superheros).\n\npraise()\n\n\n\nChi-Squared test\nTo test for independence in 2 categorical variables, statisticians use a Chi-squared test. This is a hypothesis test so it has both a null hypothesis (\\(H_0\\)) and an alternative hypothesis (\\(H_A\\)).\n\\(H_0\\) = the variables are independent, there is no relationship between the two categorical variables.\n\\(H_A\\) = Knowing the value of one variable helps to predict the value of the other variable\nJust like a fair court case, where a defendant is assumed to be innocent until proven guilty, here we assume \\(H_0\\) until proven otherwise. The proof that we present at the case is a formula relating the actual values in our sample to what we would expect to get under the independence assumption.\nTo perform a Chi-squared test of independence on gender and alignment we must find the marginal distributions of both gender and alignment.\n\n\nMarginal distribution\n\nThe marginal distribution is the count of each variable.\n\ncomics %>% \n  count(Alignment, Gender) %>% \n  pivot_wider(names_from = Alignment, \n              values_from = n)\n\n\nWe take the original two-way table (with two categorical variables) and added up the cells across each level of align (ie \\(1+7+19+2 = 29\\)) to get the marginal distribution for each variable.\n\ncomics %>% # the dataset\n  count(Gender) # and marginal distribution of gender\n\n\npraise()\n\n\ncomics %>% # the dataset\n  count(Alignment) # and marginal distribution of the Alignment\n\nIf these are independent, we would expect \\(\\frac{200}{29+200+505}\\) of the \\(207\\) (about \\(56.4\\)) bad superheros to be female, and the same proportion of \\(496\\) (about \\(135.1\\)) good superheros to be female, and so on.\n\nAssuming independence of Alignment and Gender, how many good superheros would we expect to be male? (hint: use the “$$” to make\\(\\frac{1}{2}\\))\n\nMore information, including mathematical notation can be found here.\nFortunately, `R` does all of this for us.\n\n# the \"$\" sign pulls out the Gender values and Alignment value from the comics dataset\nchisq.test(comics$Gender, comics$Alignment)\n\nWe get a warning because the missing values (“-”) expected counts are probably quite small. You can safely use the chi-square test with critical values from the chi-square distribution when no more than 20% of the expected counts are less than \\(5\\) and all individual expected counts are 1 or greater. In particular, all four expected counts in a \\(2 \\times 2\\) table should be 5 or greater.\n\npraise()\n\nThe \\(p-value\\) for this test \\(0.0003298\\) is very small which means that we have very strong evidence against \\(H_0\\) — that there is independence between Alignment and Gender — and we can reject the null hypothesis. This suggests that the association between Gender and Alignment is not likely due to chance.\n\nThere are a few characters with missing data “-” in the Alignment and Gender variables? How many are in each?\n\ncomics %>% \n  INSERT_FUNCTION(INSERT_VARIABLE)\n\n\ncomics %>% \n  INSERT_FUNCTION(INSERT_VARIABLE)\n\nSince there are only a few missing values (<10%) in each category, there is no reason to keep them especially since our analysis is largely about these two variables.\nWhat does the “!=” operator do?\n\ncomics_filtered <- # assign a name to a new dataset\n  comics %>% # tell R which dataset to start with\n  INSERT_FUNCTION(Alignment != \"-\", # remove \"-\" values in Alignment\n                  Gender != \"-\") # remove \"-\" values in Gender\n\nRemake the “Alignment colored with gender” plot and the “Gender colored with alignment” plots with this new dataset.\nRedo the Chi-Squared test for independence between Alignement and Gender. Include all hypotheses, and computations for at least \\(2\\) expected values. Does this change our conclusion?\n\npraise()\n\nSide-by-side barcharts allow us to represent the counts from a contingency table graphically. Telling R to make a side-by-side barchart involves adding the words position = \"dodge” to the coding from above. Now created the other bar chart side-by-side, the “Gender colored with alignment.” use the comics_filter dataset.\n\nggplot(data = INSERT) + # add the data\n  geom_bar(mapping = aes(x = INSERT, # bar chart of alignment\n                         fill = INSERT), # colored by gender\n           position = INSERT) + # side_by_side\n  labs(title = \"Alignment colored with gender\")  # add title\n\n\n\n\nProportions\nWe have been focusing mostly on counts. Proportional data is also interesting to explore and it is easy to get R to produce those for us.\n\nWhat combination of Gender and Alignment has the highest proportion? What is the value? What is the sum of all of the proportions in the table below? (Note: Use the View() function to go through each line of code and ensure you know what each step does.)\n\ncomics %>% # which dataset to use \n  count(Gender, Alignment) %>% # counts all combinations\n  mutate(prop = n / sum(n)) %>% # finds proportions\n  select(Gender, Alignment, prop) %>% # selecting which cols to pivot\n  pivot_wider(names_from = Alignment, # the variable to pivot\n              values_from = prop) # which values to pivot\n\n\n\n\nConditional proportions\n\nIf we’re curious about systematic associations between variables, we should look to conditional proportions. An example of a conditional proportion is the proportion of female superheroes that are good. To build a table of these conditional proportions, we need to specify a grouping variable before we calculate the proportions. What proportion of female characters are good? What do the rows sum to? (Note: Use the View() function to go through each line of code and ensure you know what each step does.)\n\ncomics %>% # which dataset to use \n  count(Gender, Alignment) %>% # counts all combinations\n  group_by(Gender) %>% # conditions on gender\n  mutate(prop = n / sum(n)) %>% # finds proportions\n  select(Gender, Alignment, prop) %>%  # selecting which cols to pivot\n  pivot_wider(names_from = Alignment, # the variable to pivot\n              values_from = prop) # which values to pivot\n\nWhat proportion of good characters are male? What do the columns sum to?\n\ncomics %>% # which dataset to use \n  count(Gender, Alignment) %>% # counts all combinations\n  group_by(Alignment) %>% # conditions on gender\n  mutate(prop = n / sum(n)) %>% # finds proportions\n  select(Gender, Alignment, prop) %>%  # selecting which cols to pivot\n  pivot_wider(names_from = Alignment, # the variable to pivot\n              values_from = prop) # which values to pivot\n\n\npraise()\n\nPlotting proportions is similar to the side-by-side bar chart completed earlier. Instead of position = \"dodge\" we use position = \"fill\" . Draw one such plot below using Gender for the x axis and color it using Alignment.\n\nggplot(data = INSERT) + # add the data\n  geom_bar(mapping = aes(x = INSERT, # bar chart of ?\n                         fill = INSERT), # colored by ?\n           position = INSERT) # fill\n\nNow, change the label of the y axis from count by including the coding\nlabs(y = \"Proportion of superheroes\")\n\npraise()\n\nDoes the above plot condition on gender or alignment? How do you know? (hint: what sums to 1?)\nCondition on the other variable.\n\nggplot(data = INSERT) + # add the data\n  geom_bar(mapping = aes(x = INSERT, # bar chart of ?\n                         fill = INSERT), # colored by ?\n           position = INSERT)  + # fill\n  lab(y = \"INSERT Y AXIS TITLE HERE\")\n\nAnother way to view the differences in variables is to facet. What is the advantage of a facet plot over a stacked barchart?\n\nggplot(comics) + # which dataset\n  geom_bar(mapping = aes(x = Gender)) + # bar chart of Gender\n  facet_wrap(~Alignment) # broken down by Alignment"
  },
  {
    "objectID": "ae/ae-13.html",
    "href": "ae/ae-13.html",
    "title": "Exam 3 revision - reduced version",
    "section": "",
    "text": "Some of you asked for the population of all possible questions for the next exam. It is here.\nPlease review AE-10 and AE-11. Some additional questions for your perusal are below:\n\nChapter 16\n\n\n\n\n\n\n\nConcept\nTerm/Symbol\n\n\n\n\n\nOne-sided null hypothesis\n\nt-statistic\n\n\n\nIdentifies the alternative hypothesis\n\nμ0\n\n\n\nMaximum tolerance for incorrectly rejecting H0\n\np-value\n\n\n\nNumber of standard errors that separate an observed statistic from the boundary of H0\n\np-value < α\n\n\n\nNumber of estimated standard errors that separate an observed statistic from the boundary of H0\n\nType I error\n\n\n\nLargest α-level for which a test rejects the null hypothesis\n\nz-statistic\n\n\n\nOccurs if the p-value is less than α when H0 is true\n\nType II error\n\n\n\nOccurs if the p-value is larger than α when H0 is false\n\nα-level\n\n\n\nSymbol for the largest or smallest mean specified by the null hypothesis\n\nH0: μ ≥ 0\n\n\n\nIndicates a statistically significant result\n\nHa,H1\n\n\n\n\nA retailer maintains a Web site that it uses to attract shoppers. The average purchase amount is $80. The retailer is evaluating a new Web site intended to encourage shoppers to spend more. Let μ represent the average amount spent per customer at its redesigned Web site. True or False: If the p-value of the test of H0 is less than α, then the test has produced a Type II error.\nAn accounting firm is considering offering investment advice in addition to its current focus on tax planning. Its analysis of the costs and benefits of adding this service indicates that it will be profitable if 40% or more of its current customer base use it. The firm pans to survey its customer who will use the service if offered, and let p̂ denote the proportion who say in a survey that they will use this service. The firm does not want to invest in the expansion unless data show that it will be profitable. True or False: If p̂ is larger than 0.4, a test will reject the appropriate null hypothesis for this context.\nConsider the following test of whether a coin is fair. Toss the coin three times. If the coin lands either all heads or all tails, reject H0 : p = 1/2. (The p denotes the chance for the coin to land on heads.)\n(a) What is the probability of a Type I error for this procedure?\n(b) If p = 3/4, what is the probability of a Type II error for this procedure? (The null hypothesis remains the same.)\nThe Human Resources (HR) group gives job applicants at a firm a personality test to assess how well they will fit into the firm and get along with colleagues. Historically, test scores have been normally distribution with mean μ and standard deviation σ = 25. The HR group wants to hire applicants whose true personality rating μ is greater than 200 points. (Test scores are an imperfect measure of true personality.)\n(a) Before seeing test result, should the HR group assert as the null hypothesis that μ for an applicants is greater than 200 or less than 200?\n(b) If the HR group chooses H0: μ ≤ 200, then for what test scores (approximately) will the HR group reject H0 if α = 2.5%?\n(c) What is the chance of a Type II error using the procedure in part (b) if the true score of an applicant is 225?\nField test of a low-calorie sport drink found that 80 of the 100 who tasted the beverage preferred it to the regular higher-calorie drink. A break-even analysis indicates that the launch of this product will be profitable if the beverage is preferred by more than 75% of all customers.\n(a) State the null and alternative hypotheses.\n(b) Describe a Type I error and a Type II error in this context.\n(c) Find the p-value for a test of the null hypothesis. If α = 0.10, does the test reject H0?\n\n\n\nChapter 17\n\nPlot used for visual comparison of results in two (or more) groups (a) t = -4.6\nDifference between the averages in two samples (b) t = 1.3\nDifference between the averages in two populations (c) \\(\\mu_1 - \\mu_2\\)\nName given to the variable that specifies the treatments in an experiment (d) \\(\\sqrt{s_1^2/n_1+s_2^2/n_2}\\)\nEstimate of the standard error of the difference between two sample means (e) n - 1\nAvoids confounding in a two-sample comparison (f) \\(\\bar{x}_1 - \\bar{x}_2\\)\nTest statistic indicating a statistically significant result if \\(\\alpha\\) = 0.05 and \\(H_0\\):\\(\\mu_1 - \\mu_2 \\geq 0\\) (g) Confounding\nTest statistic indicating that a mean difference is not statistically significant if \\(\\alpha\\) = 0.05 (h) Randomization\nThe number of degrees of freedom in a paired t-test (i) Factor\nMultiple factors explain the difference between two samples (j) Comparison boxplot\nTrue or False: The t-statistic in a two-sample test does not depend on units.\nA business offers its employees free membership in a local fitness center. Rather than ask the employees if they like this benefit, the company developed a measure of productivity for each employee based on the number of claims handled. To assess the program, managers measured productivity of staff members both before and after the introduction of this program. How should these data be analyzed in order to judge the effect of the exercise program?\nMembers of a sales force were randomly assigned to two management groups. Each group employed a different technique for motivating and supporting the sales team. Let’s label these groups A and B, and let \\(\\mu_A\\) and \\(\\mu_A\\) denote the mean weekly sales generated by members of the two groups. The 95% confidence interval for \\(\\mu_A - \\mu_B\\) was found to be [$500, $2,200].\n(a) If profits are 40% of sales, what’s the 95% confidence interval for the difference in profits generated by the two methods?\n(b) Assuming the usual conditions, should we conclude that the approach taken by Group A sells more than that taken by Group B, or can we dismiss the observed difference as due to chance?\n(c) The manager responsible for Group B complained that his group had been assigned the “poor performers” and that the results were not his fault. How would you respond?\n\n\n\nChapter 19\n\n\n\n\n\n\n\nConcept\nTerm/Symbol\n\n\n\n\n\nSymbol for the explanatory variable in a regression\n\n\\(r^2\\)\n\n\n\nSymbol for the response in a regression\n\nb0\n\n\n\nFitted value from an estimated regression equation\n\nȳ\n\n\n\nResidsual from an estimated regression equation\n\nb1\n\n\n\nIdentifies the intercept in a fitted line\n\nX\n\n\n\nIdentifies the slope in a fitted line\n\nŷ\n\n\n\nPercentage variation described by a fitted line\n\nb0 + b1\n\n\n\nSymbol for the standard deviation of the residuals\n\nY\n\n\n\nPrediction from a fitted line is x = x̄\n\ny - ŷ\n\n\n\nPrediction from a fitted line is x = 1\n\nse\n\n\n\n\nTrue or False: The use of a linear equation to describe the association between price and sales implies that we expect equal differences in sales when comparing periods with prices $10 and $11 and periods with prices $20 and $21.\nTrue or False: The sum of the fitted value \\(\\hat{y}\\) plus the residual \\(e\\) is equal to the original data value \\(y\\).\nThe value of \\(r^2 = 1\\) if data lie along a simple line. Is it possible to fit a linear regression for which \\(r^2\\) is exactly equal to zero?\nCosts for building a new elementary school in the United States average about $100 per square foot. In a review of school construction projects in Arizona, the head of the Department of Education examined a scatterplot of the cost of recently completed schools (Y) versus the size of the school (in square feet, X).\n(a) Would you expect a linear equation to describe these data?\n(b) What would you expect for the intercept of the linear model?\n(c) What would you expect for the slope?\n(d) Do you expect patterns in the variation around the equation?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 1010: Introduction to Business Statistics",
    "section": "",
    "text": "Week\nDate\nTopic\nPrepare\nSlides\nAE\nHW\nExam\nProject\n\n\n\n\n1\nMon, 29 Aug\nNo class - opening exercises\n\n\n\n\n\n\n\n\n\nWed, 31 Aug\nStarting with R\n\n🖥️\n📋\n\n\n\n\n\n2\nMon, 5 Sept\nNo class - Labor Day\n\n\n\n\n\n\n\n\n\nWed, 7 Sept\nContinuation of Intro to R\n📖\n🖥️\n\n\n\n\n\n\n3\nMon, 12 Sept\nCategorical variables\n📖\n🖥️\n📋\n\n\n\n\n\n\nWed, 14 Sept\nNumeric variables\n\n🖥️\n📋\n✍️\n\n\n\n\n4\nMon, 19 Sept\nIntro to probability\n📖\n🖥️\n📋\n✍️\n\n\n\n\n\nWed, 21 Sept\nMore probability rules\n\n🖥️\n📋\n\n\n\n\n\n5\nMon, 26 Sept\nRevision of AE 1-4\n📖\n\n📋\n\n\n\n\n\n\nWed, 28 Sept\nRevision of HW-1\n📖\n\n📋\n\n\n\n\n\n6\nMon, 3 Oct\nExam 1\n\n\n\n\n✅\n\n\n\n\nWed, 5 Oct\nRandom variables\n📖\n🖥️\n📋\n\n\n\n\n\n7\nMon, 10 Oct\nAssociation between quantitative variables\n📖\n🖥️\n\n\n\n\n\n\n\nWed, 12 Oct\nAssociation between random variables\n\n🖥️\n📋\n✍️\n\n\n\n\n8\nMon, 17 Oct\nProbability models for counts\n📖\n🖥️\n\n\n\n\n\n\n\nWed, 19 Oct\nThe normal probability model\n\n🖥️\n\n\n\n\n\n\n9\nMon, 24 Oct\nSamples and surveys\n📖\n🖥️\n\n\n\n\n\n\n\nWed, 26 Oct\nSampling variation and quality\n\n🖥️\n📋\n\n\n📂\n\n\n10\nMon, 31 Oct\nRevision for Exam 2\n📖\n🖥️\n\n\n\n\n\n\n\nWed, 2 Nov\nExam 2\n\n\n\n\n✅\n\n\n\n11\nMon, 7 Nov\nConfidence intervals\n📖\n🖥️\n📋\n\n\n\n\n\n\nWed, 9 Nov\nStatistical tests\n\n🖥️\n\n\n\n\n\n\n12\nMon, 14 Nov\nComparison\n📖\n🖥️\n\n\n\n\n\n\n\nWed, 16 Nov\nInference for counts\n\n🖥️\n📋\n✍️\n\n\n\n\n13\nMon, 21 Nov\nLinear patterns\n📖\n🖥️\n\n\n\n\n\n\n\nWed, 22 Nov\nNo class - Thanksgiving\n\n\n\n\n\n\n\n\n14\nMon, 28 Nov\nRevision\n\n\n📋\n\n\n\n\n\n\nWed, 30 Nov\nExam 3\n\n\n📋\n\n✅\n\n\n\n15\nMon, 5 Dec\nPresentations & Revision\n\n\n📋\n\n\n📂\n\n\n\nWed, 7 Dec\nPresentations & Revision\n\n\n📋\n✍️\n\n📂\n\n\n16\nMon, 12 Dec\nRevision for last midterm\n\n\n📋\n\n\n\n\n\n\nThur, 15 - 19 Dec\nLast midterm - 3 hours online\n\n\n\n\n✅"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "STAT 1010 - Fall 2022",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "repo-structure/proposal/proposal.html",
    "href": "repo-structure/proposal/proposal.html",
    "title": "Project proposal",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)"
  },
  {
    "objectID": "repo-structure/proposal/proposal.html#introduction",
    "href": "repo-structure/proposal/proposal.html#introduction",
    "title": "Project proposal",
    "section": "1. Introduction",
    "text": "1. Introduction"
  },
  {
    "objectID": "repo-structure/proposal/proposal.html#data",
    "href": "repo-structure/proposal/proposal.html#data",
    "title": "Project proposal",
    "section": "2. Data",
    "text": "2. Data"
  },
  {
    "objectID": "repo-structure/proposal/proposal.html#data-analysis-plan",
    "href": "repo-structure/proposal/proposal.html#data-analysis-plan",
    "title": "Project proposal",
    "section": "3. Data analysis plan",
    "text": "3. Data analysis plan"
  },
  {
    "objectID": "repo-structure/presentation/presentation.html",
    "href": "repo-structure/presentation/presentation.html",
    "title": "R is a great language",
    "section": "",
    "text": "Our project will demonstrate that the praise package is by far the best R package."
  },
  {
    "objectID": "weeks/week-02.html",
    "href": "weeks/week-02.html",
    "title": "Week 2",
    "section": "",
    "text": "📖 Read the syllabus\n📖 Read the support resources"
  },
  {
    "objectID": "weeks/week-02.html#participate",
    "href": "weeks/week-02.html#participate",
    "title": "Week 2",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 2"
  },
  {
    "objectID": "weeks/week-02.html#practice",
    "href": "weeks/week-02.html#practice",
    "title": "Week 2",
    "section": "Practice",
    "text": "Practice\n📋 diamonds"
  },
  {
    "objectID": "weeks/week-02.html#perform",
    "href": "weeks/week-02.html#perform",
    "title": "Week 2",
    "section": "Perform",
    "text": "Perform\n⌨️ Diagnostic test\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-09.html",
    "href": "weeks/week-09.html",
    "title": "Week 9",
    "section": "",
    "text": "📖 Read Chpts 13 & 14 from Stine and Foster\n📖 Read FGLI: Chpts 13 & 14 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-09.html#participate",
    "href": "weeks/week-09.html#participate",
    "title": "Week 9",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 12: Samples and survey\n🖥️ Lecture 13: Sampling variation and quality"
  },
  {
    "objectID": "weeks/week-09.html#perform",
    "href": "weeks/week-09.html#perform",
    "title": "Week 9",
    "section": "Perform",
    "text": "Perform\n✍️ HW 2 - Titanic\n📂 Proposal for project\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-14.html",
    "href": "weeks/week-14.html",
    "title": "Week 14",
    "section": "",
    "text": "🖥️ Lecture 18\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "🖥️ Happy Halloween"
  },
  {
    "objectID": "weeks/week-10.html#practice",
    "href": "weeks/week-10.html#practice",
    "title": "Week 10",
    "section": "Practice",
    "text": "Practice\n📋 Revision for exam 2"
  },
  {
    "objectID": "weeks/week-10.html#perform",
    "href": "weeks/week-10.html#perform",
    "title": "Week 10",
    "section": "Perform",
    "text": "Perform\n✅ Exam 2\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "📖 Read Chpts 15 & 16 from Stine and Foster\n📖 Read FGLI: Chpts 15 & 16 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-11.html#participate",
    "href": "weeks/week-11.html#participate",
    "title": "Week 11",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 14\n🖥️ Lecture 15"
  },
  {
    "objectID": "weeks/week-11.html#practice",
    "href": "weeks/week-11.html#practice",
    "title": "Week 11",
    "section": "Practice",
    "text": "Practice\n📋 AE - 10\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "📖 Read Chpts 17 & 18 from Stine and Foster\n📖 Read FGLI: Chpts 17 & 18 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-12.html#participate",
    "href": "weeks/week-12.html#participate",
    "title": "Week 12",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 16\n🖥️ Lecture 17"
  },
  {
    "objectID": "weeks/week-12.html#practice",
    "href": "weeks/week-12.html#practice",
    "title": "Week 12",
    "section": "Practice",
    "text": "Practice\n📋 AE - 11\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-05.html",
    "href": "weeks/week-05.html",
    "title": "Week 5",
    "section": "",
    "text": "📖 Read Solutions to AEs\n📖 Read Solutions to HW-1"
  },
  {
    "objectID": "weeks/week-05.html#practice",
    "href": "weeks/week-05.html#practice",
    "title": "Week 5",
    "section": "Practice",
    "text": "Practice\n📋 Revision 1 for exam 1\n📋 Revision 2 for exam 1"
  },
  {
    "objectID": "weeks/week-05.html#perform",
    "href": "weeks/week-05.html#perform",
    "title": "Week 5",
    "section": "Perform",
    "text": "Perform\n⌨️ Course evaluation\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-08.html",
    "href": "weeks/week-08.html",
    "title": "Week 8",
    "section": "",
    "text": "📖 Read Chpts 11 & 12 from Stine and Foster\n📖 Read FGLI: Chpts 11 & 12 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-08.html#participate",
    "href": "weeks/week-08.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 10\n🖥️ Lecture 11"
  },
  {
    "objectID": "weeks/week-08.html#perform",
    "href": "weeks/week-08.html#perform",
    "title": "Week 8",
    "section": "Perform",
    "text": "Perform\n✍️ HW 2 - Titanic\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "📖 Read Chpts 19 from Stine and Foster\n📖 Read FGLI: Chpts 19 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-13.html#participate",
    "href": "weeks/week-13.html#participate",
    "title": "Week 13",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 18\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-16.html",
    "href": "weeks/week-16.html",
    "title": "Week 14",
    "section": "",
    "text": "📖 Read Chpts 19 from Stine and Foster\n📖 Read FGLI: Chpts 19 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-16.html#participate",
    "href": "weeks/week-16.html#participate",
    "title": "Week 14",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 18\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-01.html",
    "href": "weeks/week-01.html",
    "title": "Week 1",
    "section": "",
    "text": "📖 Read the syllabus\n📖 Read the support resources"
  },
  {
    "objectID": "weeks/week-01.html#participate",
    "href": "weeks/week-01.html#participate",
    "title": "Week 1",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 1 - Download R and plotting your first graph"
  },
  {
    "objectID": "weeks/week-01.html#practice",
    "href": "weeks/week-01.html#practice",
    "title": "Week 1",
    "section": "Practice",
    "text": "Practice\n📋 diamonds"
  },
  {
    "objectID": "weeks/week-01.html#perform",
    "href": "weeks/week-01.html#perform",
    "title": "Week 1",
    "section": "Perform",
    "text": "Perform\n⌨️ Diagnostic test\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-04.html",
    "href": "weeks/week-04.html",
    "title": "Week 4",
    "section": "",
    "text": "📖 Read chapter 14 of Introduction to Data Science\n📖 Read chapter 2 of Probability, Statistics, and Data: A fresh approach using R"
  },
  {
    "objectID": "weeks/week-04.html#participate",
    "href": "weeks/week-04.html#participate",
    "title": "Week 4",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 5\n🖥️ Lecture 6"
  },
  {
    "objectID": "weeks/week-04.html#practice",
    "href": "weeks/week-04.html#practice",
    "title": "Week 4",
    "section": "Practice",
    "text": "Practice\n📋 Exploring probabilities\n📋 The birthday simulation"
  },
  {
    "objectID": "weeks/week-04.html#perform",
    "href": "weeks/week-04.html#perform",
    "title": "Week 4",
    "section": "Perform",
    "text": "Perform\n⌨️ Diagnostic test - week 3\n✍️ HW 1 - Titanic\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-15.html",
    "href": "weeks/week-15.html",
    "title": "Week 14",
    "section": "",
    "text": "📖 Read Chpts 19 from Stine and Foster\n📖 Read FGLI: Chpts 19 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-15.html#participate",
    "href": "weeks/week-15.html#participate",
    "title": "Week 14",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 18\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-06.html",
    "href": "weeks/week-06.html",
    "title": "Week 6",
    "section": "",
    "text": "📖 Read Chpt 9 from Stine and Foster\n📖 Read FGLI: Chpt 9 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-06.html#participate",
    "href": "weeks/week-06.html#participate",
    "title": "Week 6",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 7"
  },
  {
    "objectID": "weeks/week-06.html#practice",
    "href": "weeks/week-06.html#practice",
    "title": "Week 6",
    "section": "Practice",
    "text": "Practice\n📋 Random variables"
  },
  {
    "objectID": "weeks/week-06.html#perform",
    "href": "weeks/week-06.html#perform",
    "title": "Week 6",
    "section": "Perform",
    "text": "Perform\n✅ Exam 1\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-03.html",
    "href": "weeks/week-03.html",
    "title": "Week 3",
    "section": "",
    "text": "📖 Read chapter 4 of Introduction to Modern Statistics\n📖 Read chapter 5 of Introduction to Modern Statistics"
  },
  {
    "objectID": "weeks/week-03.html#participate",
    "href": "weeks/week-03.html#participate",
    "title": "Week 3",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 3\n🖥️ Lecture 4"
  },
  {
    "objectID": "weeks/week-03.html#practice",
    "href": "weeks/week-03.html#practice",
    "title": "Week 3",
    "section": "Practice",
    "text": "Practice\n📋 Exploring qualitative variables\n📋 Exploring numeric variables"
  },
  {
    "objectID": "weeks/week-03.html#perform",
    "href": "weeks/week-03.html#perform",
    "title": "Week 3",
    "section": "Perform",
    "text": "Perform\n⌨️ Diagnostic test - week 3\n✍️ HW 6 - Statistics Experience\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-07.html",
    "href": "weeks/week-07.html",
    "title": "Week 7",
    "section": "",
    "text": "📖 Read Chpts 6 & 10 from Stine and Foster\n📖 Read FGLI: Chpts 6 & 10 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-07.html#participate",
    "href": "weeks/week-07.html#participate",
    "title": "Week 7",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 8\n🖥️ Lecture 9"
  },
  {
    "objectID": "weeks/week-07.html#practice",
    "href": "weeks/week-07.html#practice",
    "title": "Week 7",
    "section": "Practice",
    "text": "Practice\n📋 AE 9"
  },
  {
    "objectID": "weeks/week-07.html#perform",
    "href": "weeks/week-07.html#perform",
    "title": "Week 7",
    "section": "Perform",
    "text": "Perform\n✍️ HW 2 - Titanic\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "slides/lect_13.html#sampling",
    "href": "slides/lect_13.html#sampling",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Sampling",
    "text": "Sampling\n\n\nSRS\nStratified sampling\nCluster sampling\nit would be nice to sample repeatedly to see how the mean values compare"
  },
  {
    "objectID": "slides/lect_13.html#benefits-of-averaging",
    "href": "slides/lect_13.html#benefits-of-averaging",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Benefits of averaging",
    "text": "Benefits of averaging\n\n\nreduces variation\nmore normal than the original distribution"
  },
  {
    "objectID": "slides/lect_13.html#normal-model",
    "href": "slides/lect_13.html#normal-model",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Normal model",
    "text": "Normal model\n\n\nFind kurtosis(\\(K_4\\))\nIf \\(n > 10 |K_4|\\), where \\(n\\) is the sample size, then a normal model adequately approximates the distribution of the sample mean \\(\\bar{X}\\).\nIf we know the data come from a normal distribution this is also true."
  },
  {
    "objectID": "slides/lect_13.html#standard-error",
    "href": "slides/lect_13.html#standard-error",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Standard error",
    "text": "Standard error\n\\[ SD(\\bar{X}) = SE(\\bar{X}) =\n\\frac{\\sigma}{\\sqrt{n}} \\]"
  },
  {
    "objectID": "slides/lect_13.html#sampling-distribution",
    "href": "slides/lect_13.html#sampling-distribution",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Sampling distribution",
    "text": "Sampling distribution\nIf \\(X \\sim N(\\mu_X, \\sigma_X^2)\\)\n\\[ \\bar{X} \\sim N(\\mu = \\mu_X, \\sigma^2 = \\frac{\\sigma_X^2}{n}) \\]"
  },
  {
    "objectID": "slides/lect_13.html#example",
    "href": "slides/lect_13.html#example",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Example",
    "text": "Example\n\n\nLet \\(Y \\sim N(\\mu = 5, \\sigma^2 = 16)\\), find the distribution of the mean of repeated samples of size 4.\n\n\\(\\bar{Y} \\sim N(\\mu = 5, \\sigma^2 = 4)\\)"
  },
  {
    "objectID": "slides/lect_13.html#control-limits",
    "href": "slides/lect_13.html#control-limits",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Control limits",
    "text": "Control limits\n\n\nif mean production is outside certain values, we may need to stop and recallibrate machinery\nthese values are called control limits\n\\(\\mu - L \\leq \\bar{X} \\leq \\mu + L\\)\n\\(\\mu - L\\) and \\(\\mu + L\\) are control limits"
  },
  {
    "objectID": "slides/lect_13.html#types-of-errors",
    "href": "slides/lect_13.html#types-of-errors",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Types of errors",
    "text": "Types of errors\n\n\nFalse positives - type 1 error\n\nact when you should not\nprobabiliy of occurence denoted \\(\\alpha\\)\n\nFalse negatives - type 2 error\n\ndon’t act when you should\nprobabiliy of occurence denoted \\(\\beta\\)"
  },
  {
    "objectID": "slides/lect_13.html#setting-control-limits---1",
    "href": "slides/lect_13.html#setting-control-limits---1",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Setting control limits - 1",
    "text": "Setting control limits - 1\nIf \\(\\bar{X} \\sim N(\\mu = 12, \\sigma^2 = 2.3)\\) how can we find the control limits?\n\n\nSet the control limits and find the \\(\\alpha\\) value\n\nWe want the control limits to be between 10 and 14\n\\(Pr(\\bar{X} < 10 \\textrm{ or } \\bar{X} > 14)\\)\n\\(\\begin{aligned}P(\\bar{X} < 10) &= P(\\frac{\\bar{X} - \\mu_\\bar{X}}{\\sigma_\\bar{X}} < \\frac{10-\\mu_\\bar{X}}{\\sigma_\\bar{X}}) \\\\ & = P(Z < \\frac{10-12}{\\sqrt{2.3}} = -1.318761)\\end{aligned}\\)\npnorm(-1.318761) \\(\\approx 0.09362451\\)\npnorm(10, mean = 12, sd = sqrt(2.3))"
  },
  {
    "objectID": "slides/lect_13.html#setting-control-limits---1-contd",
    "href": "slides/lect_13.html#setting-control-limits---1-contd",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Setting control limits - 1 cont’d",
    "text": "Setting control limits - 1 cont’d\nIf \\(\\bar{X} \\sim N(\\mu = 12, \\sigma^2 = 2.3)\\) how can we find the control limits?\n\n\nSet the control limits and find the \\(\\alpha\\) value\n\nWe want the control limits to be between 10 and 14\n\\(\\begin{aligned}P(\\bar{X} > 14) &= P(\\frac{\\bar{X} - \\mu_\\bar{X} }{\\sigma_\\bar{X}} > \\frac{14-\\mu_\\bar{X}}{\\sigma_\\bar{X}}) \\\\ & = P(Z > \\frac{14-12}{\\sqrt{2.3}} = 1.318761)\\end{aligned}\\)\n1 - pnorm(1.318761) \\(\\approx 0.09362451\\)\n1 - pnorm(14, mean = 12, sd = sqrt(2.3))\nthe \\(\\alpha\\) value is \\(19\\%\\) which is very high!"
  },
  {
    "objectID": "slides/lect_13.html#setting-control-limits---2",
    "href": "slides/lect_13.html#setting-control-limits---2",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Setting control limits - 2",
    "text": "Setting control limits - 2\n\n\nSet the \\(\\alpha\\) value and find the control limits\n\n\nWe want the \\(\\alpha\\) to be \\(0.025\\)\n\n\\(Pr(\\bar{X} < z_{0.0125} \\textrm{ or } \\bar{X} > z_{0.0125})\\)\nqnorm(0.0125) \\(\\approx -2.241403\\)\n\\(\\begin{aligned}-2.241403 =& \\frac{X - \\mu_\\bar{X}}{\\sigma_\\bar{X}}\\\\ & = \\frac{X - 12}{\\sqrt{2.3}} \\\\ &= -2.241403\\sqrt{2.3} +12 = 8.600744 \\end{aligned}\\)\nqnorm(0.0125, mean = 12, sd = sqrt(2.3))"
  },
  {
    "objectID": "slides/lect_13.html#setting-control-limits---2---contd",
    "href": "slides/lect_13.html#setting-control-limits---2---contd",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Setting control limits - 2 - cont’d",
    "text": "Setting control limits - 2 - cont’d\n\n\nSet the \\(\\alpha\\) value and find the control limits\n\n\nWe want the \\(\\alpha\\) to be \\(0.025\\)\n\n\\(Pr(\\bar{X} < z_{0.0125} \\textrm{ or } \\bar{X} > z_{0.0125})\\)\nqnorm(1- 0.0125) \\(\\approx 2.241403\\)\n\\(\\begin{aligned}2.241403 =& \\frac{X - \\mu_\\bar{X}}{\\sigma_\\bar{X}}\\\\ & = \\frac{X - 12}{\\sqrt{2.3}} \\\\ &= 2.241403\\sqrt{2.3} +12 = 15.39926 \\end{aligned}\\)\nqnorm(1-0.0125, mean = 12, sd = sqrt(2.3))"
  },
  {
    "objectID": "slides/lect_13.html#repeated-testing",
    "href": "slides/lect_13.html#repeated-testing",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Repeated testing",
    "text": "Repeated testing\nWe are not testing once, but multiple times. Assuming independence:\n\n\n\\(\\begin{aligned}P(\\textrm{within limits for 10 days}) =& P(\\textrm{within limits for day 1}) \\cdot P(\\textrm{within limits for day 2}) \\cdot \\dots \\cdot P(\\textrm{within limits for day 10})\\\\ &= 0.975^{10} \\approx 0.7763296 \\end{aligned}\\)\nThere is a \\(1-0.7763296 = 0.2236704\\) percent false positive rate\nManagement must decide if there is a false positive by checking for mechanical errors and inspecting equipment\nAdjust \\(\\alpha\\) value to address this"
  },
  {
    "objectID": "slides/lect_13.html#control-charts-for-variation",
    "href": "slides/lect_13.html#control-charts-for-variation",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Control charts for variation",
    "text": "Control charts for variation\nX-bar charts are slow to detect under or over filling\n\n\nS-Chart tracks the standard deviation from sample to sample\nR-Chart tracks the range from sample to sample\n\n\n\n\nstat1010-f22.github.io/website"
  },
  {
    "objectID": "slides/lect_12.html#revision",
    "href": "slides/lect_12.html#revision",
    "title": "Chapter 13: Samples and surveys",
    "section": "Revision",
    "text": "Revision\n\nWe have not yet discussed the sampling process. Now is our chance!\nOne of the differences between statistics and machine learning - in ml not much thought of sampling."
  },
  {
    "objectID": "slides/lect_12.html#predictive-policing",
    "href": "slides/lect_12.html#predictive-policing",
    "title": "Chapter 13: Samples and surveys",
    "section": "Predictive policing",
    "text": "Predictive policing\n\n\nResearch question - Can police use crime data from disparate sources to anticipate and prevent future crime?\npopulation - all crime\nsample - arrests recorded in police database\nIs this a random sample?\narrests is a surrogate measurement because crime is hard to track"
  },
  {
    "objectID": "slides/lect_12.html#predictive-policing-1",
    "href": "slides/lect_12.html#predictive-policing-1",
    "title": "Chapter 13: Samples and surveys",
    "section": "Predictive policing",
    "text": "Predictive policing\n\n\nfor arrests to occur police must be present\nsome areas are over policed, so have more arrests\nthis algorithm sends police to those areas\nread more here\nuse census data to predict crime instead of arrests, we may do better"
  },
  {
    "objectID": "slides/lect_12.html#census",
    "href": "slides/lect_12.html#census",
    "title": "Chapter 13: Samples and surveys",
    "section": "Census",
    "text": "Census\n\n\nsample everyone in the population\nthis is often difficult because some individuals are hard to locate, and these people may have certain characterisitics that distinguish them from the rest of the population\nPopulations move so getting a perfect measure is hard.\nA census may be more complex than sampling"
  },
  {
    "objectID": "slides/lect_12.html#landon-vs-roosevelt",
    "href": "slides/lect_12.html#landon-vs-roosevelt",
    "title": "Chapter 13: Samples and surveys",
    "section": "Landon vs Roosevelt",
    "text": "Landon vs Roosevelt\n\n\nLiterary digest - correctly predicted presidential elections from 1916 - 1932 with mock ballots\n1936 election\n\n10 million ballots\nLandon win by landslide with 57%\n\n10 million names and addresses in 1936\npoor people were unlikely to have phones, so they over sampled the wealthy\nGallup predicted a Roosevelt win\nweighting can be used to correct biased samples"
  },
  {
    "objectID": "slides/lect_12.html#sampling-1",
    "href": "slides/lect_12.html#sampling-1",
    "title": "Chapter 13: Samples and surveys",
    "section": "Sampling",
    "text": "Sampling\n\n\ntasting is analogous to exploratory analysis\nstirring helps ensure that the taste is representative because it randomizes\nif we add ingredients and don’t stir we may get a biased sample\nif we generalize and decide that it needs more salt, that’s an inference"
  },
  {
    "objectID": "slides/lect_12.html#sampling-frame",
    "href": "slides/lect_12.html#sampling-frame",
    "title": "Chapter 13: Samples and surveys",
    "section": "Sampling frame",
    "text": "Sampling frame\n\n\nlists every member of the population of interest\ncan be complex to identify\nsample from registered voters, but really want people who will vote\nhypothetical populations are more complex\na brewery must sample hops across farmers, and different geographic regions prior to formalizing brewing"
  },
  {
    "objectID": "slides/lect_12.html#simple-random-sample",
    "href": "slides/lect_12.html#simple-random-sample",
    "title": "Chapter 13: Samples and surveys",
    "section": "Simple random sample",
    "text": "Simple random sample"
  },
  {
    "objectID": "slides/lect_12.html#stratified-sampling",
    "href": "slides/lect_12.html#stratified-sampling",
    "title": "Chapter 13: Samples and surveys",
    "section": "Stratified sampling",
    "text": "Stratified sampling"
  },
  {
    "objectID": "slides/lect_12.html#cluster-sample",
    "href": "slides/lect_12.html#cluster-sample",
    "title": "Chapter 13: Samples and surveys",
    "section": "Cluster sample",
    "text": "Cluster sample"
  },
  {
    "objectID": "slides/lect_12.html#sampling-2",
    "href": "slides/lect_12.html#sampling-2",
    "title": "Chapter 13: Samples and surveys",
    "section": "Sampling",
    "text": "Sampling\n\n\nschools or nursing homes for cluster sampling\nstrata imply some similarity, for example high income or low income people\n\n\n\n\nRandomly select cases from the population, where there is no implied connection between the points that are selected.\nStrata are made up of similar observations. We take a simple random sample from each stratum.\nClusters are usually not made up of homogeneous observations. We take a simple random sample of clusters, and then sample all observations in that cluster. Usually preferred for economical reasons."
  },
  {
    "objectID": "slides/lect_12.html#sampling-in-r",
    "href": "slides/lect_12.html#sampling-in-r",
    "title": "Chapter 13: Samples and surveys",
    "section": "Sampling in R",
    "text": "Sampling in R\n\n# SRS\ndata %>% \n  slice_sample(n = sample_size)\n\n# Stratified sampling\ndata %>% \n  group_by(strata) %>% \n  slice_sample(n = size_from_each_strata)\n\n# Cluster sampling\nrandom_cluster <- slice_sample(data$cluster, \n                               n = no_of_clusters)"
  },
  {
    "objectID": "slides/lect_12.html#not-great-sampling-methods",
    "href": "slides/lect_12.html#not-great-sampling-methods",
    "title": "Chapter 13: Samples and surveys",
    "section": "Not great sampling methods",
    "text": "Not great sampling methods\n\nmeta and other tech companies often use NZ as a sampling ground to see how a new product works prior to releasing it to the USA.\nmy cousin when she started her allergen-free cakes b/c her son couldn’t eat anything\n\n\n\nvoluntary response\nconvenience sampling - asking friends or families feedback about your product"
  },
  {
    "objectID": "slides/lect_15.html#does-esp-exist-1",
    "href": "slides/lect_15.html#does-esp-exist-1",
    "title": "Chapter 16: Statistical tests",
    "section": "Does ESP exist?",
    "text": "Does ESP exist?\nSince there are 5 cards, it would be possible to guess the correct card at random \\(p=1/5\\) times, or 20% of the time.\nA person with ESP should be able to guess the correct card more often than 20%. But how much more often do they need to get it right for us to believe that ESP exists?"
  },
  {
    "objectID": "slides/lect_15.html#statistical-test",
    "href": "slides/lect_15.html#statistical-test",
    "title": "Chapter 16: Statistical tests",
    "section": "Statistical test",
    "text": "Statistical test\nOne way to determine something like this is to use a statistical test. A statistical test is a procedure to determine if the results from the sample are convincing enough to allow us to conclude something about the population."
  },
  {
    "objectID": "slides/lect_15.html#hypotheses",
    "href": "slides/lect_15.html#hypotheses",
    "title": "Chapter 16: Statistical tests",
    "section": "Hypotheses",
    "text": "Hypotheses\nWhen we perform a statistical test, we set out our hypotheses before we begin. There are two hypotheses,\nnull hypothesis \\(H_0\\), a statement about there being no effect, or no difference.\nalternative hypothesis \\(H_A\\), the thing that we secretly hope will turn out to be true."
  },
  {
    "objectID": "slides/lect_15.html#esp-hypotheses",
    "href": "slides/lect_15.html#esp-hypotheses",
    "title": "Chapter 16: Statistical tests",
    "section": "ESP hypotheses",
    "text": "ESP hypotheses\nThinking about the ESP experiment, we could use words to state our hypotheses\n\\(\\begin{eqnarray*} &H_0:& \\text{ ESP does not exist} \\\\ &H_A:& \\text{ESP exists} \\end{eqnarray*}\\)\nWe could also write the hypotheses in terms of parameters,\n\\(\\begin{eqnarray*} H_0: p \\leq 1/5 \\\\ H_A: p > 1/5 \\end{eqnarray*}\\)"
  },
  {
    "objectID": "slides/lect_15.html#hypotheses-1",
    "href": "slides/lect_15.html#hypotheses-1",
    "title": "Chapter 16: Statistical tests",
    "section": "Hypotheses",
    "text": "Hypotheses\n\n\nHypotheses are always written about the population parameter (\\(\\mu\\), \\(p\\), \\(\\mu_1-\\mu_2\\), \\(p_1-p_2\\)), never about the sample statistics (\\(\\bar{x}\\), \\(\\hat{p}\\), \\(\\bar{x}_1-\\bar{x}_2\\), \\(\\hat{p}_1-\\hat{p}_2\\)).\nThe null hypothesis is the boring thing that we’re trying to gather evidence against\nThe alternative hypothesis is the exciting thing that would make headlines"
  },
  {
    "objectID": "slides/lect_15.html#one-and-two-sided-hypotheses",
    "href": "slides/lect_15.html#one-and-two-sided-hypotheses",
    "title": "Chapter 16: Statistical tests",
    "section": "One and two sided hypotheses",
    "text": "One and two sided hypotheses\n\n\n\\(H_A\\) has a \\(>\\) sign: upper tail, or “one-sided”\n\\(H_A\\) has a \\(<\\) sign: lower tail, or “one-sided”\n\\(H_A\\) has a \\(\\neq\\) sign: both sides, or “two-sided”"
  },
  {
    "objectID": "slides/lect_15.html#example-1",
    "href": "slides/lect_15.html#example-1",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 1",
    "text": "Example 1\nWhat are the null and alternate hypothesis for these decisions?\n\n\nA person interviews for a job opening. The company has to decide whether to hire the person\nAn inventor proposes a new way to wrap packages that they say will speed up the manufacturing process. Should they adopt the new method?\nA sales representative submits receipts from a recent business trip. Staff must determine whether the claims are legitimate."
  },
  {
    "objectID": "slides/lect_15.html#example-1-solns",
    "href": "slides/lect_15.html#example-1-solns",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 1 solns",
    "text": "Example 1 solns\n\n\n$H_0 = $ Person is not hired\n$H_0 = $ Retain the current method\n$H_0 = $ Treat claims as legitimate. The employee is innocent until evidence to the contrary is found."
  },
  {
    "objectID": "slides/lect_15.html#past-tests",
    "href": "slides/lect_15.html#past-tests",
    "title": "Chapter 16: Statistical tests",
    "section": "Past tests",
    "text": "Past tests\n\n\nVisual assocation tests\nqqplots\n\\(\\chi^2\\) tests"
  },
  {
    "objectID": "slides/lect_15.html#example-2",
    "href": "slides/lect_15.html#example-2",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 2",
    "text": "Example 2\nA snack-food chain runs a promotion in which shoppers are told that 1 in 4 kids’ meals includes a prize. A father buys two kids’ meals, and neither has a prize. He concludes that because neither has a prize, the chain is being deceptive.\n\nWhat is the null and alternative hypothesis?\nDescribe a Type I error?\nDescribe a Type II error?\nWhat is the probability that the father has made a Type I error?"
  },
  {
    "objectID": "slides/lect_15.html#example-2---solns",
    "href": "slides/lect_15.html#example-2---solns",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 2 - solns",
    "text": "Example 2 - solns\n\n\\(H_0\\) = chain is being honest; \\(H_A\\) = chain is not being honest\nFalsely accusing the chain of being deceptive\nFailing to realize that the chain is being deceptive\nFather rejects if both are missing a prize. \\(P(neither has a prize) = (1 - 1/4)^2 \\approx 0.56\\)"
  },
  {
    "objectID": "slides/lect_15.html#test-statistic",
    "href": "slides/lect_15.html#test-statistic",
    "title": "Chapter 16: Statistical tests",
    "section": "Test statistic",
    "text": "Test statistic\nIn the previous example, we used probability to determine how likely it is to get two meals neither of which contain a prize. This process involves computing a test statistic (\\(0.56\\))."
  },
  {
    "objectID": "slides/lect_15.html#testing-a-proportion---contd",
    "href": "slides/lect_15.html#testing-a-proportion---contd",
    "title": "Chapter 16: Statistical tests",
    "section": "Testing a proportion - contd",
    "text": "Testing a proportion - contd\nUnder \\(H_0\\), we find\n\\[z_0 = \\frac{\\hat{p} - p_0} {\\sqrt{\\frac{p_0(1-p_0)}{n}}} \\]"
  },
  {
    "objectID": "slides/lect_15.html#assumptions",
    "href": "slides/lect_15.html#assumptions",
    "title": "Chapter 16: Statistical tests",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nSRS and sample must be < \\(10\\%\\) of population\nBoth \\(np_0\\) and \\(n(1-p_0)\\) are larger than 10"
  },
  {
    "objectID": "slides/lect_15.html#example-3",
    "href": "slides/lect_15.html#example-3",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 3",
    "text": "Example 3\nIn the ESP example, we know that the population average for guessing ESP cards is 9 out of 24 cards. An interested participant took a training course to enhance their ESP. In the followup exam, they guessed 17 out of 36 cards. Did the course improve their ESP abilities?"
  },
  {
    "objectID": "slides/lect_15.html#steps-for-a-hypothesis-test",
    "href": "slides/lect_15.html#steps-for-a-hypothesis-test",
    "title": "Chapter 16: Statistical tests",
    "section": "Steps for a hypothesis test",
    "text": "Steps for a hypothesis test\n\nState hypotheses\nDetermine the level of significance\nCheck conditions\nCalculate test statistic\nCompute p-value\nGeneric conclusion\nInterpret in context"
  },
  {
    "objectID": "slides/lect_15.html#example-3---solns",
    "href": "slides/lect_15.html#example-3---solns",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 3 - solns",
    "text": "Example 3 - solns\n\n\n\\(H_0: p \\leq 9/24\\) and \\(H_A: p > 9/24\\)\n\\(\\alpha = 0.05\\)\nYes 9/24*36 and (1-9/24)*36 both > 10\n\\(\\hat{p} = \\frac{x}{n} = \\frac{17}{36} =0.472\\)\n\\(\\begin{aligned} z_0 &= \\frac{\\hat{p} - p_0} {\\sqrt{\\frac{p_0(1-p_0)}{n}}} \\\\ &= \\frac{0.472 - 0.375} {\\sqrt{\\frac{0.375(1-0.375)}{36}}} \\\\ &= \\frac{0.0972}{0.0807} \\approx 1.2 \\end{aligned}\\)"
  },
  {
    "objectID": "slides/lect_15.html#example-3---solns-1",
    "href": "slides/lect_15.html#example-3---solns-1",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 3 - solns",
    "text": "Example 3 - solns\n\n\n1 -pnorm(1.2) or 1- pnorm((17/36- 9/24)/sqrt((0.375*(1-0.375))/36)) \\(\\approx 0.115\\)\nSince \\(0.115 > \\alpha = 0.05\\) this is not significant.\nWe fail to reject \\(H_0\\). The score does not provide evidence that the intervention improved ESP."
  },
  {
    "objectID": "slides/lect_15.html#testing-a-mean---contd",
    "href": "slides/lect_15.html#testing-a-mean---contd",
    "title": "Chapter 16: Statistical tests",
    "section": "Testing a mean - contd",
    "text": "Testing a mean - contd\nUnder \\(H_0\\), we find\n\\[t = \\frac{\\bar{X} - \\mu_0} {s/\\sqrt{{n}}} \\]"
  },
  {
    "objectID": "slides/lect_15.html#assumptions-1",
    "href": "slides/lect_15.html#assumptions-1",
    "title": "Chapter 16: Statistical tests",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nSRS and sample must be < \\(10\\%\\) of population\nIf we do not know if the population is normal, \\(n > 10|K_4|\\)"
  },
  {
    "objectID": "slides/lect_15.html#example-4",
    "href": "slides/lect_15.html#example-4",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 4",
    "text": "Example 4\nLet \\(\\bar{x} = 3281\\), \\(s = 529\\), and \\(n = 59\\). Perform a hypothesis test that the sample comes from a distribution where the population mean is less than \\(\\mu_0 = 4000\\)"
  },
  {
    "objectID": "slides/lect_15.html#steps-for-a-hypothesis-test-1",
    "href": "slides/lect_15.html#steps-for-a-hypothesis-test-1",
    "title": "Chapter 16: Statistical tests",
    "section": "Steps for a hypothesis test",
    "text": "Steps for a hypothesis test\n\nState hypotheses\nDetermine the level of significance\nCheck conditions\nCalculate test statistic\nCompute p-value\nGeneric conclusion\nInterpret in context"
  },
  {
    "objectID": "slides/lect_15.html#example-4---solns",
    "href": "slides/lect_15.html#example-4---solns",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 4 - solns",
    "text": "Example 4 - solns\n\n\n\\(H_0: p \\geq 4000\\) and \\(H_A: p < 4000\\)\n\\(\\alpha = 0.05\\)\nYes\n\\(\\bar{x} = 3281\\)\n\\(\\begin{aligned} t &= \\frac{\\bar{X} - \\mu_0} {s/\\sqrt{{n}}}\\\\ &= \\frac{3281-4000}{529/\\sqrt{59}} \\\\ &= \\frac{-719}{68.87} \\approx -10.44 \\end{aligned}\\)"
  },
  {
    "objectID": "slides/lect_15.html#example-4---solns-1",
    "href": "slides/lect_15.html#example-4---solns-1",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 4 - solns",
    "text": "Example 4 - solns\n\n\npt(-10.44, df = 58) or pt((3281-4000)/(529/sqrt(59)), df = 58) \\(\\approx 3.07e-15\\)\nSince \\(3.07e-15 < \\alpha = 0.05\\) this is significant.\nWe reject \\(H_0\\). There is very strong evidence that the mean value is less than 4000."
  },
  {
    "objectID": "slides/lect_15.html#errors",
    "href": "slides/lect_15.html#errors",
    "title": "Chapter 16: Statistical tests",
    "section": "Errors",
    "text": "Errors\nThere are two types of errors defined in hypothesis testing:\n\nType I error, rejecting a true null\nType II error, not rejecting a false null"
  },
  {
    "objectID": "slides/lect_15.html#law-analogy",
    "href": "slides/lect_15.html#law-analogy",
    "title": "Chapter 16: Statistical tests",
    "section": "Law analogy",
    "text": "Law analogy\nIn the US, a person is innocent until proven guilty, and evidence of guilt must be beyond “the shadow of a doubt.” We can make two types of mistakes:\n\nConvict an innocent person (type I error)\nRelease a guilty person (type II error)"
  },
  {
    "objectID": "slides/lect_15.html#extremists-and-black-and-white",
    "href": "slides/lect_15.html#extremists-and-black-and-white",
    "title": "Chapter 16: Statistical tests",
    "section": "Extremists and black and white",
    "text": "Extremists and black and white\nTwo options:\n\nthe original study (p-value 0.01) made a Type I error, and the \\(H_0\\) was really true\nthe second study (p-value 0.59) made a Type II error, and \\(H_A\\) is really true\n\nor…\n\nmaybe there were no errors made, just different studies found different things"
  },
  {
    "objectID": "slides/lect_15.html#multiple-testing",
    "href": "slides/lect_15.html#multiple-testing",
    "title": "Chapter 16: Statistical tests",
    "section": "Multiple testing",
    "text": "Multiple testing\nBecause the probability of a Type I error is \\(\\alpha\\), if you do many tests you will find significance in \\(\\alpha\\) of them just by chance.\nIf you do 100 tests, you should expect to find 5 of them to be significant, just by chance.\nThis is the problem of multiple testing."
  },
  {
    "objectID": "slides/lect_15.html#multiple-testing-publication-bias",
    "href": "slides/lect_15.html#multiple-testing-publication-bias",
    "title": "Chapter 16: Statistical tests",
    "section": "Multiple testing + publication bias",
    "text": "Multiple testing + publication bias\nOkay, so \\(\\alpha\\) of all tests show significance, just by random chance.\nAnd things that look significant get published…\nThat means that a fair number of things that are published are actually false! This is pretty scary."
  },
  {
    "objectID": "slides/lect_15.html#how-to-fix-the-problem",
    "href": "slides/lect_15.html#how-to-fix-the-problem",
    "title": "Chapter 16: Statistical tests",
    "section": "How to fix the problem",
    "text": "How to fix the problem\nAs a researcher:\n\nmake sure your results can be replicated (like Motyl tried to do with the politics and grey study)\npublish code and data so others can study your work\n\nAs someone who reads about statistics:\n\nbe skeptical about claims that are just one of many tests\nlook for replication and reproducibility!"
  },
  {
    "objectID": "slides/lect_15.html#reducing-the-probability-of-type-ii-error",
    "href": "slides/lect_15.html#reducing-the-probability-of-type-ii-error",
    "title": "Chapter 16: Statistical tests",
    "section": "Reducing the probability of Type II error",
    "text": "Reducing the probability of Type II error\nIn order to reduce the probability of making a Type II error, we can either\n\nincrease the significance level\nincrease the sample size"
  },
  {
    "objectID": "slides/lect_14.html#revision",
    "href": "slides/lect_14.html#revision",
    "title": "Chapter 15: Confidence intervals",
    "section": "Revision",
    "text": "Revision"
  },
  {
    "objectID": "slides/lect_14.html#variability",
    "href": "slides/lect_14.html#variability",
    "title": "Chapter 15: Confidence intervals",
    "section": "Variability",
    "text": "Variability\n\nask them what they population mean should be\nwe also need to think about the variability\n\n\n\ndependent on \\(p\\) - fatter in middle, skinnier near 0 and 1\ndependent on \\(n\\), number of samples, bigger sample, less variability"
  },
  {
    "objectID": "slides/lect_14.html#variability---math",
    "href": "slides/lect_14.html#variability---math",
    "title": "Chapter 15: Confidence intervals",
    "section": "Variability - math",
    "text": "Variability - math\n\\[ \\hat{p} \\sim N(\\mu = p, \\sigma^2 = \\frac{p(1-p)}{n}) \\]\n\n\nnumerator of the variance is similar to that of a Binomial distribution\nas sample size increases, variance decreases"
  },
  {
    "objectID": "slides/lect_14.html#ci---math",
    "href": "slides/lect_14.html#ci---math",
    "title": "Chapter 15: Confidence intervals",
    "section": "CI - math",
    "text": "CI - math\n\\[\\hat{p} \\pm z_{\\alpha/2} \\cdot \\sqrt{\\hat{p}(1-\\hat{p})/n} \\] For \\(95\\%\\) CI, \\(z_{\\alpha/2} = 1.96\\)\nAs confidence level increases, interval widens\nIn R: qnorm(0.025)"
  },
  {
    "objectID": "slides/lect_14.html#assumptions",
    "href": "slides/lect_14.html#assumptions",
    "title": "Chapter 15: Confidence intervals",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nSRS condition The observed sample is SRS from the appropriate population. If population is finite, less than 10% of total population\nSample size condition (for proportion) both \\(n\\hat{p}\\) and \\(n(1-\\hat{p})\\) are larger than 10."
  },
  {
    "objectID": "slides/lect_14.html#example-1",
    "href": "slides/lect_14.html#example-1",
    "title": "Chapter 15: Confidence intervals",
    "section": "Example 1",
    "text": "Example 1\n\n\nLet \\(\\hat{p}= 0.14\\) and \\(n = 350\\), find a \\(95\\%\\) CI for the proportion\n\\(\\begin{aligned} se(\\hat{p}) &= \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} &= \\sqrt{\\frac{0.14(1-0.14)}{350}}\\end{aligned} \\approx 0.0185\\)\nlower value: \\(\\hat{p} - 1.96 \\cdot 0.0185 \\approx 0.10374\\)\nupper value: \\(\\hat{p} + 1.96 \\cdot 0.0185 \\approx 0.17626\\)"
  },
  {
    "objectID": "slides/lect_14.html#example-2",
    "href": "slides/lect_14.html#example-2",
    "title": "Chapter 15: Confidence intervals",
    "section": "Example 2",
    "text": "Example 2\nAn auditor checks a sample of 225 randomly chosen transactions from among the thousands processed in an office. Thirty-five contains errors in crediting or debiting the appropriate account\na. Does this situation meet the conditions required for a \\(z\\)-interval for the proportion?\nb. Find the \\(95\\%\\) confidence interval for \\(p\\), the proportion of all transactions processed in the this office that have these errors.\nc. Managers claim that the proportion of errors is about \\(10\\%\\). Does that seem reasonable?"
  },
  {
    "objectID": "slides/lect_14.html#example-2---solns",
    "href": "slides/lect_14.html#example-2---solns",
    "title": "Chapter 15: Confidence intervals",
    "section": "Example 2 - solns",
    "text": "Example 2 - solns\n\n\nyes, \\(\\hat{p} = 35/225 \\approx 0.156\\) so \\(n\\hat{p}, n(1-\\hat{p})> 10\\) and we assume sampled \\(<10\\%\\)\n\\(\\begin{aligned} \\hat{p} \\pm z_{\\alpha/2} \\cdot \\sqrt{\\hat{p}(1-\\hat{p})/n} &= 0.156 \\pm 1.96 \\sqrt{0.156(1-0.156)/225}\\\\ &= 0.156 \\pm 1.96 \\cdot 0.0242 \\\\ &= 0.156 \\pm 0.047\\\\ & [0.109, 0.203]\\end{aligned}\\)\nNo \\(10\\%\\) is too low, with \\(95\\%\\) confidence it is higher"
  },
  {
    "objectID": "slides/lect_14.html#variability-1",
    "href": "slides/lect_14.html#variability-1",
    "title": "Chapter 15: Confidence intervals",
    "section": "Variability",
    "text": "Variability\n\n\nto account for smaller sample size we use a different distribution\n\n\n\n\nstandard deviation of distribution\n\nhigher sd -> wider CI\n\nsample size\n\nsmaller sample size -> wider CI"
  },
  {
    "objectID": "slides/lect_14.html#section-3",
    "href": "slides/lect_14.html#section-3",
    "title": "Chapter 15: Confidence intervals",
    "section": "",
    "text": "t distribution more density in tails"
  },
  {
    "objectID": "slides/lect_14.html#ci---mean",
    "href": "slides/lect_14.html#ci---mean",
    "title": "Chapter 15: Confidence intervals",
    "section": "CI - mean",
    "text": "CI - mean\n\\[\\bar{x} \\pm t_{\\alpha/2, n-1} \\cdot \\frac{s}{\\sqrt{n}} \\]"
  },
  {
    "objectID": "slides/lect_14.html#t---distribution",
    "href": "slides/lect_14.html#t---distribution",
    "title": "Chapter 15: Confidence intervals",
    "section": "T - distribution",
    "text": "T - distribution\n\n\ndon’t know sigma so use s in it’s place\nas df get bigger (a proxy for n) S approaches sigma\n\n\n\n\n\\(Z = \\frac{X-\\mu_X}{\\sigma_X}\\)\n\\(\\bar{X} \\sim N(\\mu = \\mu\\_X, \\sigma^2 = \\frac{\\sigma_X^2}{n})\\)\n\\(T_{n-1} = \\frac{\\bar{X}-\\mu_X}{S/\\sqrt{n}}\\)\n\\(t\\) distribution has more density in the tails to account for smaller sample sizes"
  },
  {
    "objectID": "slides/lect_14.html#variability-2",
    "href": "slides/lect_14.html#variability-2",
    "title": "Chapter 15: Confidence intervals",
    "section": "Variability",
    "text": "Variability\n\n\ndependent on \\(confidence \\text{ }level\\) - as confidence level increases so does the width of the interval\ndependent on \\(n\\), number of samples, bigger sample, less variability"
  },
  {
    "objectID": "slides/lect_14.html#assumptions-1",
    "href": "slides/lect_14.html#assumptions-1",
    "title": "Chapter 15: Confidence intervals",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nSRS condition The observed sample is SRS from the appropriate population. If population is finite, less than 10% of total population\nSample size condition the sample size is larger than 10 times the absolute value of the kurtosis, \\(n>10|K_4|\\)."
  },
  {
    "objectID": "slides/lect_14.html#example-1-1",
    "href": "slides/lect_14.html#example-1-1",
    "title": "Chapter 15: Confidence intervals",
    "section": "Example 1",
    "text": "Example 1\n\n\nLet \\(\\bar{x}= \\$3285\\), \\(n = 150\\), and \\(s = \\$238\\) find a \\(95\\%\\) CI for the mean\n\\(\\begin{aligned} se(\\bar{x}) &= s/\\sqrt{n} &= 238/\\sqrt{150}\\end{aligned} \\approx 19.43262\\)\nqt(.025, df = 149) \\(\\approx -1.976\\)\nlower value: \\(3285 - 1.976 \\cdot 19.43262 \\approx \\$3,320\\)\nupper value: \\(3285 + 1.976 \\cdot 19.43262 \\approx \\$3,247\\)\n3285 + qt(.025, df = 149)* 238/sqrt(150)"
  },
  {
    "objectID": "slides/lect_14.html#example-2-1",
    "href": "slides/lect_14.html#example-2-1",
    "title": "Chapter 15: Confidence intervals",
    "section": "Example 2",
    "text": "Example 2\nOffice administrators claim that the average amount on a purchase order is $6,000. A SRS of 49 purchase orders averages \\(\\bar{x} = \\$4,200\\) with \\(s = \\$3,500\\).\n\nWhat is the relevant sampling distribution?\nFind the 95% confidence interval for \\(\\mu\\), the mean of purchase orders handled by this office during the sampling period.\nDo you think the administrators claim is reasonable?"
  },
  {
    "objectID": "slides/lect_14.html#example-2---solns-1",
    "href": "slides/lect_14.html#example-2---solns-1",
    "title": "Chapter 15: Confidence intervals",
    "section": "Example 2 - solns",
    "text": "Example 2 - solns\n\n\n\\(N(\\mu, \\sigma^2/49)\\)\n\\[\\begin{aligned} 4200 \\pm 2.01 \\cdot 3500/\\sqrt{49} &= 4200 \\pm 2.01 \\cdot 500 \\\\ &= 4200 \\pm -1005.317 \\\\ &\\approx [3195, 5205]\\end{aligned}\\]\nNo \\(\\$6,00\\) is way above the \\(95\\%\\) confidence interval"
  },
  {
    "objectID": "slides/lect_14.html#example-2---solns-2",
    "href": "slides/lect_14.html#example-2---solns-2",
    "title": "Chapter 15: Confidence intervals",
    "section": "Example 2 - solns",
    "text": "Example 2 - solns\n\\(10 \\cdot [3195, 5205] = [31950, 52050]\\)"
  },
  {
    "objectID": "slides/lect_14.html#sample-size---mean",
    "href": "slides/lect_14.html#sample-size---mean",
    "title": "Chapter 15: Confidence intervals",
    "section": "Sample size - mean",
    "text": "Sample size - mean\n\\[n = \\frac{4s^2}{\\text{(Margin of error)}^2} \\]\n\n\nFor \\(p\\) use \\(p = 0.5\\) for biggest variance"
  },
  {
    "objectID": "slides/lect_18.html#variable-names",
    "href": "slides/lect_18.html#variable-names",
    "title": "Chapter 19: Linear patterns",
    "section": "Variable names",
    "text": "Variable names\n\n\nvariable on the \\(y\\) axis\n\ndependent\nresponse\noutcome\n\nvariable on the \\(x\\) axis\n\nindependent (not this kind of independence)\nexplanatory\npredictor"
  },
  {
    "objectID": "slides/lect_18.html#plot-the-data",
    "href": "slides/lect_18.html#plot-the-data",
    "title": "Chapter 19: Linear patterns",
    "section": "Plot the data",
    "text": "Plot the data"
  },
  {
    "objectID": "slides/lect_18.html#fit",
    "href": "slides/lect_18.html#fit",
    "title": "Chapter 19: Linear patterns",
    "section": "Fit",
    "text": "Fit\n\\[ \\hat{y} = b_0 + b_1\\cdot x \\]\n\n\n\\(b_1\\) is gradient \\(b_0\\) is the \\(y\\) intercept\n\\(\\hat{y}\\) is the fitted value, the result of the fitted line"
  },
  {
    "objectID": "slides/lect_18.html#possible-fits",
    "href": "slides/lect_18.html#possible-fits",
    "title": "Chapter 19: Linear patterns",
    "section": "Possible fits",
    "text": "Possible fits"
  },
  {
    "objectID": "slides/lect_18.html#least-squares",
    "href": "slides/lect_18.html#least-squares",
    "title": "Chapter 19: Linear patterns",
    "section": "Least squares",
    "text": "Least squares"
  },
  {
    "objectID": "slides/lect_18.html#gradient-and-slope",
    "href": "slides/lect_18.html#gradient-and-slope",
    "title": "Chapter 19: Linear patterns",
    "section": "Gradient and slope",
    "text": "Gradient and slope\n\nPull out your calculators and work on this\n\n\\[ b_1 = \\frac{r \\cdot s_y}{s_x} \\] \\[ b_0 = \\bar{y} - b_1 \\bar{x} \\]"
  },
  {
    "objectID": "slides/lect_18.html#example-1",
    "href": "slides/lect_18.html#example-1",
    "title": "Chapter 19: Linear patterns",
    "section": "Example 1",
    "text": "Example 1\n\ncarat is on the bottom b/c it is the x, always x on the bottom.\n\n\n\n\\(r = 0.9215913\\)\n\\(s_{price} = 3989.44\\)\n\\(s_{carat} = 0.4740112\\)\n\\(b_1 = \\frac{r \\cdot s_{price}}{s_{carat}}\\)\n\\(\\begin{aligned} b_1 &= \\frac{r \\cdot s_{price}}{s_{carat}} \\\\ &= \\frac{0.9215913 \\cdot 3989.44}{0.4740112} \\\\ &= 7756.426 \\end{aligned}\\)"
  },
  {
    "objectID": "slides/lect_18.html#example-1-1",
    "href": "slides/lect_18.html#example-1-1",
    "title": "Chapter 19: Linear patterns",
    "section": "Example 1",
    "text": "Example 1\n\n\n\\(b_0 = \\overline{price} - b_1 \\cdot \\overline{carat}\\)\n\\(\\overline{price} = 3932.8\\)\n\\(\\overline{carat} = 0.7979397\\)\n\\(b_1 = 7756.426\\)\n\\[\\begin{aligned}\nb_0 &= \\overline{price} - b_1 \\cdot \\overline{carat} \\\\\n&= 3932.8 - 7756.426 \\cdot 0.7979397\\\\\n&= -2256.36\n\\end{aligned}\\]\nOur model is: \\(\\widehat{\\text{price}} = -2256.36 + 7756.426 \\cdot \\text{carat}\\)"
  },
  {
    "objectID": "slides/lect_18.html#example-1-2",
    "href": "slides/lect_18.html#example-1-2",
    "title": "Chapter 19: Linear patterns",
    "section": "Example 1",
    "text": "Example 1\n\nPull out your calculators and work on this\n\nfor carat values \\(2.5\\)\n\n\n\\(\\widehat{\\text{price}} = -2256.36 + 7756.426 \\cdot \\text{carat}\\)\n\\(\\hat{y} = -2256.36 + 7756.426 \\cdot x\\)\n\\(\\begin{aligned}\\hat{y} &= -2256.36 + 7756.426 \\cdot x \\\\ &= -2256.36 + 7756.426 \\cdot 2.5 \\\\&= \\$17135 \\end{aligned}\\)"
  },
  {
    "objectID": "slides/lect_18.html#residuals",
    "href": "slides/lect_18.html#residuals",
    "title": "Chapter 19: Linear patterns",
    "section": "Residuals",
    "text": "Residuals\nThe residual for the \\(i^{th}\\) observation is \\[ \\begin{aligned} e_i &= y_i - \\hat{y_i} \\\\\n&= y_i - b_0 - b_1 \\cdot x_i\n\\end{aligned}\\]\n\nThe sum of squared residuals is\n\n\\[e^2_1 + e^2_2 + \\dots + e^2_n\\]\n\nThe least squares line is the one that minimizes the sum of squared residuals"
  },
  {
    "objectID": "slides/lect_18.html#example-2",
    "href": "slides/lect_18.html#example-2",
    "title": "Chapter 19: Linear patterns",
    "section": "Example 2",
    "text": "Example 2\nFind the residual for the pair \\((2.5, 16955)\\)\n\n\n\\[ \\begin{aligned} e &= y - \\hat{y} \\\\\n&= 16955 - (-2256.36 + 7756.426 \\cdot 2.5) \\\\\n&= 16955 - 17134.71\\\\\n&= -179.705\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lect_18.html#in-r",
    "href": "slides/lect_18.html#in-r",
    "title": "Chapter 19: Linear patterns",
    "section": "In R",
    "text": "In R\n\n# for model fitting and to get residuals\nlibrary(tidymodels)\n\n\n## Assign the least\n## squares line\nlm_fit<- lm(price ~ carat, diamonds) \n## NOTE: outcome first, predictor second\n\n## find predictions and residuals\nhead(augment(lm_fit), 5)\n\n# A tibble: 5 × 8\n  price carat .fitted .resid      .hat .sigma     .cooksd .std.resid\n  <int> <dbl>   <dbl>  <dbl>     <dbl>  <dbl>       <dbl>      <dbl>\n1   326  0.23 -472.     798. 0.0000452  1549. 0.00000600       0.516\n2   326  0.21 -628.     954. 0.0000471  1549. 0.00000892       0.616\n3   327  0.23 -472.     799. 0.0000452  1549. 0.00000602       0.516\n4   334  0.29   -7.00   341. 0.0000398  1549. 0.000000966      0.220\n5   335  0.31  148.     187. 0.0000382  1549. 0.000000278      0.121"
  },
  {
    "objectID": "slides/lect_18.html#visual-interpretation",
    "href": "slides/lect_18.html#visual-interpretation",
    "title": "Chapter 19: Linear patterns",
    "section": "Visual interpretation",
    "text": "Visual interpretation"
  },
  {
    "objectID": "slides/lect_18.html#extrapolation",
    "href": "slides/lect_18.html#extrapolation",
    "title": "Chapter 19: Linear patterns",
    "section": "Extrapolation",
    "text": "Extrapolation\nThis classic example is my favorite.\nThe model is only useful inside the values for which we have data. Outside of those values anything could happen."
  },
  {
    "objectID": "slides/lect_18.html#example-3",
    "href": "slides/lect_18.html#example-3",
    "title": "Chapter 19: Linear patterns",
    "section": "Example 3",
    "text": "Example 3\n\nthink about this in a manufacturing sense\n\nA manufacturing plant receives orders for customized mechanical parts. The orders vary in size, from about 30 to 130 units. After configuring the production line, a supervisor oversees the production. The least squares regression line that predicts time in hours using number of units to produce is:\n\\[\\widehat{\\text{time}} = 2.1 + 0.031 \\cdot \\text{Number of units}\\] a. Interpret the intercept of the estimated line.\nb. Interpret the slope of the estimated line.\nc. Using the fitted line, estimate the amount of time needed for an order with 100 unit. Is this estimate an extrapolation?\nd. Based on the fitted line, how much more time does an order with 100 units require over an order with 50 units?"
  },
  {
    "objectID": "slides/lect_18.html#example-3---solns",
    "href": "slides/lect_18.html#example-3---solns",
    "title": "Chapter 19: Linear patterns",
    "section": "Example 3 - solns",
    "text": "Example 3 - solns\n\n\nThe intercept (2.1 hrs) is the estimated time for any orders, regardless of size (to set up production)\nOnce it is running, the estimated tiem for an order is 0.031 hours per unit (or 60 * 0.031 = 1.9 minutes)\n\\(2.1 + 0.031\\cdot 100 = 5.2 hours\\), 100 units is inside the range\nFifty more units would need \\(0.031\\cdot 50 = 1.55\\) more hours"
  },
  {
    "objectID": "slides/lect_18.html#in-r-1",
    "href": "slides/lect_18.html#in-r-1",
    "title": "Chapter 19: Linear patterns",
    "section": "In R",
    "text": "In R\nThis plot has an obvious trend, the model does not fit the data well. We can use the visual association test to check.\n\n## plotting residuals\naugment(lm_fit) %>%\n  ggplot() + \n  geom_point(aes(x = carat, \n                 y = .resid))"
  },
  {
    "objectID": "slides/lect_18.html#sd-of-residuals",
    "href": "slides/lect_18.html#sd-of-residuals",
    "title": "Chapter 19: Linear patterns",
    "section": "sd of residuals",
    "text": "sd of residuals\nIf the residuals are nearly normal, we can summarise them with the mean and standard deviation.\n\\[ s_e = \\sqrt{\\frac{e_1^2 + e_2^2 + ... + e_n^2}{n-2}}\\] the denominator is \\(n-2\\) because we need two estimates for the line of best fit: \\(b_0\\) and \\(b_1\\).\nAlso known as RMSE root mean squared error. This can be used to compare multiple models to see which model best explains the variation in the data."
  },
  {
    "objectID": "slides/lect_18.html#in-r-2",
    "href": "slides/lect_18.html#in-r-2",
    "title": "Chapter 19: Linear patterns",
    "section": "In R",
    "text": "In R\n\nsummary(lm_fit)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18585.3   -804.8    -18.9    537.4  12731.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2256.36      13.06  -172.8   <2e-16 ***\ncarat        7756.43      14.07   551.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1549 on 53938 degrees of freedom\nMultiple R-squared:  0.8493,    Adjusted R-squared:  0.8493 \nF-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "slides/lect_04.html#definition-of-numeric-variable",
    "href": "slides/lect_04.html#definition-of-numeric-variable",
    "title": "Lec 4 - Exploring numeric data",
    "section": "Definition of numeric variable",
    "text": "Definition of numeric variable\nA numeric or quantitative variable is a variable that can be measured."
  },
  {
    "objectID": "slides/lect_04.html#the-purpose-of-exploratory-data-analysis-eda",
    "href": "slides/lect_04.html#the-purpose-of-exploratory-data-analysis-eda",
    "title": "Lec 4 - Exploring numeric data",
    "section": "The purpose of Exploratory Data Analysis (EDA)",
    "text": "The purpose of Exploratory Data Analysis (EDA)\n\nEDA is about learning the structure of a dataset through a series of numerical and graphical techniques.\nWhen you do EDA, you’ll look for both\n\ngeneral trends and\ninteresting outliers in your data.\n\n\ngenerate questions that will help inform subsequent analysis."
  },
  {
    "objectID": "slides/01_introduction.html#workspace-image",
    "href": "slides/01_introduction.html#workspace-image",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Workspace image",
    "text": "Workspace image\n\nSaving workspace"
  },
  {
    "objectID": "slides/01_introduction.html#rainbow-parenthesis",
    "href": "slides/01_introduction.html#rainbow-parenthesis",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Rainbow parenthesis",
    "text": "Rainbow parenthesis\n\nParenthesis are important"
  },
  {
    "objectID": "slides/01_introduction.html#code-font",
    "href": "slides/01_introduction.html#code-font",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Code font",
    "text": "Code font\n\nUpdate background and font color of code"
  },
  {
    "objectID": "slides/01_introduction.html#pane-layout",
    "href": "slides/01_introduction.html#pane-layout",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Pane layout",
    "text": "Pane layout\n\nUpdate panes and their layout"
  },
  {
    "objectID": "slides/01_introduction.html#new-directory",
    "href": "slides/01_introduction.html#new-directory",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "New directory",
    "text": "New directory\n\nSelect “New Directory”"
  },
  {
    "objectID": "slides/01_introduction.html#directory-type",
    "href": "slides/01_introduction.html#directory-type",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Directory type",
    "text": "Directory type\n\nThen select “New Project”"
  },
  {
    "objectID": "slides/01_introduction.html#new-folder",
    "href": "slides/01_introduction.html#new-folder",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "New folder",
    "text": "New folder\nHit the “Browse” button, then the “New Folder” button and name it “Stats1010.” All work related to this class will be in this folder, and we will create a new document for each chapter and homework.\n\nthen hit “Create project.”"
  },
  {
    "objectID": "slides/01_introduction.html#name-the-document",
    "href": "slides/01_introduction.html#name-the-document",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Name the document",
    "text": "Name the document\nName the document “lec_01” and hit create\n\nNaming the document"
  },
  {
    "objectID": "slides/01_introduction.html#insert-an-r-code-chuck",
    "href": "slides/01_introduction.html#insert-an-r-code-chuck",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Insert an R code chuck",
    "text": "Insert an R code chuck\nInsert a code chunk\n\nInsert code chunk"
  },
  {
    "objectID": "slides/01_introduction.html#write-your-first-line-of-coding",
    "href": "slides/01_introduction.html#write-your-first-line-of-coding",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Write your first line of coding",
    "text": "Write your first line of coding\nClick here or the qr code below to write your first line of code\n\n\n\nstat1010-f22.github.io/website"
  },
  {
    "objectID": "slides/lect_16.html#different-types-of-comparisons",
    "href": "slides/lect_16.html#different-types-of-comparisons",
    "title": "Chapter 17: Comparison",
    "section": "Different types of comparisons",
    "text": "Different types of comparisons\nFrom our last lecture, there are two courses that participants can take to improve their ESP abilities. One series of courses focuses on somatic training of the participant and the other on eye contact between the sender and receiver. For the purposes of this exercise, we let S be the event that a person took the first somatic course, and I be the event that a person took the first eye training course. This lesson will teach us how to assess these two courses. We will answer these questions:\n\n\nDoes a higher proportion of people sign up for subsequent eye contact or somatic courses?\nAre the earning for the somatic group higher than those of the eye contact group?"
  },
  {
    "objectID": "slides/lect_16.html#data---contd",
    "href": "slides/lect_16.html#data---contd",
    "title": "Chapter 17: Comparison",
    "section": "Data - cont’d",
    "text": "Data - cont’d\nThere are a few ways that we can get data to explore these hypotheses:\n\n\nExperiment: random sample, assigns treatment, compares between treatments.\nObtain random samples from two populations.\nCompare two sets of observations: can be problematic"
  },
  {
    "objectID": "slides/lect_16.html#confounding",
    "href": "slides/lect_16.html#confounding",
    "title": "Chapter 17: Comparison",
    "section": "Confounding",
    "text": "Confounding\nWhen levels of one variable are associated with levels of another the variables are said to be confounded.\nIn our example, perhaps we run a course Sedona, AZ where belief in ESP is high and people are likely to take the whole course. If the other was run in Phoenix, AZ this would not be the case. The belief in ESP in the location is confounded with taking subsequent courses."
  },
  {
    "objectID": "slides/lect_16.html#example-1",
    "href": "slides/lect_16.html#example-1",
    "title": "Chapter 17: Comparison",
    "section": "Example 1",
    "text": "Example 1\nWhich of the following appear safe from confounding and which appear to be contaminated?\n\nA comparison of two promotional displays using average daily sales in one store with one type of display and another store with a different type.\nA comparison of two promotional diplays using average daily sales in one store with one type of display on Monday and the other display on Friday.\nA comparison of two landscaping offers sent at random to potential customers in the same zip code."
  },
  {
    "objectID": "slides/lect_16.html#example-1---solns",
    "href": "slides/lect_16.html#example-1---solns",
    "title": "Chapter 17: Comparison",
    "section": "Example 1 - solns",
    "text": "Example 1 - solns\n\nConfounded by differences between the store, such as location or sales volume\nConfounded by differences in shopping patterns during the week.\nFree of confounding, though may not generalize to other zip codes"
  },
  {
    "objectID": "slides/lect_16.html#assumptions",
    "href": "slides/lect_16.html#assumptions",
    "title": "Chapter 17: Comparison",
    "section": "Assumptions",
    "text": "Assumptions\n\nNo obvious lurking variable or confounders\nSRS condition, or independent random samples from two populations\nWe need to check \\(\\begin{eqnarray*} n_1\\cdot \\hat{p}_1 \\geq 10 \\\\ n_1\\cdot(1-\\hat{p}_1) \\geq 10\\\\ n_2\\cdot \\hat{p}_2 \\geq 10\\\\ n_2\\cdot(1-\\hat{p}_2) \\geq 10 \\end{eqnarray*}\\)"
  },
  {
    "objectID": "slides/lect_16.html#hypothesis-test-for-a-difference-in-proportions",
    "href": "slides/lect_16.html#hypothesis-test-for-a-difference-in-proportions",
    "title": "Chapter 17: Comparison",
    "section": "Hypothesis test for a difference in proportions",
    "text": "Hypothesis test for a difference in proportions\n\\[ z = \\frac{\\hat{p}_1 - \\hat{p}_2 - D_0}{se(\\hat{p}_1 - \\hat{p}_2)}\\]"
  },
  {
    "objectID": "slides/lect_16.html#example-2",
    "href": "slides/lect_16.html#example-2",
    "title": "Chapter 17: Comparison",
    "section": "Example 2",
    "text": "Example 2\nIn the ESP example, spiritual groups decided to randomly assign members to either the somatic (\\(n_s = 809\\)) or eye contact group (\\(n_i = 646\\)). From those in the somatic group \\(280\\) re-enrolled, subsequent eye contact groups had \\(197\\). Perform a two sample \\(z\\)-test for proportions to determine if the enrollment in the somatic course was 2% more, on average, than that in the eye contact course."
  },
  {
    "objectID": "slides/lect_16.html#steps-for-a-hypothesis-test",
    "href": "slides/lect_16.html#steps-for-a-hypothesis-test",
    "title": "Chapter 17: Comparison",
    "section": "Steps for a hypothesis test",
    "text": "Steps for a hypothesis test\n\nState hypotheses\nDetermine the level of significance\nCheck conditions\nCalculate test statistic\nCompute p-value\nGeneric conclusion\nInterpret in context"
  },
  {
    "objectID": "slides/lect_16.html#example-2---solns",
    "href": "slides/lect_16.html#example-2---solns",
    "title": "Chapter 17: Comparison",
    "section": "Example 2 - solns",
    "text": "Example 2 - solns\n\n\n\\(H_0: p_s - p_i \\leq 0.02\\) and \\(H_A: p_s - p_i > 0.02\\)\n\\(\\alpha = 0.05\\)\nYes, 280/809*809 and (1-280/809)*809 both > 10 Yes, 197/646*646 and (1-197/646)*646 both > 10\n\\(\\hat{p_i} = \\frac{x_i}{n_i} = \\frac{197}{646} = 0.305\\) \\(\\hat{p_s} = \\frac{x_s}{n_s} = \\frac{280}{809} = 0.346\\)"
  },
  {
    "objectID": "slides/lect_16.html#example-2---solns-1",
    "href": "slides/lect_16.html#example-2---solns-1",
    "title": "Chapter 17: Comparison",
    "section": "Example 2 - solns",
    "text": "Example 2 - solns\n\n\n\\(\\begin{aligned} se(\\hat{p}_s - \\hat{p}_i) &= \\sqrt{\\frac{\\hat{p}_s(1-\\hat{p}_s)}{n_s}+\\frac{\\hat{p}_i(1-\\hat{p}_i)}{n_i}} \\\\ &= \\sqrt{\\frac{0.305(1-0.305)}{646}+\\frac{0.346(1-0.346)}{809}} \\\\ &= \\sqrt{0.0003281 + 0.0002797} \\approx 0.02465 \\end{aligned}\\) \\(\\begin{aligned} z_0 &= \\frac{\\hat{p_s} - \\hat{p_i} - D_0} {se(\\hat{p_s} - \\hat{p_i})} \\\\ &= \\frac{0.346 - 0.305 - 0.02} {0.02465} \\\\ &\\approx 0.851927 \\end{aligned}\\)\n\n1 - pnorm(0.851927) or \\(\\approx 0.197\\)\n\nSince \\(0.197 > \\alpha = 0.05\\) this is not significant.\nWe fail to reject \\(H_0\\). The reenrollment in the somatic course is not significantly more than the reenrollment in the eye contact course."
  },
  {
    "objectID": "slides/lect_16.html#math---two-sample-ci-for-props",
    "href": "slides/lect_16.html#math---two-sample-ci-for-props",
    "title": "Chapter 17: Comparison",
    "section": "Math - two sample CI for props",
    "text": "Math - two sample CI for props\n\\[ \\hat{p}_1 - \\hat{p}_2 - z_{\\alpha/2} se(\\hat{p}_1 - \\hat{p}_2) \\text{  to  }\\\\ \\hat{p}_1 - \\hat{p}_2 + z_{\\alpha/2} se(\\hat{p}_1 - \\hat{p}_2)\\]"
  },
  {
    "objectID": "slides/lect_16.html#assumptions-1",
    "href": "slides/lect_16.html#assumptions-1",
    "title": "Chapter 17: Comparison",
    "section": "Assumptions",
    "text": "Assumptions\n\nNo obvious lurking variables or confounders\nSRS condition\nSample size condition"
  },
  {
    "objectID": "slides/lect_16.html#example-3",
    "href": "slides/lect_16.html#example-3",
    "title": "Chapter 17: Comparison",
    "section": "Example 3",
    "text": "Example 3\nIn the ESP example, spiritual groups decided to randomly assign members to either the somatic (\\(n_s = 809\\)) or eye contact group (\\(n_i = 646\\)). From those in the somatic contact group \\(280\\) re-enrolled, subsequent eye contact groups had \\(197\\). Find the 95% confidence interval for the difference between the proportions who take subsequent courses on the somatic and eye contact group."
  },
  {
    "objectID": "slides/lect_16.html#example-3---solns",
    "href": "slides/lect_16.html#example-3---solns",
    "title": "Chapter 17: Comparison",
    "section": "Example 3 - solns",
    "text": "Example 3 - solns\n\n\n\\(\\begin{aligned} \\text{lower bound} &= \\hat{p}_s - \\hat{p}_i - z_{\\alpha/2} se(\\hat{p}_s - \\hat{p}_i) \\\\ &= 0.346 - 0.305 - 1.96 \\cdot 0.02465 \\\\ &= 0.041 - 0.0483 \\approx -0.0073 \\end{aligned}\\)\n\\(\\begin{aligned} \\text{upper bound} &= \\hat{p}_s - \\hat{p}_i + z_{\\alpha/2} se(\\hat{p}_s - \\hat{p}_i) \\\\ &= 0.346 - 0.305 + 1.96 \\cdot 0.02465 \\\\ &= 0.041 + 0.0483 \\approx 0.0893 \\end{aligned}\\)"
  },
  {
    "objectID": "slides/lect_16.html#interpretation",
    "href": "slides/lect_16.html#interpretation",
    "title": "Chapter 17: Comparison",
    "section": "Interpretation",
    "text": "Interpretation\nSince \\(0 \\text{ is in } [-0.0073, 0.0893]\\) the difference is not statistically signficant at the \\(95\\%\\) significance level. It is possible that the reenrollment rates for eye contact and the somatic course come from populations with the same proportion. We do not know which is higher, either could be.\nIf \\(0\\) is not in the \\(95\\%\\) confidence interval, then the difference is significant and we can be \\(95\\%\\) confident that the difference is between the lower and upper bounds. Any value in the region could plausibly be the difference in proportions between the two populations."
  },
  {
    "objectID": "slides/lect_16.html#assumptions-2",
    "href": "slides/lect_16.html#assumptions-2",
    "title": "Chapter 17: Comparison",
    "section": "Assumptions",
    "text": "Assumptions\n\nNo lurking variables\nSRS condition\nSimilar variances\nEach sample must exceed \\(10|K_4|\\)"
  },
  {
    "objectID": "slides/lect_16.html#in-r",
    "href": "slides/lect_16.html#in-r",
    "title": "Chapter 17: Comparison",
    "section": "In R",
    "text": "In R\nThis computation is long and complicated, so it’s best done in R: t.test()\nSince we only have summary data for the ESP example, we’ll show with the diamonds dataset. In this situation, we’ll ask if there is a relationship between carat (weight of the diamond) and color (D is best and J is worst). Which color has on average the largest diamonds?\n\\[ H_0: \\mu_D - \\mu_j \\leq D_0\\]"
  },
  {
    "objectID": "slides/lect_16.html#in-r-1",
    "href": "slides/lect_16.html#in-r-1",
    "title": "Chapter 17: Comparison",
    "section": "In R",
    "text": "In R\n\n## install packages\nlibrary(tidyverse)\n\nd.color <- diamonds %>% \n  filter(color == \"D\") %>% \n  select(carat)\n  \nj.color <- diamonds %>% \n  filter(color == \"J\") %>% \n  select(carat)\n\n## 2 - sided test\n## Alternative hypothesis: x != y\nt.test(x = d.color, y = j.color, alternative = \"two.sided\")\n\n## Alternative hypothesis: x < y by 0.1\nt.test(x = d.color, y = j.color, alternative  = \"less\", mu = .1)"
  },
  {
    "objectID": "slides/lect_16.html#in-r-2",
    "href": "slides/lect_16.html#in-r-2",
    "title": "Chapter 17: Comparison",
    "section": "In R",
    "text": "In R\n\n\n\n    Welch Two Sample t-test\n\ndata:  d.color and j.color\nt = -41.811, df = 3683.7, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5279915 -0.4806923\nsample estimates:\nmean of x mean of y \n0.6577948 1.1621368 \n\n\n\n    Welch Two Sample t-test\n\ndata:  d.color and j.color\nt = -50.101, df = 3683.7, p-value < 2.2e-16\nalternative hypothesis: true difference in means is less than 0.1\n95 percent confidence interval:\n       -Inf -0.4844961\nsample estimates:\nmean of x mean of y \n0.6577948 1.1621368"
  },
  {
    "objectID": "slides/lect_16.html#interpretation-1",
    "href": "slides/lect_16.html#interpretation-1",
    "title": "Chapter 17: Comparison",
    "section": "Interpretation",
    "text": "Interpretation\nIf the p-value is less than \\(\\alpha\\) there is evidence against \\(H_0\\) and we can reject \\(H_0\\) in favor of the alternative.\n“If the p-value is low then the null must go.”\n2 - sided\nThere is extremely strong evidence that the mean carat for diamonds with the best color (D) is not the same as that for diamonds with the worst color (J).\nalternative = “less”\nThere is extremely strong evidence that the mean carat for diamonds with the best color (D) is smaller by at least 0.1 then diamonds with the worst color (J).\nBut what is the magnitude of the difference?"
  },
  {
    "objectID": "slides/lect_16.html#in-r-3",
    "href": "slides/lect_16.html#in-r-3",
    "title": "Chapter 17: Comparison",
    "section": "In R",
    "text": "In R\n\nt.test(x = d.color, y = j.color, conf.level = 0.95)$conf.int\n\n[1] -0.5279915 -0.4806923\nattr(,\"conf.level\")\n[1] 0.95"
  },
  {
    "objectID": "slides/lect_16.html#another-example",
    "href": "slides/lect_16.html#another-example",
    "title": "Chapter 17: Comparison",
    "section": "Another example",
    "text": "Another example\nCertain people refused to get vacinated from COVID-19. To compare COVID infection rates it’s best to pair people who are vaccinated and not vaccinated but who also have similar education levels, incomes, and risk of infection. This hapenned last year in a UK government report."
  },
  {
    "objectID": "slides/lect_16.html#math",
    "href": "slides/lect_16.html#math",
    "title": "Chapter 17: Comparison",
    "section": "Math",
    "text": "Math\nGiven the paired data we find the differences (\\(d_i = x_i – y_i\\))\nThe \\(100(1 - \\alpha)%\\) confidence paired t- interval is\n\\[\\bar{d} \\pm  t_{\\alpha/2, n-1} \\frac{s_d}{\\sqrt{n}}\\]\nChecklist: No obvious lurking variables. SRS condition. Sample size condition."
  },
  {
    "objectID": "slides/lect_16.html#in-r-4",
    "href": "slides/lect_16.html#in-r-4",
    "title": "Chapter 17: Comparison",
    "section": "In R",
    "text": "In R\n\n# Weight of mice before treatment\nbefore <-c(200.1, 190.9, 192.7, 213, 241.4, 196.9, 172.2, 185.5, 205.2, 193.7)\n# Weight of mice after treatment\nafter <-c(392.9, 393.2, 345.1, 393, 434, 427.9, 422, 383.9, 392.3, 352.2)\n\n# A tibble\nmice <- tibble(\n  group = rep(c(\"before\", \"after\"), each = 10),\n  weight = c(before, after)\n  )\n\nt.test(weight ~ group, data = mice, paired = TRUE)"
  },
  {
    "objectID": "slides/lect_16.html#in-r-5",
    "href": "slides/lect_16.html#in-r-5",
    "title": "Chapter 17: Comparison",
    "section": "In R",
    "text": "In R\n\n\n\n    Paired t-test\n\ndata:  weight by group\nt = 20.883, df = 9, p-value = 6.2e-09\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 173.4219 215.5581\nsample estimates:\nmean difference \n         194.49"
  },
  {
    "objectID": "slides/lect_16.html#plot",
    "href": "slides/lect_16.html#plot",
    "title": "Chapter 17: Comparison",
    "section": "Plot",
    "text": "Plot\n\nggplot(data = mice) +\n  geom_boxplot(aes(x = group, y = weight))"
  },
  {
    "objectID": "slides/lect_16.html#interpretation-2",
    "href": "slides/lect_16.html#interpretation-2",
    "title": "Chapter 17: Comparison",
    "section": "Interpretation",
    "text": "Interpretation\nThere is very strong evidence that the underlying population means of the mice before and after treatment are not the same. On average, the mice are much heavier after treatment.\n\n\nstat1010-f22.github.io/website"
  },
  {
    "objectID": "slides/here_lecture.html#why",
    "href": "slides/here_lecture.html#why",
    "title": "Here lecture",
    "section": "Why?",
    "text": "Why?\nToday we will explore the here package. This package is used to organize files within R:\n\ninstall.packages(\"here\")"
  },
  {
    "objectID": "slides/here_lecture.html#including-your-own-data",
    "href": "slides/here_lecture.html#including-your-own-data",
    "title": "Here lecture",
    "section": "Including your own data",
    "text": "Including your own data\nTo add data to your directory, first create a folder that\n\nFigure @ref(fig:3) Adding a folder.and name it “data”. All data related to this chapter should be contained in this folder. In your file management system place your data in this folder. Then in the “Files” menu double-click on the dataset that you would like to import into R."
  },
  {
    "objectID": "slides/here_lecture.html#importing-data",
    "href": "slides/here_lecture.html#importing-data",
    "title": "Here lecture",
    "section": "Importing data",
    "text": "Importing data\n\n\n\nDouble click on the data you would like to install in the data folder in your project and select “Import Dataset.”\n\n\nIf this is the first time you are importing data, you may be prompted to download a package first. Follow the Graphical User Interface (GUI) to import the data. Check that the data is as you expect and update “Import Options” until it seems reasonable. Copy the coding from “Code Preview” and save put it in your document “so that when you restart R, you have coding that will import the data.\n\n\n\nWhen the data is imported, check the “Data Preview” to make sure it is what you expect. If not, change the “Import Options.”\n\n\n\n\nstat1010-f22.github.io/website"
  },
  {
    "objectID": "slides/lect_08.html#price-vs-carat",
    "href": "slides/lect_08.html#price-vs-carat",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Price vs carat",
    "text": "Price vs carat"
  },
  {
    "objectID": "slides/lect_08.html#visual-association-test",
    "href": "slides/lect_08.html#visual-association-test",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Visual association test",
    "text": "Visual association test\n\nAsk them to run the coding in RStudio and see what they get - can connect some of their computers to the big screen\nCan they see a difference between their plots and the real plot? What is the difference?\n\n\n\n\n\n\n\n\n\n\ndiamonds %>% # filtered data\n  slice_sample(n = nrow(.)) %>% # random sample rows\n  pull(carat) %>% # take out the variable carat\n  bind_cols(., diamonds$price) %>% # price in the same order and bound to carat in different order\n  ggplot() + # into ggplot\n  geom_point(aes(y = ...2, x = ...1)) + # using the new names\n  labs(title = \"Simulated association test\", \n       x = \"Weight of diamond in carat\", \n       y = \"Price of diamonds in US$\")"
  },
  {
    "objectID": "slides/lect_08.html#describing-a-scatter-plot",
    "href": "slides/lect_08.html#describing-a-scatter-plot",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Describing a scatter plot",
    "text": "Describing a scatter plot\n\nWhat is the difference between the actual plot and the simulated plots?\nDiscuss each point.\nBig words make you sound smart to people that aren’t that smart.\n\n\n\n\nTrend or direction\n\npositive\nnegative\n\nCurvature\n\nlinear\nnonlinear\n\nexponential\nquadratic\n\n\n\n\n\nVariation\n\nhomoscedasticity (similar variance)\nheteroscedasticity (different variance)\n\nOutliers\n\nany weird points (explore these)\n\nGroupings"
  },
  {
    "objectID": "slides/lect_08.html#describe-these-plots",
    "href": "slides/lect_08.html#describe-these-plots",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Describe these plots",
    "text": "Describe these plots\n\nWhat are the relationships between each plot?\nWhat do the numbers mean?"
  },
  {
    "objectID": "slides/lect_08.html#measuring-association-1",
    "href": "slides/lect_08.html#measuring-association-1",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Measuring association",
    "text": "Measuring association\n\nRemind them of the variance and what we did with the squares. What squares could we use now?"
  },
  {
    "objectID": "slides/lect_08.html#covariance",
    "href": "slides/lect_08.html#covariance",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Covariance",
    "text": "Covariance\n\nIf the product is negative then there it contributes to a negative trend - downhill from left to right.\nSame if positive\nhard to get ones that are negative for this dataset b/c tbere is a really strong positive association"
  },
  {
    "objectID": "slides/lect_08.html#covariance-math",
    "href": "slides/lect_08.html#covariance-math",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Covariance math",
    "text": "Covariance math\n\nThis is a measure of the squares. We’d like a measure that is one dimensional. We want to look at more than just squares.\n\n\\(cov(x, y) = \\frac{(x_1 - \\bar{x})(y_1 - \\bar{y}) + (x_2 - \\bar{x})(y_2 - \\bar{y}) + \\ldots + (x_n - \\bar{x})(y_n - \\bar{y})}{n-1}\\)"
  },
  {
    "objectID": "slides/lect_08.html#correlation-math",
    "href": "slides/lect_08.html#correlation-math",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Correlation math",
    "text": "Correlation math\n\nStandardizes the strength of the association\n\n\\[corr(x, y) = \\frac{cov(x, y)}{s_x \\cdot s_y}\\]"
  },
  {
    "objectID": "slides/lect_08.html#correlation-characteristics",
    "href": "slides/lect_08.html#correlation-characteristics",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Correlation characteristics",
    "text": "Correlation characteristics\n\nStandardizes the strength of the association\n\n\n\nReferred to as \\(r\\)\nStrength of linear association\n\\(r\\) is always between \\(-1\\) and \\(+1\\), \\(-1 \\leq r \\leq 1\\).\n\\(r\\) does not have units"
  },
  {
    "objectID": "slides/lect_08.html#computing-in-r",
    "href": "slides/lect_08.html#computing-in-r",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Computing in R",
    "text": "Computing in R\n\nStandardizes the strength of the association\n\n\n# covariance of price and carat\ncov(diamonds$price, diamonds$carat)\n\n[1] 1742.765\n\n# correlation of price and carat\ncor(diamonds$price, diamonds$carat)\n\n[1] 0.9215913\n\n# coding for the pairs plots\n# library(GGally)\n# diamonds %>% # dataset\n#  select_if(is.numeric) %>% # numeric variables\n#  filter(y < 20, # y less than 20\n#         z < 20, # z less than 20\n#         table < 90) %>% # table less than 90\n#  ggpairs(.) # make the plot"
  },
  {
    "objectID": "slides/lect_08.html#gradient-and-slope",
    "href": "slides/lect_08.html#gradient-and-slope",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Gradient and slope",
    "text": "Gradient and slope\n\nPull out your calculators and work on this\n\n\\[ m = \\frac{r \\cdot s_y}{s_x} \\] \\[ b = \\bar{y} - m \\bar{x} \\]"
  },
  {
    "objectID": "slides/lect_08.html#fitting-by-hand",
    "href": "slides/lect_08.html#fitting-by-hand",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Fitting by hand",
    "text": "Fitting by hand\n\n\n\\(r = 0.9215913\\)\n\\(s_{price} = 3989.44\\)\n\\(s_{carat} = 0.4740112\\)\n\\(m = \\frac{r \\cdot s_{price}}{s_{carat}}\\)\n\\[\\begin{aligned}\n   m &= \\frac{r \\cdot s_{price}}{s_{carat}} \\\\\n&= \\frac{0.9215913 \\cdot 3989.44}{0.4740112} \\\\\n&= 7756.426\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lect_08.html#fitting-by-hand-1",
    "href": "slides/lect_08.html#fitting-by-hand-1",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Fitting by hand",
    "text": "Fitting by hand\n\n\n\\(b = \\overline{price} - m \\cdot \\overline{carat}\\)\n\\(\\overline{price} = 3932.8\\)\n\\(\\overline{carat} = 0.7979397\\)\n\\(m = 7756.426\\)\n\\[\\begin{aligned}\nb &= \\overline{price} - m \\cdot \\overline{carat} \\\\\n&= 3932.8 - 7756.426 \\cdot 0.7979397\\\\\n&= -2256.36\n\\end{aligned}\\]\nOur model is: \\(\\widehat{\\text{price}} = -2256.36 + 7756.426 \\cdot \\text{carat}\\)"
  },
  {
    "objectID": "slides/lect_08.html#prediction-by-hand",
    "href": "slides/lect_08.html#prediction-by-hand",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Prediction by hand",
    "text": "Prediction by hand\n\nPull out your calculators and work on this\n\nfor carat values \\(2.5\\)\n\n\n\\(\\widehat{\\text{price}} = -2256.36 + 7756.426 \\cdot \\text{carat}\\)\n\\[\\begin{aligned}\n\\widehat{\\text{price}}  &= -2256.36 + 7756.426 \\cdot \\text{carat} \\\\\n&= -2256.36 + 7756.426 \\cdot 2.5 \\\\\n&= \\$17135\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lect_08.html#fit-predict-in-r",
    "href": "slides/lect_08.html#fit-predict-in-r",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Fit & predict in R",
    "text": "Fit & predict in R\n\nfirst use of tidymodels package\n\n\n# library tidymodels\nlibrary(tidymodels)\n\n## Assign the least\n## squares line\nleast_squares_fit <- \n  linear_reg() %>% \n  set_engine(\"lm\") %>% \n  fit(price ~ carat, data = diamonds) \n## NOTE: outcome first, predictor second\n\n## Find the prediction\npredict(least_squares_fit, tibble(carat = c(1, 2, 2.5, 4)))\n\n# A tibble: 4 × 1\n   .pred\n   <dbl>\n1  5500.\n2 13256.\n3 17135.\n4 28769."
  },
  {
    "objectID": "slides/lect_08.html#warnings",
    "href": "slides/lect_08.html#warnings",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Warnings",
    "text": "Warnings\n\n\nALWAYS draw plots first\nnumeric variables\nLinear relationship\nCheck for outliers\nLurking variables"
  },
  {
    "objectID": "slides/lect_08.html#more-spurious-correlations",
    "href": "slides/lect_08.html#more-spurious-correlations",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "More spurious correlations",
    "text": "More spurious correlations\n\n\nThese variables are called lurking variables or confounders.\nLurking variables are not considered in the statistical analysis\nConfounders are considered\nThere are both known and unknown confounders, uknown confounders are lurking variables.\n\n\nClick here for more spurious correlations"
  },
  {
    "objectID": "slides/lect_03.html#definition-of-categorical-variable",
    "href": "slides/lect_03.html#definition-of-categorical-variable",
    "title": "Lec 3 - Exploring categorical data",
    "section": "Definition of categorical variable",
    "text": "Definition of categorical variable\nA categorical or qualitative variable is a variable that can not be measured. They are descriptors or grouping factors."
  },
  {
    "objectID": "slides/lect_03.html#the-purpose-of-exploring-categorical-variables",
    "href": "slides/lect_03.html#the-purpose-of-exploring-categorical-variables",
    "title": "Lec 3 - Exploring categorical data",
    "section": "The purpose of exploring categorical variables",
    "text": "The purpose of exploring categorical variables\n\nExploratory Data Analysis is about learning the structure of a dataset through a series of numerical and graphical techniques.\nWhen you do EDA, you’ll look for both\n\ngeneral trends and\ninteresting outliers in your data.\n\n\ngenerate questions that will help inform subsequent analysis."
  },
  {
    "objectID": "slides/lect_17.html#revision",
    "href": "slides/lect_17.html#revision",
    "title": "Chapter 18: Inference for counts",
    "section": "Revision",
    "text": "Revision\nWe have discussed this before and now we will formalize this work. Like walking up a lighthouse."
  },
  {
    "objectID": "slides/lect_17.html#test-of-independence",
    "href": "slides/lect_17.html#test-of-independence",
    "title": "Chapter 18: Inference for counts",
    "section": "Test of independence",
    "text": "Test of independence\n\\[H_0: \\text{Qualitative variable 1 and Qualitative variable 2 are independent}\\]\n\\[H_A: \\text{Qualitative variable 1 and Qualitative variable 2 are not independent}\\]"
  },
  {
    "objectID": "slides/lect_17.html#example-1",
    "href": "slides/lect_17.html#example-1",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 1",
    "text": "Example 1\nA manufacturing firm is considering a shift from a 5-day workweek (8 hours per day) to a 4-day workweek (10 hours per day). Samples of the preferences of 188 employees in two divisions produced the following contingency table:\nObserved counts\n\n\n\n\n\nDivisions\n\n\n\n\n\n\n\n\nClerical\nProduction\nTotal\n\n\nPreferences\n5-day\n17\n46\n63\n\n\n\n4-day\n28\n38\n66\n\n\n\nTotal\n45\n84\n129\n\n\n\na. What would it mean if the preference of employees is independent of division?\nb. State \\(H_0\\) for the \\(\\chi^2\\) test of independence in terms of the parameters of two segments of the population of employees."
  },
  {
    "objectID": "slides/lect_17.html#example-1---solns",
    "href": "slides/lect_17.html#example-1---solns",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 1 - solns",
    "text": "Example 1 - solns\n\nIndependence means that all the percent of people who prefer a 5-day work week is the same in all divisions. Lack of independence implies different percentages across divisions\n\\(H_0: p_{clerical} = p_{production}\\) \\(H_A: \\text{at least one of these differs}\\)"
  },
  {
    "objectID": "slides/lect_17.html#calculating-chi2",
    "href": "slides/lect_17.html#calculating-chi2",
    "title": "Chapter 18: Inference for counts",
    "section": "Calculating \\(\\chi^2\\)",
    "text": "Calculating \\(\\chi^2\\)\nIf one variable is independent of the other, then the variable should have the same proportion in each level as the totals have in each level.\nExpected counts\n\n\n\n\n\n\n\n\n\n\n\n\nDivisions\n\n\n\n\n\n\n\n\nClerical\nProduction\nTotal\n\n\nPreferences\n5-day\n\\(45/129\\cdot63 \\approx 22\\)\n\\(84/129\\cdot63 \\approx 41\\)\n63\n\n\n\n4-day\n\\(45/129\\cdot66 \\approx 23\\)\n\\(84/129\\cdot66 \\approx 43\\)\n66\n\n\n\nTotal\n45\n84\n129"
  },
  {
    "objectID": "slides/lect_17.html#calculating-chi2-1",
    "href": "slides/lect_17.html#calculating-chi2-1",
    "title": "Chapter 18: Inference for counts",
    "section": "Calculating \\(\\chi^2\\)",
    "text": "Calculating \\(\\chi^2\\)\n\\[ \\chi^2 = sum\\frac{(observed - expected)^2}{expected}\\]\nWe want to know how much this varies from the expected that is why the expected is the denominator."
  },
  {
    "objectID": "slides/lect_17.html#example-2",
    "href": "slides/lect_17.html#example-2",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 2",
    "text": "Example 2\nFind the expected counts for the following observed variables\n\n\n\n\n \n  \n    gender \n    happy \n    meh \n    sad \n  \n \n\n  \n    female \n    100 \n    30 \n    110 \n  \n  \n    male \n    70 \n    32 \n    120"
  },
  {
    "objectID": "slides/lect_17.html#example-2---solns",
    "href": "slides/lect_17.html#example-2---solns",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 2 - solns",
    "text": "Example 2 - solns\n\n\n\n\n \n  \n    happy \n    meh \n    sad \n  \n \n\n  \n    88.31169 \n    32.20779 \n    119.4805 \n  \n  \n    81.68831 \n    29.79221 \n    110.5195"
  },
  {
    "objectID": "slides/lect_17.html#plot",
    "href": "slides/lect_17.html#plot",
    "title": "Chapter 18: Inference for counts",
    "section": "Plot",
    "text": "Plot\n\n\n\ngender <- c(rep(\"male\", 222), \n            rep(\"female\", 240))\nmood <- c(rep(\"happy\", 70), rep(\"meh\",32),\n          rep(\"sad\", 120), rep(\"happy\", 100),\n          rep(\"meh\", 30), rep(\"sad\", 110))\nd <- tibble(gender, mood)\n \n## draw the plot\nggplot(data = d) +\n  geom_bar(aes(x = mood, fill = gender),\n           position = \"fill\")"
  },
  {
    "objectID": "slides/lect_17.html#reading-plots",
    "href": "slides/lect_17.html#reading-plots",
    "title": "Chapter 18: Inference for counts",
    "section": "Reading plots",
    "text": "Reading plots\nIf the color change in the bars are approximately at the same height in every level of the variable on the \\(x\\) axis, this is evidence against rejecting \\(H_0\\). We might be more inclined to think that \\(H_0\\) is true."
  },
  {
    "objectID": "slides/lect_17.html#assumptions",
    "href": "slides/lect_17.html#assumptions",
    "title": "Chapter 18: Inference for counts",
    "section": "Assumptions",
    "text": "Assumptions\n\nNo lurking variables\nData is a random sample from the population\nCategories must be mutually exclusive\nEach cell count must be at least 5 or 10 (more here)"
  },
  {
    "objectID": "slides/lect_17.html#degrees-of-freedom",
    "href": "slides/lect_17.html#degrees-of-freedom",
    "title": "Chapter 18: Inference for counts",
    "section": "degrees of freedom",
    "text": "degrees of freedom\n\\[df \\text{ for a } \\chi^2 \\text{ test of independence} = (r-1)(c-1)\\]\nwhere \\(r= \\text{number of rows}\\) and \\(c= \\text{number of columns}\\)\nAssumptions:\n\nEach cell count ideally 10, or 5 if the test has 4 or more degrees of freedom."
  },
  {
    "objectID": "slides/lect_17.html#in-r",
    "href": "slides/lect_17.html#in-r",
    "title": "Chapter 18: Inference for counts",
    "section": "In R",
    "text": "In R\n\n\nFind the totals\nCompute the expected counts\nCalculate \\(\\chi^2 = sum\\frac{(observed - expected)^2}{expected}\\)\nFind the df \\((r-1)(c-1)\\)\nSolve for the p-value 1 - pchisq(chi-square, df)"
  },
  {
    "objectID": "slides/lect_17.html#example-3",
    "href": "slides/lect_17.html#example-3",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 3",
    "text": "Example 3\nIn example 2, perform a hypothesis test and find the \\(\\chi^2\\) value and the p-value. Are gender and mood independent?"
  },
  {
    "objectID": "slides/lect_17.html#steps-for-a-hypothesis-test",
    "href": "slides/lect_17.html#steps-for-a-hypothesis-test",
    "title": "Chapter 18: Inference for counts",
    "section": "Steps for a hypothesis test",
    "text": "Steps for a hypothesis test\n\nState hypotheses\nDetermine the level of significance\nCheck conditions\nCalculate test statistic\nCompute p-value\nGeneric conclusion\nInterpret in context"
  },
  {
    "objectID": "slides/lect_17.html#example-3---solns",
    "href": "slides/lect_17.html#example-3---solns",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 3 - solns",
    "text": "Example 3 - solns\n\n\n\\(H_0: p_h = p_m = p_s\\) and \\(H_A: \\text{at least one differs}\\)\n\\(\\alpha = 0.05\\)\nYes.\n\\(\\begin{aligned} \\chi^2 &= sum\\frac{(observed - expected)^2}{expected} \\\\ &= \\frac{(100-88.3)^2}{88.3} + \\frac{(70-81.7)^2}{81.7} + ... + \\frac{(120-119.5)^2}{119.5}\\\\ &\\approx 5.0999\\end{aligned}\\) \\(df = (3-1)\\cdot(2-1)\\)"
  },
  {
    "objectID": "slides/lect_17.html#example-3---solns-1",
    "href": "slides/lect_17.html#example-3---solns-1",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 3 - solns",
    "text": "Example 3 - solns\n\n\n1 - pchisq(5.099, df = 2)\\(\\approx 0.078\\)\nWe fail to reject \\(H_0\\)\nWe do not have evidence that gender and mood are associated, the data suggests they are independent."
  },
  {
    "objectID": "slides/lect_17.html#example-3---solns-2",
    "href": "slides/lect_17.html#example-3---solns-2",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 3 - solns",
    "text": "Example 3 - solns\n\nd %>% \n  count(mood, gender) %>% \n  pivot_wider(names_from = mood, values_from = n) %>% \n  select(-gender) %>% \n  chisq.test()\n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 5.0999, df = 2, p-value = 0.07809"
  },
  {
    "objectID": "slides/lect_17.html#test-for-goodness-of-fit",
    "href": "slides/lect_17.html#test-for-goodness-of-fit",
    "title": "Chapter 18: Inference for counts",
    "section": "Test for goodness of fit",
    "text": "Test for goodness of fit\nWhen an outcome is either Binomial or Poisson one way to check that predictions are correct is to perform a \\(\\chi^2\\) test on the actual and predicted.\n\n\nA small p-value to indicates that these values are not independent."
  },
  {
    "objectID": "slides/lect_11.html#binomial---normal",
    "href": "slides/lect_11.html#binomial---normal",
    "title": "Chapter 12: The normal probability model",
    "section": "Binomial -> Normal",
    "text": "Binomial -> Normal\nAs the number of trials increases, the binomial pdf becomes well approximated by a normal distribution.\n“Observed data often represent the accumulation of many small factors.”"
  },
  {
    "objectID": "slides/lect_11.html#central-limit-theorem",
    "href": "slides/lect_11.html#central-limit-theorem",
    "title": "Chapter 12: The normal probability model",
    "section": "Central limit theorem",
    "text": "Central limit theorem\n\nThere are multiple versions of the CLT\n\nThe probability distribution of a sum of independent random variables of comparable variance approaches a normal distribution as the number of summed random variables increases."
  },
  {
    "objectID": "slides/lect_11.html#shifts-scales",
    "href": "slides/lect_11.html#shifts-scales",
    "title": "Chapter 12: The normal probability model",
    "section": "Shifts & scales",
    "text": "Shifts & scales"
  },
  {
    "objectID": "slides/lect_11.html#standardizing",
    "href": "slides/lect_11.html#standardizing",
    "title": "Chapter 12: The normal probability model",
    "section": "Standardizing",
    "text": "Standardizing\n\nBooks and tables of just normal probabilities to multiple decimal places.\n\nHistorically, it could be quite hard to find probabilities, so standardizing was important.\n\n\nuse shift and scale information from above\n\\(Z = \\frac{X-\\mu_X}{\\sigma_X}\\)\nFind \\(E(Z)\\)\n\\(E(Z) = E(\\frac{X-\\mu_X}{\\sigma_X})\\)\n\\[\\begin{aligned}\nE(Z) & = E(\\frac{X-\\mu_X}{\\sigma_X})\\\\\n& = \\frac{1}{\\sigma_X}E(X-\\mu_X) \\\\\n&= 0\n\\end{aligned}\\]\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_11.html#standardizing-1",
    "href": "slides/lect_11.html#standardizing-1",
    "title": "Chapter 12: The normal probability model",
    "section": "Standardizing",
    "text": "Standardizing\n\n\nuse shift and scale information from above\n\\(Z = \\frac{X-\\mu_X}{\\sigma_X}\\)\nFind \\(Var(Z)\\)\n\\[\\begin{aligned}\nVar(Z) & = Var(\\frac{X-\\mu_X}{\\sigma_X})\\\\\n& = \\frac{1}{\\sigma^2}Var(X-\\mu_X) \\\\\n&= \\frac{\\sigma^2}{\\sigma^2} = 1\n\\end{aligned}\\]\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_11.html#example",
    "href": "slides/lect_11.html#example",
    "title": "Chapter 12: The normal probability model",
    "section": "Example",
    "text": "Example\nLet \\(Y \\sim N(\\mu_Y = 5, \\sigma_Y^2 = 4)\\), standardize the following and find the probability:\n\n\n\\(P(Y < 3)\\)\n\\[\\begin{aligned}\nP(Y < 3) &= P(\\frac{Y - \\mu_Y}{\\sigma_Y} < \\frac{3-\\mu_Y}{\\sigma_Y}) \\\\\n& = P(Z < \\frac{3-5}{2} = -1)\n\\end{aligned}\\]\npnorm(-1) \\(0.1586553\\)\npnorm(-1, mean = 5, sd = 2)\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_11.html#queen-bee",
    "href": "slides/lect_11.html#queen-bee",
    "title": "Chapter 12: The normal probability model",
    "section": "Queen Bee",
    "text": "Queen Bee\n\nTo the left, to the left, pnorm is to the left\n\nPercentiles are always to the left."
  },
  {
    "objectID": "slides/lect_11.html#example-1",
    "href": "slides/lect_11.html#example-1",
    "title": "Chapter 12: The normal probability model",
    "section": "Example",
    "text": "Example\n\nBoth of these probabilities are the same because of symmetry of the normal distribution. How does this generalize to all normal distributions?\n\nLet \\(Y \\sim N(\\mu_Y = 5, \\sigma_Y^2 = 4)\\), standardize the following and find the probability:\n\n\n\\(P(Y > 7)\\)\n\\[\\begin{aligned}\nP(Y > 7) &= P(\\frac{Y - \\mu_Y}{\\sigma_Y} > \\frac{7-\\mu_Y}{\\sigma_Y}) \\\\\n& = P(Z > \\frac{7-5}{2} = 1)\n\\end{aligned}\\]\npnorm(1) \\(0.8413447\\)\npnorm(1, mean = 5, sd = 2)\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_11.html#graph",
    "href": "slides/lect_11.html#graph",
    "title": "Chapter 12: The normal probability model",
    "section": "Graph",
    "text": "Graph"
  },
  {
    "objectID": "slides/lect_11.html#example-contd",
    "href": "slides/lect_11.html#example-contd",
    "title": "Chapter 12: The normal probability model",
    "section": "Example (cont’d)",
    "text": "Example (cont’d)\n\nThis and the previous example have the same probabilities attached to them b/c the distribution is symmetric\nSince they generally have the same shape what other things are true?\n\n\n\n\\(P(Z > 1) = 1 - P(Z<1)\\)\n1 - pnorm(1) \\(0.1586553\\)\n1 - pnorm(1, mean = 5, sd = 2)\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_11.html#rule",
    "href": "slides/lect_11.html#rule",
    "title": "Chapter 12: The normal probability model",
    "section": "Rule",
    "text": "Rule\n\nThis is true of all normal distributions, point of inflection, other things.\nWe will also need to learn to undo things. If we have a probability go backwards and find the z-score."
  },
  {
    "objectID": "slides/lect_11.html#undo",
    "href": "slides/lect_11.html#undo",
    "title": "Chapter 12: The normal probability model",
    "section": "Undo",
    "text": "Undo"
  },
  {
    "objectID": "slides/lect_11.html#plot---normal",
    "href": "slides/lect_11.html#plot---normal",
    "title": "Chapter 12: The normal probability model",
    "section": "Plot - normal",
    "text": "Plot - normal"
  },
  {
    "objectID": "slides/lect_11.html#plot---tale-of-2-tails",
    "href": "slides/lect_11.html#plot---tale-of-2-tails",
    "title": "Chapter 12: The normal probability model",
    "section": "Plot - tale of 2 tails",
    "text": "Plot - tale of 2 tails"
  },
  {
    "objectID": "slides/lect_11.html#tale-of-2-tails",
    "href": "slides/lect_11.html#tale-of-2-tails",
    "title": "Chapter 12: The normal probability model",
    "section": "Tale of 2 tails",
    "text": "Tale of 2 tails"
  },
  {
    "objectID": "slides/lect_11.html#fat-tails",
    "href": "slides/lect_11.html#fat-tails",
    "title": "Chapter 12: The normal probability model",
    "section": "Fat tails",
    "text": "Fat tails"
  },
  {
    "objectID": "slides/lect_11.html#thin-tails",
    "href": "slides/lect_11.html#thin-tails",
    "title": "Chapter 12: The normal probability model",
    "section": "Thin tails",
    "text": "Thin tails"
  },
  {
    "objectID": "slides/lect_11.html#bimodal",
    "href": "slides/lect_11.html#bimodal",
    "title": "Chapter 12: The normal probability model",
    "section": "Bimodal",
    "text": "Bimodal"
  },
  {
    "objectID": "slides/lect_11.html#qqplot-in-r",
    "href": "slides/lect_11.html#qqplot-in-r",
    "title": "Chapter 12: The normal probability model",
    "section": "QQPlot in R",
    "text": "QQPlot in R\n\nggplot(diamonds, aes(sample=price)) +\n  stat_qq() + # add the dots\n  stat_qq_line() # and the line"
  },
  {
    "objectID": "slides/lect_11.html#skewness",
    "href": "slides/lect_11.html#skewness",
    "title": "Chapter 12: The normal probability model",
    "section": "Skewness",
    "text": "Skewness\n\n\nfind the \\(z\\) scores for all data (\\(z_i = \\frac{x_i - \\bar{x}}{s}\\))\n\\[K_3 = \\frac{z_1^3 +z_2^3 + ... + z_n^3}{n}\\]\nIf \\(K_3 \\approx 0\\), then \\(x\\) is symmetric\nAs \\(K_3\\) gets larger than 0, more right-skewed\nAs \\(K_3\\) gets smaller than 0, more left-skewed"
  },
  {
    "objectID": "slides/lect_11.html#kurtosis",
    "href": "slides/lect_11.html#kurtosis",
    "title": "Chapter 12: The normal probability model",
    "section": "Kurtosis",
    "text": "Kurtosis\n\n\nfind the \\(z\\) scores for all data (\\(z_i = \\frac{x_i - \\bar{x}}{s}\\))\n\\[K_4 = \\frac{z_1^4 +z_2^4 + ... + z_n^4}{n} - 3\\]\nIf \\(K_4 \\approx 0\\), then \\(x\\) is approximately normal\nAs \\(K_4 < 0\\) flat uniform distribution without tails\nAs \\(K_4 > 0\\) many outliers"
  },
  {
    "objectID": "slides/lect_11.html#take-home",
    "href": "slides/lect_11.html#take-home",
    "title": "Chapter 12: The normal probability model",
    "section": "Take home",
    "text": "Take home\nIf you see departures from normality (large or small kurtosis, QQ plots that deviate from a straight line) PLOT the data and check.\n\n\nstat1010-f22.github.io/website"
  },
  {
    "objectID": "slides/lect_09.html#compute-the-sharpe-ratio",
    "href": "slides/lect_09.html#compute-the-sharpe-ratio",
    "title": "Chapter 10: Association between random variables",
    "section": "Compute the Sharpe ratio",
    "text": "Compute the Sharpe ratio\n\nhave them go back to the previous slide and see what they got\n\n\n\n\nCompany\nRandom Variable\nMean per month\nSD\n\n\n\n\nApple\nA\n2.45%\n13.3%\n\n\nMcDonalds\nM\n1.14%\n6.2%\n\n\n\nassume \\(r_f\\) is the risk-free rate of interest is \\(0.1\\%\\)"
  },
  {
    "objectID": "slides/lect_09.html#compute-the-sharpe-ratio-1",
    "href": "slides/lect_09.html#compute-the-sharpe-ratio-1",
    "title": "Chapter 10: Association between random variables",
    "section": "Compute the Sharpe ratio",
    "text": "Compute the Sharpe ratio\n\nwork through it, then what if we don’t know mu and sigma could we find it if we had a pdf?\n\n\n\n\n\n\\[\\begin{aligned}\nS(A) &= \\frac{\\mu_A - r_f}{\\sigma_A}\\\\\n&= \\frac{2.45 - 0.1}{13.3}\\\\\n&=  0.177\n\\end{aligned}\\]\n\\[\\begin{aligned}\nS(M) &= \\frac{\\mu_M - r_f}{\\sigma_M}\\\\\n&= \\frac{1.14 - 0.1}{6.2}\\\\\n&=  0.168\n\\end{aligned}\\]\n\n\n\nWe prefer Apple because it has a higher Sharpe ratio \\(0.177 > 0.168\\)"
  },
  {
    "objectID": "slides/lect_09.html#revision-2",
    "href": "slides/lect_09.html#revision-2",
    "title": "Chapter 10: Association between random variables",
    "section": "Revision 2",
    "text": "Revision 2\n\nfind the mean, stdev, and use them to find the Sharpe ratio\nput microsoft is y on the board and IBM is x\ncopy down the means and st deviation when they get them\n\nInstead of knowing \\(\\mu\\) and \\(\\sigma\\) we have a pdf\n\n\n\n\nIBM stock\nIBM stock\nMicrosoft\nMicrosoft\n\n\n\n\n\n\\(x\\)\n\\(P(X = x)\\)\n\\(y\\)\n\\(P(Y = y)\\)\n\n\nIncreases\n$5\n0.11\n$4\n0.18\n\n\nNo change\n0\n0.80\n0\n0.67\n\n\nDecreases\n-$5\n0.09\n-$4\n0.15"
  },
  {
    "objectID": "slides/lect_09.html#sharpe-ratio-by-hand",
    "href": "slides/lect_09.html#sharpe-ratio-by-hand",
    "title": "Chapter 10: Association between random variables",
    "section": "Sharpe ratio by hand",
    "text": "Sharpe ratio by hand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIBM\nIBM\nIBM\nIBM\nIBM\nIBM\nIBM\n\n\n\n\n\n\\(x\\)\n\\(P(X = x)\\)\n\\(\\mu_X\\)\n\\((x-\\mu_X)^2 \\cdot p_X(x)\\)\n\\(Var(X)\\)\n\\(sd(X)\\)\nSharpe ratio\n\n\nIncreases\n$5\n0.11\n\n\n\n\n\n\n\nNo change\n0\n0.80\n\n\n\n\n\n\n\nDecreases\n-$5\n0.09"
  },
  {
    "objectID": "slides/lect_09.html#sharpe-ratio-by-hand-1",
    "href": "slides/lect_09.html#sharpe-ratio-by-hand-1",
    "title": "Chapter 10: Association between random variables",
    "section": "Sharpe ratio by hand",
    "text": "Sharpe ratio by hand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIBM\nIBM\nIBM\nIBM\nIBM\nIBM\nIBM\n\n\n\n\n\n\\(x\\)\n\\(P(X = x)\\)\n\\(\\mu_X\\)\n\\((x-\\mu_X)^2 \\cdot p_X(x)\\)\n\\(Var(X)\\)\n\\(sd(X)\\)\nSharpe ratio\n\n\nIncreases\n$5\n0.11\n0.1\n2.6411\n4.99\n2.23\n0.03805123\n\n\nNo change\n0\n0.80\n0.1\n0.0080\n4.99\n2.23\n0.03805123\n\n\nDecreases\n-$5\n0.09\n0.1\n2.3409\n4.99\n2.23\n0.03805123"
  },
  {
    "objectID": "slides/lect_09.html#sharpe-ratio-by-hand-2",
    "href": "slides/lect_09.html#sharpe-ratio-by-hand-2",
    "title": "Chapter 10: Association between random variables",
    "section": "Sharpe ratio by hand",
    "text": "Sharpe ratio by hand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMCSFT\nMCSFT\nMCSFT\nMCSFT\nMCSFT\nMCSFT\nMCSFT\n\n\n\n\n\n\\(y\\)\n\\(P(Y =y)\\)\n\\(\\mu_Y\\)\n\\((y-\\mu_Y)^2 \\cdot p_Y(y)\\)\n\\(Var(Y)\\)\n\\(sd(Y)\\)\nSharpe ratio\n\n\nIncreases\n$4\n0.18\n\n\n\n\n\n\n\nNo change\n0\n0.67\n\n\n\n\n\n\n\nDecreases\n-$4\n0.15"
  },
  {
    "objectID": "slides/lect_09.html#sharpe-ratio-by-hand-3",
    "href": "slides/lect_09.html#sharpe-ratio-by-hand-3",
    "title": "Chapter 10: Association between random variables",
    "section": "Sharpe ratio by hand",
    "text": "Sharpe ratio by hand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMCSFT\nMCSFT\nMCSFT\nMCSFT\nMCSFT\nMCSFT\nMCSFT\n\n\n\n\n\n\\(y\\)\n\\(P(Y =y)\\)\n\\(\\mu_Y\\)\n\\((y-\\mu_Y)^2 \\cdot p_Y(y)\\)\n\\(Var(Y)\\)\n\\(sd(Y)\\)\nSharpe ratio\n\n\nIncreases\n$4\n0.18\n0.12\n2.709792\n5.2656\n2.29469\n0.04575782\n\n\nNo change\n0\n0.67\n0.12\n0.009648\n5.2656\n2.29469\n0.04575782\n\n\nDecreases\n-$4\n0.15\n0.12\n2.546160\n5.2656\n2.29469\n0.04575782"
  },
  {
    "objectID": "slides/lect_09.html#the-sharpe-ratio---in-r",
    "href": "slides/lect_09.html#the-sharpe-ratio---in-r",
    "title": "Chapter 10: Association between random variables",
    "section": "The Sharpe ratio - in R",
    "text": "The Sharpe ratio - in R\n\nCan an investor do better by diversifing the investment?\n\n\n# Input the data and the pdf\nstock <- tibble(x = c(5, 0, -5), \n                p_x = c(0.11, 0.8, 0.09), \n                y = c(4, 0, -4), \n                p_y = c(.18, .67, .15))\n\n\nSharpe_ratio <- # name this the Sharpe ratio\n  stock %>% # use the data from above\n  mutate(part_mean_x = x*p_x, # find mean for each part of x \n         part_mean_y = y*p_y, # now for y\n         mean_x = sum(part_mean_x), # find mean x \n         mean_y = sum(part_mean_y),# now for y\n         part_var_x = p_x * (mean_x - x)^2, # find var for each part of x \n         part_var_y = p_y * (mean_y - y)^2,# now for y\n         var_x = sum(part_var_x), # find var of x \n         var_y = sum(part_var_y),# now for y\n         sd_x = sqrt(var_x), # find sd of x \n         sd_y = sqrt(var_y)) %>% # now for y\n  summarise(S_x = (mean_x - 0.015)/sd_x, # find Sharpe ratio of x with rf = 0.015\n            S_y = (mean_y - 0.015)/sd_y) %>% # now for y\n  slice(1)"
  },
  {
    "objectID": "slides/lect_09.html#expected-value-of-x-y",
    "href": "slides/lect_09.html#expected-value-of-x-y",
    "title": "Chapter 10: Association between random variables",
    "section": "Expected value of \\(X + Y\\)",
    "text": "Expected value of \\(X + Y\\)\n\nIf x=-5 then we subtract 5 from all y’s and multiply by \\(p_{x, y}\\) algebraically it all washes out\nWhat about independence? We want to find the standard deviation to compute the Sharpe Ratio\n\n\n\n\n\n\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\n\n\n\n\n\n\\(x = -5\\)\n\\(x = 0\\)\n\\(x = 5\\)\n\\(p(y)\\)\n\n\n\\(Y\\)\n\\(y=4\\)\n\\(0.00\\)\n\\(0.11\\)\n\\(0.07\\)\n\\(0.18\\)\n\n\n\\(Y\\)\n\\(y=0\\)\n\\(0.03\\)\n\\(0.62\\)\n\\(0.02\\)\n\\(0.67\\)\n\n\n\\(Y\\)\n\\(y=-4\\)\n\\(0.06\\)\n\\(0.07\\)\n\\(0.02\\)\n\\(0.15\\)\n\n\n\n\\(p(x)\\)\n\\(0.09\\)\n\\(0.80\\)\n\\(0.11\\)\n\\(1\\)\n\n\n\n\n\n\\(E(X+Y) = E(X) + E(Y)\\)\n\\(E(X+Y) = E(X) + E(Y) = 0.1 + 0.12 = 0.22\\)"
  },
  {
    "objectID": "slides/lect_09.html#are-x-and-y-independent",
    "href": "slides/lect_09.html#are-x-and-y-independent",
    "title": "Chapter 10: Association between random variables",
    "section": "Are \\(X\\) and \\(Y\\) independent?",
    "text": "Are \\(X\\) and \\(Y\\) independent?\n\nfind any p(x) X p(y) != p(x,y)\nBut we still need to find the stdev in order to find Sharpes ratio\n\n\n\n\n\n\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\n\n\n\n\n\n\\(x = -5\\)\n\\(x = 0\\)\n\\(x = 5\\)\n\\(p(y)\\)\n\n\n\\(Y\\)\n\\(y=4\\)\n\\(0.00\\)\n\\(0.11\\)\n\\(0.07\\)\n\\(0.18\\)\n\n\n\\(Y\\)\n\\(y=0\\)\n\\(0.03\\)\n\\(0.62\\)\n\\(0.02\\)\n\\(0.67\\)\n\n\n\\(Y\\)\n\\(y=-4\\)\n\\(0.06\\)\n\\(0.07\\)\n\\(0.02\\)\n\\(0.15\\)\n\n\n\n\\(p(x)\\)\n\\(0.09\\)\n\\(0.80\\)\n\\(0.11\\)\n\\(1\\)\n\n\n\n\n\n\\(p_x(-5) \\cdot p_y(4) = 0.18 \\cdot 0.09 \\neq 0.00\\)\n\\(p_x(-5) \\cdot p_y(-4) = 0.09 \\cdot 0.15 = 0.0135 \\neq 0.06\\)\nNO! NO! NO!"
  },
  {
    "objectID": "slides/lect_09.html#covariance-of-sum",
    "href": "slides/lect_09.html#covariance-of-sum",
    "title": "Chapter 10: Association between random variables",
    "section": "Covariance of sum",
    "text": "Covariance of sum\n\nbecause we square things and \\((a + b)^2 = a^2 + b^2 + 2ab\\) we have to add 2 of the covariance\n\n\n\n\\((a+b)^2 = a^2 + 2ab +b^2\\)\nit follows that\n\\(Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)\\)"
  },
  {
    "objectID": "slides/lect_09.html#correlation",
    "href": "slides/lect_09.html#correlation",
    "title": "Chapter 10: Association between random variables",
    "section": "Correlation",
    "text": "Correlation\n\nwhat about our joint pdf? is it positively or negatively correlated? 3D plot\n\n\n\n\\(corr(x, y) = \\frac{cov(x, y)}{s_x \\cdot s_y}\\)\n\\[\\rho = Corr(X, Y) = \\frac{Cov(X, Y)}{\\sigma_X \\cdot \\sigma_Y}\\]\n\nAs \\(X\\) increases, what happens to \\(Y\\)?\n\n\n\n\n\n\n\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\n\n\n\n\n\n\\(x = -5\\)\n\\(x = 0\\)\n\\(x = 5\\)\n\\(p(y)\\)\n\n\n\\(Y\\)\n\\(y=4\\)\n\\(0.00\\)\n\\(0.11\\)\n\\(0.07\\)\n\\(0.18\\)\n\n\n\\(Y\\)\n\\(y=0\\)\n\\(0.03\\)\n\\(0.62\\)\n\\(0.02\\)\n\\(0.67\\)\n\n\n\\(Y\\)\n\\(y=-4\\)\n\\(0.06\\)\n\\(0.07\\)\n\\(0.02\\)\n\\(0.15\\)\n\n\n\n\\(p(x)\\)\n\\(0.09\\)\n\\(0.80\\)\n\\(0.11\\)\n\\(1\\)"
  },
  {
    "objectID": "slides/lect_09.html#independence-cov",
    "href": "slides/lect_09.html#independence-cov",
    "title": "Chapter 10: Association between random variables",
    "section": "Independence & Cov",
    "text": "Independence & Cov\nHow does independence impact upon correlation and covariance?\n\\[ E((X-\\mu_X)(Y-\\mu_Y))\\]\n\n\n\\[\\begin{aligned}\nCov(X,Y) &= E((X-\\mu_X)(Y-\\mu_Y))\\\\\n&= E(X - \\mu_X)E(Y-\\mu_Y)\\\\\n&= 0\n\\end{aligned}\\]\nIf \\(X\\),\\(Y\\) are independent, then \\(Cov(X, Y) = 0\\)\nThe opposite is not true (if \\(Cov(X, Y) = 0\\), then \\(X\\),\\(Y\\) are independent)"
  },
  {
    "objectID": "slides/lect_09.html#independence-var",
    "href": "slides/lect_09.html#independence-var",
    "title": "Chapter 10: Association between random variables",
    "section": "Independence & Var",
    "text": "Independence & Var\n\\(Var(X) + Var(Y) = Var(X) + Var(Y) + 2Cov(X, Y)\\)\n\n\nIf \\(X\\), \\(Y\\) are independent, \\(Cov(X, Y) = 0\\), so\n\\[Var(X+ Y) = Var(X) + Var(Y)\\]"
  },
  {
    "objectID": "slides/lect_09.html#sharpe-ratio-of-a-sum",
    "href": "slides/lect_09.html#sharpe-ratio-of-a-sum",
    "title": "Chapter 10: Association between random variables",
    "section": "Sharpe ratio of a sum",
    "text": "Sharpe ratio of a sum\n\\[ S(X + Y) = \\frac{(\\mu_X + \\mu_Y) - 2r_f}{\\sqrt{(Var(X+Y))}} \\]"
  },
  {
    "objectID": "slides/lect_09.html#sharpe-ratio-of-sum---r",
    "href": "slides/lect_09.html#sharpe-ratio-of-sum---r",
    "title": "Chapter 10: Association between random variables",
    "section": "Sharpe ratio of sum - R",
    "text": "Sharpe ratio of sum - R\n\nWhat if we want to invest in two subsequent days instead of investing in two stocks for 1 day?\n\n\n# input x & y\nx <- c(-5, 0, 5)\ny <- c(4, 0, -4)\n\n# input data\ndata <- bind_cols(expand_grid(x, y), \n                  probs = c(0, .03, .06, \n                            .11, .62, .07, \n                            .07, .02, .02), \n                  mu_x = rep(0.1, 9), \n                  mu_y = rep(0.12, 9), \n                  var_x = rep(4.99, 9), \n                  var_y = rep(5.2656, 9))\n\ndata %>% \n  mutate(cov_xy_part = (x - mu_x)*\n           (y - mu_y)*probs, # multiply together\n         cov_xy = sum(cov_xy_part), # sum them\n         var_xy = var_x + var_y + 2*cov_xy) %>% # using the rule\n  summarise(S_r = (mu_x + mu_y - 2*0.015)/\n                      (sqrt(var_xy))) %>% # find the Sharpe value\n  slice(1) # only the first\n\n# A tibble: 1 × 1\n     S_r\n   <dbl>\n1 0.0497\n\n\n\n\nThe Sharpe ratio for \\(X\\) is \\(0.038\\), for \\(Y\\) it’s \\(0.046\\), investing in both gives a better return \\(0.050\\)"
  },
  {
    "objectID": "slides/lect_09.html#double-for-one-day",
    "href": "slides/lect_09.html#double-for-one-day",
    "title": "Chapter 10: Association between random variables",
    "section": "Double for one day",
    "text": "Double for one day\n\\[S(2X) = \\frac{2\\mu_X - 2r_f}{\\sqrt{Var(2X)}}\\]\n\n\n\\(\\mu_X = 0.1\\)\n\\(Var(X) = 4.99\\)\n\\(r_f = .015\\)\n\\[\\begin{aligned}\n  &= \\frac{2 \\cdot 0.1 - 2 \\cdot 0.015}{\\sqrt{4 \\cdot 4.99}}\\\\\n  &= \\frac{.2 - 0.03}{\\sqrt{19.96}}\\\\\n  &= 0.038\n  \\end{aligned}\\]\nThis is the same as before, how do we compute if we invest for two subsequent days?"
  },
  {
    "objectID": "slides/lect_09.html#iid",
    "href": "slides/lect_09.html#iid",
    "title": "Chapter 10: Association between random variables",
    "section": "IID",
    "text": "IID\nIf we invested in a stock on 2 subsequent days instead of investing in 2 stocks on one day the return on those two days are:\nindependent and identically distributed (IID)\n\n\nidentically distributed - the outcomes on each day are likely to be different, but the probability of the outcomes is the same\nindependent - very common assumption for stocks (part of the reason for the 2008 financial crises)"
  },
  {
    "objectID": "slides/lect_09.html#addition-rules-for-iid-variables",
    "href": "slides/lect_09.html#addition-rules-for-iid-variables",
    "title": "Chapter 10: Association between random variables",
    "section": "Addition rules for IID variables",
    "text": "Addition rules for IID variables\n\n\n\n\n\n\nImportant\n\n\nIf \\(n\\) random variables \\((X_1, X_2, ..., X_n)\\) are iid with mean \\(\\mu_X\\) and standard deviation \\(\\sigma_X\\), then\n\\[ E(X_1 + X_2 + ... + X_n) = n \\cdot \\mu_X \\] \\[ Var(X_1 + X_2 + ... + X_n) = n \\cdot \\sigma^2_x \\] \\[ SD(X_1 + X_2 + ... + X_n) = \\sqrt{n} \\sigma_X \\]"
  },
  {
    "objectID": "slides/lect_09.html#sharpe-ratio---two-days",
    "href": "slides/lect_09.html#sharpe-ratio---two-days",
    "title": "Chapter 10: Association between random variables",
    "section": "Sharpe ratio - two days",
    "text": "Sharpe ratio - two days\n\nif you decide to leave money in the bank, you may end up with a constant\n\n\\[ S(X_1 + X_2) = \\frac{2\\mu_X - 2r_f}{\\sqrt{2 \\sigma^2_x}}\\]\n\n\n\\[\\begin{aligned}\n&= \\frac{0.20 - 0.03}{\\sqrt{9.98}}\\\\\n&= 0.054\n\\end{aligned}\\]\nyou expect more variance if you have two stock for one day b/c anything that happens that day is magnified\nif you have one stock for two days you reduce the variance"
  },
  {
    "objectID": "slides/lect_09.html#expectation-of-weighted-sum",
    "href": "slides/lect_09.html#expectation-of-weighted-sum",
    "title": "Chapter 10: Association between random variables",
    "section": "Expectation of weighted sum",
    "text": "Expectation of weighted sum\n\\[ E(2X + 4Y + 0.06)\\]\n\n\n\n\\[\\begin{aligned}\n&= 2E(X) + 4E(Y) + 0.06\\\\\n&= 2(0.10) + 4(0.12) + 0.06\\\\\n&= \\$0.74\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lect_09.html#variance-of-weighted-sum",
    "href": "slides/lect_09.html#variance-of-weighted-sum",
    "title": "Chapter 10: Association between random variables",
    "section": "Variance of weighted sum",
    "text": "Variance of weighted sum\n\\[ Var(2X + 4Y + 0.06)\\]\n\n\n\\[\\begin{aligned}\n&= 2^2Var(X) + 4^2Var(Y) + 2 \\cdot(2 \\cdot 4) \\cdot Cov(X,Y)\\\\\n&= 4(4.99) + 16(5.27) + 16(2.19)\\\\\n&= 139.32\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lect_10.html#ebernoulli",
    "href": "slides/lect_10.html#ebernoulli",
    "title": "Chapter 11: Probability models for counts",
    "section": "E(Bernoulli)",
    "text": "E(Bernoulli)\n\n\n\\(E(B) = 0 \\cdot P(B=0) + 1 \\cdot P(B=1)\\)\n\\[\\begin{aligned}\nE(B) &= 0 \\cdot P(B=0) + 1 \\cdot P(B=1)\\\\\n&= 0 \\cdot (1-p) + 1 \\cdot p\\\\\n&= p\n\\end{aligned}\\]\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_10.html#varbernoulli",
    "href": "slides/lect_10.html#varbernoulli",
    "title": "Chapter 11: Probability models for counts",
    "section": "Var(Bernoulli)",
    "text": "Var(Bernoulli)\n\nWe usually don’t want this information about 1 person, we want it about a whole plane full of people, or a whole town.\nIn that case, we sum together \\(n\\) independent Bernoulli trials.\n\n\n\n\\(Var(B) = (0 - p)^2 \\cdot P(B=0) + (1 - p)^2 \\cdot P(B=1)\\)\n\\[\\begin{aligned}\nVar(B) &= (0 - p)^2 \\cdot P(B=0) + (1 - p)^2 \\cdot P(B=1)\\\\\n&= p^2 \\cdot (1-p) + (1 - p)^2 \\cdot p\\\\\n&= p^2 - p^3 + p(1 - 2p + p^2) \\\\\n&= p^2 - p^3 + p - 2p^2 + p^3\\\\\n&= p(1-p)\n\\end{aligned}\\]\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_10.html#multiple-bernoulli-trials",
    "href": "slides/lect_10.html#multiple-bernoulli-trials",
    "title": "Chapter 11: Probability models for counts",
    "section": "Multiple Bernoulli trials",
    "text": "Multiple Bernoulli trials\n\nThis happens so often that we have another name for this distribution\n\n\\[Y = B_1 + B_2 + ...  + B_n\\]\n\n\n\\(Y\\) is the sum of independent and identically distributed random variables\nWhat is the mean and variance?"
  },
  {
    "objectID": "slides/lect_10.html#ebinomial",
    "href": "slides/lect_10.html#ebinomial",
    "title": "Chapter 11: Probability models for counts",
    "section": "E(Binomial)",
    "text": "E(Binomial)\n\\[E(Y) = E(B_1 + B_2 + ... + B_n)\\]\n\n\n\\[\\begin{aligned}\nE(Y) &= E(B_1) + E(B_2) + ... + E(B_n) \\\\\n&= p + p + ... + p \\\\\n&= np\n\\end{aligned}\\]\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_10.html#varbinomial",
    "href": "slides/lect_10.html#varbinomial",
    "title": "Chapter 11: Probability models for counts",
    "section": "Var(Binomial)",
    "text": "Var(Binomial)\n\nthere are multiple ways that we can have 5 successes, so we must also learn to count\n\n\\[Var(Y) = Var(B_1 + B_2 + ... + B_n)\\]\n\n\n\\[\\begin{aligned}\nVar(Y) &= Var(B_1) + Var(B_2) + ... + Var(B_n) \\\\\n&= p(1-p) + p(1-p) + ... + p(1-p) \\\\\n&= np(1-p)\n\\end{aligned}\\]\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_10.html#counting-things",
    "href": "slides/lect_10.html#counting-things",
    "title": "Chapter 11: Probability models for counts",
    "section": "Counting things",
    "text": "Counting things\n\nwe just went into groups how many groups are possible?\n\nIn a class of 85 students how many groups of 4 are possible?\n\n\n\\(85 \\cdot 84 \\cdot 83 \\cdot 82\\)\n\\(48,594,840\\) groups\nIf we don’t care about order\n\\(\\frac{85 \\cdot 84 \\cdot 83 \\cdot 82}{4 \\cdot 3 \\cdot 2 \\cdot 1}\\)\n\\(2,024,785\\) groups\nCombination written as \\({}_{n}C_{k}\\) here \\(n = 85\\), \\(k = 4\\) or \\(\\binom{85}{4}\\)\n\n\n\nexp(lfactorial(85))/exp(lfactorial(81))\n\n[1] 48594840\n\nexp(lfactorial(85))/(exp(lfactorial(81))*exp(lfactorial(4)))\n\n[1] 2024785"
  },
  {
    "objectID": "slides/lect_10.html#binomial-pdf",
    "href": "slides/lect_10.html#binomial-pdf",
    "title": "Chapter 11: Probability models for counts",
    "section": "Binomial pdf",
    "text": "Binomial pdf\nIf \\(Y \\sim Bin(n, p)\\) where\n\n$n = $ number of trials\n$p = $ probability of success\n$y = $ number of successes in \\(n\\) Bernoulli trials\n\n\nthen  \\[P(Y = y) = \\binom{n}{y}p^y(1-p)^{n-y}\\]"
  },
  {
    "objectID": "slides/lect_10.html#example",
    "href": "slides/lect_10.html#example",
    "title": "Chapter 11: Probability models for counts",
    "section": "Example",
    "text": "Example\n\n\nLet \\(Y \\sim Bin(n = 5, p = 0.2)\\) find the \\(E(Y)\\) and \\(Var(Y)\\)\n\n\n\n\\(E(Y) = np = 1\\)\n\\(Var(Y) = np(1-p) = 0.8\\)\nWhat is the probability that \\(y = 3\\)? In R: dbinom(size  = 5, prob = 0.2, x = 3) \\(0.0512\\)\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_10.html#warnings",
    "href": "slides/lect_10.html#warnings",
    "title": "Chapter 11: Probability models for counts",
    "section": "Warnings",
    "text": "Warnings\n10% Condition: if trials are selected at random, it is OK to ignore dependence caused by sampling from a finite population if the selected trials make up less than 10% of the population"
  },
  {
    "objectID": "slides/lect_10.html#limit-of-p--mins",
    "href": "slides/lect_10.html#limit-of-p--mins",
    "title": "Chapter 11: Probability models for counts",
    "section": "Limit of \\(p\\)- mins",
    "text": "Limit of \\(p\\)- mins\n\nsome of these cars may come by in the same minute..\n\nI know about 7 cars drive by my house in an hour, what’s the probability 18 cars drive by in the next 60 mins?\n\n\n\\(p = \\frac{7}{60}\\)\n\\(Y \\sim Bin(60, \\frac{7}{60})\\)\n\\(\\binom{60}{18}p^{18}(1-p)^{42}\\)"
  },
  {
    "objectID": "slides/lect_10.html#limit-of-p--secs",
    "href": "slides/lect_10.html#limit-of-p--secs",
    "title": "Chapter 11: Probability models for counts",
    "section": "Limit of \\(p\\)- secs",
    "text": "Limit of \\(p\\)- secs\nI know about 7 cars drive by my house in an hour, what’s the probability 18 cars drive by in the next 3600 secs?\n\n\n\\(p = \\frac{7}{3600}\\)\n\\(Y \\sim Bin(3600, \\frac{7}{3600})\\)\n\\(\\binom{3600}{18}p^{18}(1-p)^{3582}\\)"
  },
  {
    "objectID": "slides/lect_10.html#limit-of-p---smallest-interval",
    "href": "slides/lect_10.html#limit-of-p---smallest-interval",
    "title": "Chapter 11: Probability models for counts",
    "section": "Limit of \\(p\\) - smallest interval",
    "text": "Limit of \\(p\\) - smallest interval\nI know about 7 cars drive by my house in an hour, what’s the probability 18 cars drive by in the next much smaller than a second? Full derivation can be found here.\n\n\n\n\n\\(\\binom{n}{18}(\\frac{7}{n})^{18}(1-\\frac{7}{n})^{n-18}\\)\n\\(\\frac{n\\cdot(n-1)\\ldots(n-17)}{18!}\\cdot \\frac{7^{18}}{n^{18}}\\cdot (1-\\frac{7}{n})^{n} \\cdot \\frac{1}{(1-\\frac{7}{n})^{18}}\\)\n\\(\\frac{n\\cdot(n-1)\\ldots(n-17)}{n^{18}} \\rightarrow 1\\)\n\\(\\frac{1}{(1-\\frac{7}{n})^{18}} \\rightarrow 1\\)\n\n\n\n\n\\(\\frac{7^{18}}{18!}e^{-7}\\)"
  },
  {
    "objectID": "slides/lect_10.html#poisson-distribution---rips",
    "href": "slides/lect_10.html#poisson-distribution---rips",
    "title": "Chapter 11: Probability models for counts",
    "section": "Poisson distribution - RIPS",
    "text": "Poisson distribution - RIPS\n\n\nPoisson - fish - Rips\nR - randomly through space or time\nI - indepedent\nP - proportional to interval size\nS - singly - no multiple occurences in space or time"
  },
  {
    "objectID": "slides/lect_10.html#epois",
    "href": "slides/lect_10.html#epois",
    "title": "Chapter 11: Probability models for counts",
    "section": "E(Pois)",
    "text": "E(Pois)\n\n\\(\\lambda\\)"
  },
  {
    "objectID": "slides/lect_10.html#varpois",
    "href": "slides/lect_10.html#varpois",
    "title": "Chapter 11: Probability models for counts",
    "section": "Var(Pois)",
    "text": "Var(Pois)\n\n\\(\\lambda\\)"
  },
  {
    "objectID": "slides/lect_10.html#example-1",
    "href": "slides/lect_10.html#example-1",
    "title": "Chapter 11: Probability models for counts",
    "section": "Example",
    "text": "Example\n\n\nLet \\(Y \\sim Pois(\\lambda = 5)\\) find the \\(E(Y)\\) and \\(Var(Y)\\)\n\n\n\\(E(Y) = 5\\)\n\\(Var(Y) = 5\\)\nWhat is the probability that \\(y = 3\\)? In R: dpois(x = 3, lambda = 5) \\(0.1403739\\)\n\n\n\n\n\n02:00\n\n\n\n\n\nstat1010-f22.github.io/website"
  },
  {
    "objectID": "slides/lect_07.html#using-proportions",
    "href": "slides/lect_07.html#using-proportions",
    "title": "Chapter 9: Random variables",
    "section": "Using proportions",
    "text": "Using proportions\n\n\n\n\\(x\\)\n\\(n\\)\n\\(P(X) = x\\)\n\n\n\n\n1.23\n3\n\n\n\n1.29\n5\n\n\n\n1.37\n4\n\n\n\n1.84\n1\n\n\n\n1.18\n6\n\n\n\n1.22\n2\n\n\n\n1.25\n4\n\n\n\nTotal\n25"
  },
  {
    "objectID": "slides/lect_07.html#mean-or-expected-value",
    "href": "slides/lect_07.html#mean-or-expected-value",
    "title": "Chapter 9: Random variables",
    "section": "Mean or expected value",
    "text": "Mean or expected value\nIf we know $P(X) = x$, then we can use this to find the mean or expected value of a random variable. The pdf includes information about both the total and the number of occurrences of \\(x\\), it does the computation for us.\n\\(\\mu = E(X)\\)\n\\(= x_1p(x_1) + x_2p(x_2) + … + x_np(x_n)\\)"
  },
  {
    "objectID": "slides/lect_07.html#price-increase-1",
    "href": "slides/lect_07.html#price-increase-1",
    "title": "Chapter 9: Random variables",
    "section": "Price increase",
    "text": "Price increase\n\n\n\n\\(x\\)\n\\(x + 5\\)\n\\(P(X) = x+5\\)\n\n\n\n\n1.23\n6.23\n0.12\n\n\n1.29\n6.29\n0.20\n\n\n1.37\n6.37\n0.16\n\n\n1.84\n6.84\n0.04\n\n\n1.18\n6.18\n0.24\n\n\n1.22\n6.22\n0.08\n\n\n1.25\n6.25\n0.16"
  },
  {
    "objectID": "slides/lect_07.html#price-increase-2",
    "href": "slides/lect_07.html#price-increase-2",
    "title": "Chapter 9: Random variables",
    "section": "Price increase",
    "text": "Price increase"
  },
  {
    "objectID": "slides/lect_07.html#stock-splits-1",
    "href": "slides/lect_07.html#stock-splits-1",
    "title": "Chapter 9: Random variables",
    "section": "Stock splits",
    "text": "Stock splits\n\n\n\n\\(x\\)\n\\(3x\\)\n\\(P(X) = x\\)\n\n\n\n\n1.23\n3.68\n0.12\n\n\n1.29\n3.87\n0.20\n\n\n1.37\n4.11\n0.16\n\n\n1.84\n5.52\n0.04\n\n\n1.18\n3.54\n0.24\n\n\n1.22\n3.66\n0.08\n\n\n1.25\n3.75\n0.16"
  },
  {
    "objectID": "slides/lect_07.html#stock-splits-2",
    "href": "slides/lect_07.html#stock-splits-2",
    "title": "Chapter 9: Random variables",
    "section": "Stock splits",
    "text": "Stock splits"
  },
  {
    "objectID": "slides/lec-1.html#meet-the-professor",
    "href": "slides/lec-1.html#meet-the-professor",
    "title": "Welcome to STA 210!",
    "section": "Meet the professor",
    "text": "Meet the professor\n\n\n\n\n\nDr. Mine Çetinkaya-Rundel (she/her)\n\n\n\n\nProfessor of the Practice & Director of Undergraduate Studies, Department of Statistical Science\nAffiliated Faculty, Computational Media, Arts & Cultures\nFind out more at mine-cr.com"
  },
  {
    "objectID": "slides/lec-1.html#meet-the-tas",
    "href": "slides/lec-1.html#meet-the-tas",
    "title": "Welcome to STA 210!",
    "section": "Meet the TAs",
    "text": "Meet the TAs\n\nMartha Aboagye (she/her, UG)\nRich Fremgen (he/him, MS)\nEmily Gentles (she/her, MS)\nSara Mehta (she/her, UG)\nRick Presman (he/him, PhD)\nShari Tian (she/her, UG)\nAaditya Warrier (he/him, UG)"
  },
  {
    "objectID": "slides/lec-1.html#check-out-conversations",
    "href": "slides/lec-1.html#check-out-conversations",
    "title": "Welcome to STA 210!",
    "section": "Check out Conversations",
    "text": "Check out Conversations\n\nGo to Conversations 💬\nAnswer the discussion question: How are you doing?"
  },
  {
    "objectID": "slides/lec-1.html#what-is-regression-analysis",
    "href": "slides/lec-1.html#what-is-regression-analysis",
    "title": "Welcome to STA 210!",
    "section": "What is regression analysis",
    "text": "What is regression analysis\n\n\n“In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or predictors). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or ‘criterion variable’) changes when any one of the independent variables is varied, while the other independent variables are held fixed.”\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lec-1.html#course-faq",
    "href": "slides/lec-1.html#course-faq",
    "title": "Welcome to STA 210!",
    "section": "Course FAQ",
    "text": "Course FAQ\n\n\nWhat background is assumed for the course? Introductory statistics or probability course.\nWill we be doing computing? Yes. We will use R.\nWill we learn the mathematical theory of regression? Yes and No. The course is primarily focused on application; however, we will discuss some of the mathematics of simple linear regression. The 1-credit course STA 211: Mathematics of Regression you can take simultaneously / after dives into more of the mathematics."
  },
  {
    "objectID": "slides/lec-1.html#course-learning-objectives",
    "href": "slides/lec-1.html#course-learning-objectives",
    "title": "Welcome to STA 210!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\n\nAnalyze real-world data to answer questions about multivariable relationships.\nFit and evaluate linear and logistic regression models.\nAssess whether a proposed model is appropriate and describe its limitations.\nUse Quarto to write reproducible reports and GitHub for version control and collaboration.\nCommunicate results from statistical analyses to a general audience."
  },
  {
    "objectID": "slides/lec-1.html#examples-of-regression-in-practice",
    "href": "slides/lec-1.html#examples-of-regression-in-practice",
    "title": "Welcome to STA 210!",
    "section": "Examples of regression in practice",
    "text": "Examples of regression in practice\n\n\nNew Yorkers Will Pay $56 A Month To Trim A Minute Off Their Commute\nHow FiveThirtyEight’s 2020 Presidential Forecast Works — And What’s Different Because Of COVID-19\nEffect of Forensic Evidence on Criminal Justice Case Processing\nWhy it’s so freaking hard to make a good COVID-19 model"
  },
  {
    "objectID": "slides/lec-1.html#homepage",
    "href": "slides/lec-1.html#homepage",
    "title": "Welcome to STA 210!",
    "section": "Homepage",
    "text": "Homepage\nsta210-s22.github.io/website\n\nAll course materials\nLinks to Sakai, GitHub, RStudio containers, etc.\nLet’s take a tour!"
  },
  {
    "objectID": "slides/lec-1.html#course-toolkit",
    "href": "slides/lec-1.html#course-toolkit",
    "title": "Welcome to STA 210!",
    "section": "Course toolkit",
    "text": "Course toolkit\nAll linked from the course website:\n\nGitHub organization: github.com/sta210-s22\nRStudio containers: cmgr.oit.duke.edu/containers\nDiscussion forum: Conversations\nAssignment submission and feedback: Gradescope\n\n\n\n\n\n\n\nImportant\n\n\nReserve an RStudio Container (titled STA 210) before lab on Monday!"
  },
  {
    "objectID": "slides/lec-1.html#activities-prepare-participate-practice-perform",
    "href": "slides/lec-1.html#activities-prepare-participate-practice-perform",
    "title": "Welcome to STA 210!",
    "section": "Activities: Prepare, Participate, Practice, Perform",
    "text": "Activities: Prepare, Participate, Practice, Perform\n\nPrepare: Introduce new content and prepare for lectures by completing the readings (and sometimes watching the videos)\nParticipate: Attend and actively participate in lectures and labs, office hours, team meetings\nPractice: Practice applying statistical concepts and computing with application exercises during lecture, graded for completion\nPerform: Put together what you’ve learned to analyze real-world data\n\nLab assignments x 7 (first individual, later team-based)\nHomework assignments x 5 (individual)\nThree take-home exams\nTerm project presented during the final exam period"
  },
  {
    "objectID": "slides/lec-1.html#cadence",
    "href": "slides/lec-1.html#cadence",
    "title": "Welcome to STA 210!",
    "section": "Cadence",
    "text": "Cadence\n\n\nLabs: Start and make large progress on Monday in lab section, finish up by Friday 5pm of that week\nHWs: Posted Friday morning, due following Friday 5pm\nExams: Exam review Thursday in class, exam posted Friday morning, no lab on Monday of following week, due Monday 11:59pm\nProject: Deadlines throughout the semester, with some lab and lecture time dedicated to working on them, and most work done in teams outside of class"
  },
  {
    "objectID": "slides/lec-1.html#teams",
    "href": "slides/lec-1.html#teams",
    "title": "Welcome to STA 210!",
    "section": "Teams",
    "text": "Teams\n\nTeam assignments\n\nAssigned by me\nApplication exercises, labs, and project\nPeer evaluation during teamwork and after completion\n\nExpectations and roles\n\nEveryone is expected to contribute equal effort\nEveryone is expected to understand all code turned in\nIndividual contribution evaluated by peer evaluation, commits, etc."
  },
  {
    "objectID": "slides/lec-1.html#grading",
    "href": "slides/lec-1.html#grading",
    "title": "Welcome to STA 210!",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nApplication exercises\n3%\n\n\nHomework\n35% (7% x 5)\n\n\nProject\n15%\n\n\nLab\n14% (2% x 7)\n\n\nExam 01\n10%\n\n\nExam 02\n10%\n\n\nExam 03\n10%\n\n\nTeamwork\n3%\n\n\n\nSee course syllabus for how the final letter grade will be determined."
  },
  {
    "objectID": "slides/lec-1.html#support",
    "href": "slides/lec-1.html#support",
    "title": "Welcome to STA 210!",
    "section": "Support",
    "text": "Support\n\nAttend office hours\nAsk and answer questions on the discussion forum\nReserve email for questions on personal matters and/or grades\nRead the course support page"
  },
  {
    "objectID": "slides/lec-1.html#announcements",
    "href": "slides/lec-1.html#announcements",
    "title": "Welcome to STA 210!",
    "section": "Announcements",
    "text": "Announcements\n\nPosted on Sakai (Announcements tool) and sent via email, be sure to check both regularly\nI’ll assume that you’ve read an announcement by the next “business” day\nI’ll (try my best to) send a weekly update announcement each Friday, outlining the plan for the following week and reminding you what you need to do to prepare, practice, and perform"
  },
  {
    "objectID": "slides/lec-1.html#diversity-inclusion",
    "href": "slides/lec-1.html#diversity-inclusion",
    "title": "Welcome to STA 210!",
    "section": "Diversity + inclusion",
    "text": "Diversity + inclusion\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength and benefit.\n\n\nIf you have a name that differs from those that appear in your official Duke records, please let me know!\nPlease let me know your preferred pronouns. You’ll also be able to note this in the Getting to know you survey.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. I want to be a resource for you. If you prefer to speak with someone outside of the course, your advisers and deans are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "slides/lec-1.html#accessibility",
    "href": "slides/lec-1.html#accessibility",
    "title": "Welcome to STA 210!",
    "section": "Accessibility",
    "text": "Accessibility\n\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments.\nI am committed to making all course materials accessible and I’m always learning how to do this better. If any course component is not accessible to you in any way, please don’t hesitate to let me know."
  },
  {
    "objectID": "slides/lec-1.html#covid-policies",
    "href": "slides/lec-1.html#covid-policies",
    "title": "Welcome to STA 210!",
    "section": "COVID policies",
    "text": "COVID policies\n\nWear a mask at all times!\nRead and follow university guidance"
  },
  {
    "objectID": "slides/lec-1.html#late-work-waivers-regrades-policy",
    "href": "slides/lec-1.html#late-work-waivers-regrades-policy",
    "title": "Welcome to STA 210!",
    "section": "Late work, waivers, regrades policy",
    "text": "Late work, waivers, regrades policy\n\nWe have policies!\nRead about them on the course syllabus and refer back to them when you need it"
  },
  {
    "objectID": "slides/lec-1.html#collaboration-policy",
    "href": "slides/lec-1.html#collaboration-policy",
    "title": "Welcome to STA 210!",
    "section": "Collaboration policy",
    "text": "Collaboration policy\n\nOnly work that is clearly assigned as team work should be completed collaboratively.\nHomeworks must be completed individually. You may not directly share answers / code with others, however you are welcome to discuss the problems in general and ask for advice.\nExams must be completed individually. You may not discuss any aspect of the exam with peers. If you have questions, post as private questions on the course forum, only the teaching team will see and answer."
  },
  {
    "objectID": "slides/lec-1.html#sharing-reusing-code-policy",
    "href": "slides/lec-1.html#sharing-reusing-code-policy",
    "title": "Welcome to STA 210!",
    "section": "Sharing / reusing code policy",
    "text": "Sharing / reusing code policy\n\nWe are aware that a huge volume of code is available on the web, and many tasks may have solutions posted\nUnless explicitly stated otherwise, this course’s policy is that you may make use of any online resources (e.g. RStudio Community, StackOverflow, etc.) but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solution(s).\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source"
  },
  {
    "objectID": "slides/lec-1.html#academic-integrity",
    "href": "slides/lec-1.html#academic-integrity",
    "title": "Welcome to STA 210!",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nTo uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "slides/lec-1.html#most-importantly",
    "href": "slides/lec-1.html#most-importantly",
    "title": "Welcome to STA 210!",
    "section": "Most importantly!",
    "text": "Most importantly!\nAsk if you’re not sure if something violates a policy!"
  },
  {
    "objectID": "slides/lec-1.html#five-tips-for-success",
    "href": "slides/lec-1.html#five-tips-for-success",
    "title": "Welcome to STA 210!",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work before class.\nAsk questions.\nDo the readings.\nDo the homework and lab.\nDon’t procrastinate and don’t let a week pass by with lingering questions."
  },
  {
    "objectID": "slides/lec-1.html#learning-during-a-pandemic",
    "href": "slides/lec-1.html#learning-during-a-pandemic",
    "title": "Welcome to STA 210!",
    "section": "Learning during a pandemic",
    "text": "Learning during a pandemic\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don’t hesitate to ask.\n\n\nYou never owe me personal information about your health (mental or physical) but you’re always welcome to talk to me. If I can’t help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded during this crisis."
  },
  {
    "objectID": "slides/lec-1.html#this-weeks-tasks",
    "href": "slides/lec-1.html#this-weeks-tasks",
    "title": "Welcome to STA 210!",
    "section": "This week’s tasks",
    "text": "This week’s tasks\n\nGet a GitHub account if you don’t have one (some advice for choosing a username here)\nComplete the Getting to know you survey if you haven’t yet done so!\nRead the syllabus\nWatch out for next week’s announcement email, in your inbox sometime tomorrow"
  },
  {
    "objectID": "slides/lec-1.html#midori-says",
    "href": "slides/lec-1.html#midori-says",
    "title": "Welcome to STA 210!",
    "section": "Midori says…",
    "text": "Midori says…"
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures",
    "href": "course-support.html#lectures",
    "title": "Course support",
    "section": "Lectures",
    "text": "Lectures\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone’s office hours here."
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that can’t wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Ed discussion as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Ed discussion), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email dr. gwynn sturdevant at gwynnc@wharton.upenn.edu.\nIf there is a question that’s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include “STAT 1010” and only “STAT 1010” in the subject line. You can also contact me privately on Canvas. Barring extenuating circumstances, I will respond to STAT 1010 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Weingarten learning center. The Weingarten Center offers a variety of resources to support all Penn students in reaching their academic goals. They offer multiple workshops throughout the semester and students can request workshops. All services are free and confidential. To contact the Weingarten Center, call 215-573-9235. The office is located in Stouffer Commons at 3702 Spruce Street, Suite 300.\nLearning Consultations offers individual consultations and group workshops that support students in developing more efficient and effective study skills and learning strategies. Learning specialists work with students to address time and project management, academic reading and writing, note-taking, problem-solving, exam preparation, test-taking, self-regulation, and flexibility.\n\nTutoring offers free access to on-campus tutors for many Penn courses in both drop-in and weekly contract format. Tutoring may be individual or in small groups. Tutors will assist with applying course information, understanding key concepts, and developing course-specific strategies. Tutoring support is available throughout the term but is best accessed early in the semester. First-time users must meet with a staff member; returning users may submit their requests online.\n\nAdditionally, Marks Family Writing Center provides expert help in writing for undergraduate and graduate students. Communication Within the Curriculum helps students express themselves orally with clarity and confidence. Language Direct provides tutoring for foreign languages. Van\nPelt Library supports students in research and instructional technologies through a range of workshops and consultations."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nCounseling and Psychological Services (CAPS) provides professional psychological and psychiatric services to students who need support in fulfilling their academic, social, and personal objectives. CAPS directly supports student mental health through counseling, crisis management, consultation, education and outreach, and training. CAPS has a satellite office in Huntsman Hall, and students with urgent concerns can talk with a CAPS clinician 24/7 at 215.898.7021 (press 1) or visit CAPS' main office at 3624 Market Street during business hours.\n\nStudent Health Service provides students with accessible, cost-effective, culturally-sensitive, and student-focused healthcare, including care for acute and chronic health problems, preventive health services, and health and wellness education.\n\nAlcohol and Other Drug Program Initiatives (AOD) works to reduce harm related to alcohol and other drug use at Penn.\n\nPenn Violence Prevention (PVP) engages the Penn community in the prevention of sexual violence, relationship violence, stalking, and sexual harassment on campus. PVP provides education and outreach and the staff serves as confidential resources for students.\n\nPublic Safety: From riding your bike on campus, to preventing unattended theft, the Division of Public Safety wants to make sure you have the information you need to protect your safety and your belongings.\n\nPenn Recreation: Penn offers many opportunities for students to participate in competitive team sports and stay physically fit at state-of-the-art, world-class training centers.\n\nWharton Wellness is a division-sponsored student organization that works to implement initiatives targeted at specific wellness issues in the Wharton community by creating experiences, fostering a positive culture of well-being, and connecting clubs/students to wellness resources.\n\nIt is important to me that you have the resources you need to be able to focus on learning in this course – this includes both the necessary academic materials as well as taking care of your day-to-day needs. Students experiencing difficulty affording the course materials should reach out to the Penn First Plus office (pennfirstplus@upenn.edu). Students who are struggling to afford sufficient food to eat every day and/or lack a safe and suitable space to live should contact Student Intervention Services (vpul-sisteam@pobox.upenn.edu). Students may also wish to contact their Financial Aid Counselor or Academic Advisor about these concerns. You are welcome to notify me if any of these challenges are affecting your success in this course, as long as you are comfortable doing so – I may have resources to support you."
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited. Additional support is also available through Student Intervention Services."
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-canvas",
    "href": "course-support.html#assistance-with-zoom-or-canvas",
    "title": "Course support",
    "section": "Assistance with Zoom or Canvas",
    "text": "Assistance with Zoom or Canvas\nFor technical help with Canvas, Gradescope, or Zoom, contact Wharton Student Computing. You can also access the self-service help documentation for Zoom here, Canvas here, and for Gradescope here.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "hw/hw-1.html",
    "href": "hw/hw-1.html",
    "title": "HW 1 - Titanic dataset",
    "section": "",
    "text": "For this homework assignment we will be exploring the Titanic dataset.\n\n\nIn this assignment, you will…\n\nDownload and import data into R\nExplore categorical variables\nCompute the expected counts of a \\(\\chi^2\\) distribution and\nPerform a \\(\\chi^2\\) test on some variables"
  },
  {
    "objectID": "hw/hw-1.html#getting-started",
    "href": "hw/hw-1.html#getting-started",
    "title": "HW 1 - Titanic dataset",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to RStudio\nClick on your Stat1010.Rproj.\nGo to File ➛ New File ➛ Quarto Document and name the document hw-1 click create"
  },
  {
    "objectID": "hw/hw-1.html#packages",
    "href": "hw/hw-1.html#packages",
    "title": "HW 1 - Titanic dataset",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used in this assignment:\n\nlibrary(tidyverse) # for data manipulation and data visualization\nlibrary(here) # to organize files"
  },
  {
    "objectID": "hw/hw-1.html#data-titanic",
    "href": "hw/hw-1.html#data-titanic",
    "title": "HW 1 - Titanic dataset",
    "section": "Data: Titanic",
    "text": "Data: Titanic\nOn April 15, 1912, the largest passenger liner ever made collided with an iceberg during her maiden voyage. When the Titanic sank it killed 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships. One of the reasons that the shipwreck resulted in such loss of life wasthat there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others.\nThe titanic.csv file contains data for 887 of the real Titanic passengers. Each row represents one person. The columns describe different attributes about the person including whether they survived, their age, their passenger-class, their sex, and the fare they paid\n\nPclass: The class of passengers on the titanic, with \\(1st\\) being the highest class, and \\(3rd\\) the lowest.\nSurvived: A variable that records whether or not a passenger survived the sinking of the titanic.\n\nPart 1: Download and import the data\n\nDownload the file into your Stat1010 R project in the data folder.\nImport your data into R.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis may be a good time to render your document to ensure that the data loads without errors.\n\n\nPart 2: Exploring categorical variables\n\nHow many observations and how many variables in this dataset?\nClassify all the variables into two categories: qualitative or quantitative\nHow many levels of passenger class are there in the variable?\nHow many passengers survived the sinking of the titanic?\nMake a contingency table of class and survival. Which combination of class and survival is the most common?\n\nWhat proportion of \\(3rd\\) class passengers survived?\nWhat proportion of \\(1st\\) class passengers survived?\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis may be a good time to render your document.\n\n\n\nDraw a plot that conditions on class and displays the proportions of each that survived and update labels. What does the plot suggest regarding the chance of survival and your class? Does it suggest that they are associated?\nAssuming Survived and Pclass are independent, write R coding to find the expected counts for all cells of the contingency table. (Hint: you will need to use the rowSums(.[2:3]) and colSums(.[2:4]) functions, depending on the order that you compute the two.)\nPerform a \\(\\chi^2\\) test on the two variables Survived and Pclass. Include both \\(H_0\\) and \\(H_A\\) and interpret the \\(p-value\\). What conclusions can you draw about the independence of Survived and Pclass?\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore submitting, make sure you render your document,"
  },
  {
    "objectID": "hw/hw-1.html#submission",
    "href": "hw/hw-1.html#submission",
    "title": "HW 1 - Titanic dataset",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nThen submit the pdf following the link on the course website."
  },
  {
    "objectID": "hw/hw-1.html#grading",
    "href": "hw/hw-1.html#grading",
    "title": "HW 1 - Titanic dataset",
    "section": "Grading",
    "text": "Grading\nTotal points available: 25 points.\n\n\n\nComponent\nPoints\n\n\n\n\nPart 1 Qu 1\n2\n\n\nPart 1 Qu 2\n3\n\n\nPart 2 Qu 1\n2\n\n\nPart 2 Qu 2\n2\n\n\nPart 2 Qu 3\n1\n\n\nPart 2 Qu 4\n2\n\n\nPart 2 Qu 5 Pt 1\n1\n\n\nPart 2 Qu 5 Pt 2\n1\n\n\nPart 2 Qu 6\n3\n\n\nPart 2 Qu 7\n4\n\n\nPart 2 Qu 8\n4"
  },
  {
    "objectID": "hw/hw-2.html",
    "href": "hw/hw-2.html",
    "title": "HW 2 - Titanic dataset",
    "section": "",
    "text": "For this homework assignment we will be exploring the Titanic dataset.\n\n\nIn this assignment, you will…\n\nImport data into R\nExplore numeric variables\nCompute means for subgroups and compare them\nComment on and produce a pairs plot of numeric variables\nAddress overplotting"
  },
  {
    "objectID": "hw/hw-2.html#getting-started",
    "href": "hw/hw-2.html#getting-started",
    "title": "HW 2 - Titanic dataset",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to RStudio\nClick on your Stat1010.Rproj that we made the first day of class.\nGo to File ➛ New File ➛ Quarto Document and name the document hw-2 click create."
  },
  {
    "objectID": "hw/hw-2.html#packages",
    "href": "hw/hw-2.html#packages",
    "title": "HW 2 - Titanic dataset",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used in this assignment:\n\nlibrary(tidyverse) # for data manipulation and data visualization\nlibrary(here) # to organize files\nlibrary(GGally) # for the pairs plot"
  },
  {
    "objectID": "hw/hw-2.html#data-titanic",
    "href": "hw/hw-2.html#data-titanic",
    "title": "HW 2 - Titanic dataset",
    "section": "Data: Titanic",
    "text": "Data: Titanic\nOn April 15, 1912, the largest passenger liner ever made collided with an iceberg during her maiden voyage. When the Titanic sank it killed 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships. One of the reasons that the shipwreck resulted in such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others.\nThe titanic.csv file contains data for 887 of the real Titanic passengers. Each row represents one person. The columns describe different attributes about the person including whether they survived, their age, their passenger-class, their sex, and the fare they paid\n\nPclass: The class of passengers on the titanic, with \\(1st\\) being the highest class, and \\(3rd\\) the lowest.\nSurvived: A variable that records whether or not a passenger survived the sinking of the titanic.\nFare: A variable that records the passenger fare.\n\n\nImport the data into R\nFor each value of Pclass, compute the mean fare and standard deviation paid by the passengers in that class. Comment on them in context.\nFor each value of Survived, compute the mean fare and standard deviation paid by the passengers with that survival status. Comment on them in context.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis may be a good time to render your document.\n\n\n\nDraw a pairs plot of all numeric variables in the dataset and comment on all scatterplots and densities. Be sure to use variable names.\nExplore any outliers by filtering and looking at the data. Answer questions like this, but include others that spark your curiosity. Include a minimum of 2 additional outliers.\n\nWhich was the biggest family aboard?\nWhich and how many people paid the highest fair?\n\nDraw one plot containing 3 boxplots of the Fare paid for each category of Pclass. What conclusions can you draw from this plot? Consider our last assignment and how the passenger class predicted survival.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis may be a good time to render your document.\n\n\n\nDraw a scatterplot of Pclass and Fare, and color it by the survival variable. (Hint: use the function geom_jitter() to avoid overplotting.) Comment on the plot.\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore submitting, make sure you render your document,"
  },
  {
    "objectID": "hw/hw-2.html#submission",
    "href": "hw/hw-2.html#submission",
    "title": "HW 2 - Titanic dataset",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nThen submit the pdf following the link on the course website."
  },
  {
    "objectID": "hw/hw-2.html#grading",
    "href": "hw/hw-2.html#grading",
    "title": "HW 2 - Titanic dataset",
    "section": "Grading",
    "text": "Grading\nTotal points available: 38 points\n\n\n\nComponent\nPoints\n\n\n\n\nQu 1\n2\n\n\nQu 2\n4\n\n\nQu 3\n4\n\n\nQu 4\n15\n\n\nQu 5\n4\n\n\nQu 6\n5\n\n\nQu 7\n4"
  },
  {
    "objectID": "hw/hw-6.html",
    "href": "hw/hw-6.html",
    "title": "HW 6 - Statistics Experience",
    "section": "",
    "text": "The world of statistics and data science is vast and continually growing! The goal of the statistics experience assignments is to help you engage with the statistics and data science communities outside of the classroom.\nYou may submit the statistics experience assignment anytime between now and the deadline.\nEach experience has two parts:\n1️⃣ Have a statistics experience\n2️⃣ Make a slide summarizing on your experience\nYou must complete both parts to receive credit."
  },
  {
    "objectID": "hw/hw-6.html#part-1-experience-statistics-outside-of-the-classroom",
    "href": "hw/hw-6.html#part-1-experience-statistics-outside-of-the-classroom",
    "title": "HW 6 - Statistics Experience",
    "section": "Part 1: Experience statistics outside of the classroom",
    "text": "Part 1: Experience statistics outside of the classroom\nComplete an activity in one of the categories below. Under each category are suggested activities. You do not have to do one these suggested activities. You are welcome to find other activities as long as they are related to statistics/data science and they fit in one of the six categories. If there is an activity you’d like to do but you’re not sure if it qualifies for the statistics experience, just ask!\n\nCategory 1: Attend a talk or conference\nAttend an talk, panel, or conference related to statistics or data science. If you are attending a single talk or panel, it must be at least 30 minutes to count towards the statistics experience. The event can be in-person or online.\n\n\nCategory 2: Talk with a statistician/ data scientist\nTalk with someone who uses statistics in their daily work. This could include a professional in industry, a speaker from the RLadies directory, or other local data science groups.\n\n\nCategory 3: Listen to a podcast / watch video\nListen to a podcast or watch a video about statistics and data science. The podcast or video must be at least 30 minutes to count towards the statistics experience. A few suggestions are below:\n\nStats + Stories Podcast\nCausal Inference Podcast\nFiveThirtyEight Model Talk\nrstudio::conf 2022\nrstudio::global 2021 talks\nrstudio::conf 2020 talks\n\nThis list is not exhaustive. You may listen to other podcasts or watch other statistics/data science videos not included on this list. Ask your professor if you are unsure whether a particular podcast or video will count towards the statistics experience.\n\n\nCategory 4: Participate in a data science competition or challenge\nParticipate in a statistics or data science competition. You can participate individually or with a team. Kaggle and Drivendata both lead online competitions. This is a comprehensive list of hackathons, and more information can be found here or here.\n\n\nCategory 5: Read a book on statistics/data science\nThere are a lot of books about statistics, data science, and related topics. A few suggestions are below. If you decide to read a book that isn’t on this list, ask your professor to make sure it counts toward the experience. Many of these books are available through the Van Pelt Library.\n\nWeapons of Math Destruction by Cathy O’Neil\nHow Charts Lie: Getting Smarter about Visual Information by Alberto Cairo\nThe Theory that Would Not Die by Sharon Bertsch McGrayne\nThe Art of Statistics: How to learn from data by David Spiegelhalter\nThe Signal and the Noise: Why so many predictions fail - but some don’t by Nate Silver\nHow Charts Lie by Alberto Cairo\nList of books about data science ethics\n\n\n\nCategory 6: TidyTuesday\nYou may also participate in a TidyTuesday challenge. New data sets are announced on Monday afternoons.You can find more information about TidyTuesday and see the data in the TidyTuesday GitHub repo.\nA few guidelines:\n✅ Create a project for your TidyTuesday submission - The Quarto file with all the code needed to reproduce your visualization. - A pdf that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n✅ The visualization should include features or customization that are beyond what we’ve done in class ."
  },
  {
    "objectID": "hw/hw-6.html#part-2-summarize-your-experience",
    "href": "hw/hw-6.html#part-2-summarize-your-experience",
    "title": "HW 6 - Statistics Experience",
    "section": "Part 2: Summarize your experience",
    "text": "Part 2: Summarize your experience\nMake one slide summarizing your experience. Submit the slide as a PDF on Canvas.\nInclude the following on your slide:\n\nName and brief description of the event/podcast/competition/etc.\nSomething you found new, interesting, or unexpected\nHow the event/podcast/competition/etc. connects to something we’ve done in class.\nCitation or link to web page for event/competition/etc.\n\nClick here to see a template to help you get started on your slide. Your slide does not have to follow this exact format; it just needs to include the information mentioned above and be easily readable (i.e. use a reasonable font size!). Creativity is encouraged!"
  },
  {
    "objectID": "hw/hw-6.html#submission",
    "href": "hw/hw-6.html#submission",
    "title": "HW 6 - Statistics Experience",
    "section": "Submission",
    "text": "Submission\nSubmit the reflection as a PDF under the HW 6 - Statistics Experience assignment on Convas by Tuesday, Nov 22 at 5 pm ET. It must be submitted by the deadline on Canvas to be considered for grading."
  },
  {
    "objectID": "hw/hw-3.html",
    "href": "hw/hw-3.html",
    "title": "HW - 3 Diamonds dataset",
    "section": "",
    "text": "For this homework assignment we will be exploring the Diamonds dataset. You will find this document which introduces R commands necessary for this homework indispensable. Please review it thoroughly.\n\n\nIn this assignment, you will…\n\nFind confidence intervals and perform hypothesis tests for means in R\nInterpret confidence intervals and hypothesis tests for the mean.\nPerform a \\(\\chi^2\\) test and determine which cells contribute most to a test statistic.\nPerform paired tests and interpret them."
  },
  {
    "objectID": "hw/hw-3.html#getting-started",
    "href": "hw/hw-3.html#getting-started",
    "title": "HW - 3 Diamonds dataset",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to RStudio\nClick on your Stat1010.Rproj that we made the first day of class.\nGo to File ➛ New File ➛ Quarto Document and name the document hw-3 click create."
  },
  {
    "objectID": "hw/hw-3.html#packages",
    "href": "hw/hw-3.html#packages",
    "title": "HW - 3 Diamonds dataset",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used in this assignment:\n\nlibrary(tidyverse) # for data manipulation and data visualization\nlibrary(here) # to organize files"
  },
  {
    "objectID": "hw/hw-3.html#data-diamonds",
    "href": "hw/hw-3.html#data-diamonds",
    "title": "HW - 3 Diamonds dataset",
    "section": "Data: Diamonds",
    "text": "Data: Diamonds\nThe diamonds dataset, from the tidyverse() set of packages, contains information on 53,940 round diamonds from the Loose Diamonds Search Engine.\n\nIn R, compute a 92% confidence interval for the average Price for a diamond and interpret it in context.\nIn R, test the hypothesis that the average Price of a diamond is greater than $3500 and interpret it. Include all hypotheses, and the test statistic.\nThe variable cut has 5 levels: fair, good, very good, premium, and ideal. Choose one level to compare to Fair and find the 95% confidence interval for the differences in prices between the level-of-your-choice and Fair diamonds. Interpret it in context.\nPerform a \\(\\chi^2\\) test on the variables cut and clarity in diamonds and interpret it in context. Include all hypotheses, expected counts, degrees of freedom, and the test statistic. Which cell(s) contributed to the value of the test statistic?\nPerform 3 hypothesis tests to see if any of the variables length (\\(x\\)), width (\\(y\\)), and depth (\\(z\\)) of a diamond come from populations with the same mean. State all hypotheses, p-values, and test statistics, then interpret the results in context. Draw a suitable plot that shows the distribution of the 3 variables and comment on it."
  },
  {
    "objectID": "hw/hw-3.html#section",
    "href": "hw/hw-3.html#section",
    "title": "HW - 3 Diamonds dataset",
    "section": "",
    "text": "Component\nPoints\n\n\n\n\n1\n2\n\n\n2\n3\n\n\n3\n3\n\n\n4\n9\n\n\n5\n15"
  },
  {
    "objectID": "hw/hw-4.html",
    "href": "hw/hw-4.html",
    "title": "hw-4-instructions",
    "section": "",
    "text": "For this homework assignment we will be exploring the Diamonds dataset. You will find this document which introduces R commands necessary for this homework indispensable. Please review it thoroughly.\n\n\nIn this assignment, you will…\n\nFind confidence intervals and perform hypothesis tests for proportions in R.\nInterpret confidence intervals and hypothesis tests for the proportions.\nFit a linear model to data in R.\nFind and plot residuals in R."
  },
  {
    "objectID": "hw/hw-4.html#getting-started",
    "href": "hw/hw-4.html#getting-started",
    "title": "hw-4-instructions",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to RStudio\nClick on your Stat1010.Rproj that we made the first day of class.\nGo to File ➛ New File ➛ Quarto Document and name the document hw-4 click create."
  },
  {
    "objectID": "hw/hw-4.html#packages",
    "href": "hw/hw-4.html#packages",
    "title": "hw-4-instructions",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used in this assignment:\n\nlibrary(tidyverse) # for data manipulation and data visualization\nlibrary(tidymodels) # for model fitting and to get residuals"
  },
  {
    "objectID": "hw/hw-4.html#data-diamonds",
    "href": "hw/hw-4.html#data-diamonds",
    "title": "hw-4-instructions",
    "section": "Data: Diamonds",
    "text": "Data: Diamonds\nThe diamonds dataset, from the tidyverse() set of packages, contains information on 53,940 round diamonds from the Loose Diamonds Search Engine.\n\nIn R, compute a 97% confidence interval for the population proportion of diamonds that have an Ideal cut and interpret it in context.\nIn R, test the hypothesis that the population proportion of Fair diamonds is not equal to 3.5% and interpret it. Include all hypotheses, and the test statistic.\nIn R, test the hypothesis that the population proportion of diamonds with Very Good cut and the best colour and is equal to the population proportion of diamonds with Premium cut and the best colour. Include all hypotheses, and the test statistic, and interpret the test in context.\nUse R to fit a least squares line of best fit to predict the price of a diamond using x. Interpret \\(b_0\\), \\(b_1\\), and \\(R^2\\) in context, then plot the residuals and comment on the plot. Find the predicted value of price for \\(x = 1.5\\) and comment on the validity of this prediction. If a diamonds length increased by 5mm, our model predicts what increase in price?\n\n\n\n\nComponent\nPoints\n\n\n\n\n1\n2\n\n\n2\n4\n\n\n3\n3\n\n\n4\n10"
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Open your Stat1010 project then go to file and open a quarto document. You can add code chunks by clicking on the green button in the IDE and answer some questions directly in the document, and add further code chunks. Hit render and if there are no errors in your code, the pdf should pop up. You can then upload to either Canvas or Gradescope"
  },
  {
    "objectID": "documents/about_R.html",
    "href": "documents/about_R.html",
    "title": "About R",
    "section": "",
    "text": "The R programming language was first released on [29 February 2000](https://www.hermetic.ch/y2k/feb29.htm). [Ross Ihaka](https://en.wikipedia.org/wiki/Ross_Ihaka) and [Robert Gentleman](https://en.wikipedia.org/wiki/Robert_Gentleman_(statistician)) wanted a free, open-source language that their statistics students at the University of Auckland could use to fit statistical models. [R is now an internationally recognized language used all over the world](https://www.nytimes.com/2009/01/07/technology/business-computing/07program.html). It is possible to do a multitude of advanced computations in R but to keep R nimble, we don’t download all of the things at once. Instead, coding is bundled together into something called “packages” that are stored on [CRAN](https://cran.r-project.org/)."
  }
]