[
  {
    "objectID": "documents/about_R.html",
    "href": "documents/about_R.html",
    "title": "About R",
    "section": "",
    "text": "The R programming language was first released on [29 February 2000](https://www.hermetic.ch/y2k/feb29.htm). [Ross Ihaka](https://en.wikipedia.org/wiki/Ross_Ihaka) and [Robert Gentleman](https://en.wikipedia.org/wiki/Robert_Gentleman_(statistician)) wanted a free, open-source language that their statistics students at the University of Auckland could use to fit statistical models. [R is now an internationally recognized language used all over the world](https://www.nytimes.com/2009/01/07/technology/business-computing/07program.html). It is possible to do a multitude of advanced computations in R but to keep R nimble, we don’t download all of the things at once. Instead, coding is bundled together into something called “packages” that are stored on [CRAN](https://cran.r-project.org/)."
  },
  {
    "objectID": "hw/hw-2.html",
    "href": "hw/hw-2.html",
    "title": "HW 2 - Titanic dataset",
    "section": "",
    "text": "For this homework assignment we will be exploring the Titanic dataset.\n\n\nIn this assignment, you will…\n\nImport data into R\nExplore numeric variables\nCompute means for subgroups and compare them\nComment on and produce a pairs plot of numeric variables\nAddress overplotting"
  },
  {
    "objectID": "hw/hw-2.html#getting-started",
    "href": "hw/hw-2.html#getting-started",
    "title": "HW 2 - Titanic dataset",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to RStudio\nClick on your Stat1010.Rproj that we made the first day of class.\nGo to File ➛ New File ➛ Quarto Document and name the document hw-2 click create."
  },
  {
    "objectID": "hw/hw-2.html#packages",
    "href": "hw/hw-2.html#packages",
    "title": "HW 2 - Titanic dataset",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used in this assignment:\n\nlibrary(tidyverse) # for data manipulation and data visualization\nlibrary(here) # to organize files\nlibrary(GGally) # for the pairs plot"
  },
  {
    "objectID": "hw/hw-2.html#data-titanic",
    "href": "hw/hw-2.html#data-titanic",
    "title": "HW 2 - Titanic dataset",
    "section": "Data: Titanic",
    "text": "Data: Titanic\nOn April 15, 1912, the largest passenger liner ever made collided with an iceberg during her maiden voyage. When the Titanic sank it killed 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships. One of the reasons that the shipwreck resulted in such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others.\nThe titanic.csv file contains data for 887 of the real Titanic passengers. Each row represents one person. The columns describe different attributes about the person including whether they survived, their age, their passenger-class, their sex, and the fare they paid\n\nPclass: The class of passengers on the titanic, with \\(1st\\) being the highest class, and \\(3rd\\) the lowest.\nSurvived: A variable that records whether or not a passenger survived the sinking of the titanic.\nFare: A variable that records the passenger fare.\n\n\nImport the data into R\nFor each value of Pclass, compute the mean fare and standard deviation paid by the passengers in that class. Comment on them in context.\nFor each value of Survived, compute the mean fare and standard deviation paid by the passengers with that survival status. Comment on them in context.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis may be a good time to render your document.\n\n\n\nDraw a pairs plot of all numeric variables in the dataset and comment on all scatterplots and densities. Be sure to use variable names.\nExplore any outliers by filtering and looking at the data. Answer questions like this, but include others that spark your curiosity. Include a minimum of 2 additional outliers.\n\nWhich was the biggest family aboard?\nWhich and how many people paid the highest fair?\n\nDraw one plot containing 3 boxplots of the Fare paid for each category of Pclass. What conclusions can you draw from this plot? Consider our last assignment and how the passenger class predicted survival.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis may be a good time to render your document.\n\n\n\nDraw a scatterplot of Pclass and Fare, and color it by the survival variable. (Hint: use the function geom_jitter() to avoid overplotting.) Comment on the plot.\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore submitting, make sure you render your document,"
  },
  {
    "objectID": "hw/hw-2.html#submission",
    "href": "hw/hw-2.html#submission",
    "title": "HW 2 - Titanic dataset",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nThen submit the pdf following the link on the course website."
  },
  {
    "objectID": "hw/hw-2.html#grading",
    "href": "hw/hw-2.html#grading",
    "title": "HW 2 - Titanic dataset",
    "section": "Grading",
    "text": "Grading\nTotal points available: 38 points\n\n\n\nComponent\nPoints\n\n\n\n\nQu 1\n2\n\n\nQu 2\n4\n\n\nQu 3\n4\n\n\nQu 4\n15\n\n\nQu 5\n4\n\n\nQu 6\n5\n\n\nQu 7\n4"
  },
  {
    "objectID": "hw/hw-1.html",
    "href": "hw/hw-1.html",
    "title": "HW 1 - Titanic dataset",
    "section": "",
    "text": "For this homework assignment we will be exploring the Titanic dataset.\n\n\nIn this assignment, you will…\n\nDownload and import data into R\nExplore categorical variables\nCompute the expected counts of a \\(\\chi^2\\) distribution and\nPerform a \\(\\chi^2\\) test on some variables"
  },
  {
    "objectID": "hw/hw-1.html#getting-started",
    "href": "hw/hw-1.html#getting-started",
    "title": "HW 1 - Titanic dataset",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to RStudio\nClick on your Stat1010.Rproj.\nGo to File ➛ New File ➛ Quarto Document and name the document hw-1 click create"
  },
  {
    "objectID": "hw/hw-1.html#packages",
    "href": "hw/hw-1.html#packages",
    "title": "HW 1 - Titanic dataset",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used in this assignment:\n\nlibrary(tidyverse) # for data manipulation and data visualization\nlibrary(here) # to organize files"
  },
  {
    "objectID": "hw/hw-1.html#data-titanic",
    "href": "hw/hw-1.html#data-titanic",
    "title": "HW 1 - Titanic dataset",
    "section": "Data: Titanic",
    "text": "Data: Titanic\nOn April 15, 1912, the largest passenger liner ever made collided with an iceberg during her maiden voyage. When the Titanic sank it killed 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships. One of the reasons that the shipwreck resulted in such loss of life wasthat there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others.\nThe titanic.csv file contains data for 887 of the real Titanic passengers. Each row represents one person. The columns describe different attributes about the person including whether they survived, their age, their passenger-class, their sex, and the fare they paid\n\nPclass: The class of passengers on the titanic, with \\(1st\\) being the highest class, and \\(3rd\\) the lowest.\nSurvived: A variable that records whether or not a passenger survived the sinking of the titanic.\n\nPart 1: Download and import the data\n\nDownload the file into your Stat1010 R project in the data folder.\nImport your data into R.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis may be a good time to render your document to ensure that the data loads without errors.\n\n\nPart 2: Exploring categorical variables\n\nHow many observations and how many variables in this dataset?\nClassify all the variables into two categories: qualitative or quantitative\nHow many levels of passenger class are there in the variable?\nHow many passengers survived the sinking of the titanic?\nMake a contingency table of class and survival. Which combination of class and survival is the most common?\n\nWhat proportion of \\(3rd\\) class passengers survived?\nWhat proportion of \\(1st\\) class passengers survived?\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis may be a good time to render your document.\n\n\n\nDraw a plot that conditions on class and displays the proportions of each that survived and update labels. What does the plot suggest regarding the chance of survival and your class? Does it suggest that they are associated?\nAssuming Survived and Pclass are independent, write R coding to find the expected counts for all cells of the contingency table. (Hint: you will need to use the rowSums(.[2:3]) and colSums(.[2:4]) functions, depending on the order that you compute the two.)\nPerform a \\(\\chi^2\\) test on the two variables Survived and Pclass. Include both \\(H_0\\) and \\(H_A\\) and interpret the \\(p-value\\). What conclusions can you draw about the independence of Survived and Pclass?\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore submitting, make sure you render your document,"
  },
  {
    "objectID": "hw/hw-1.html#submission",
    "href": "hw/hw-1.html#submission",
    "title": "HW 1 - Titanic dataset",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nThen submit the pdf following the link on the course website."
  },
  {
    "objectID": "hw/hw-1.html#grading",
    "href": "hw/hw-1.html#grading",
    "title": "HW 1 - Titanic dataset",
    "section": "Grading",
    "text": "Grading\nTotal points available: 25 points.\n\n\n\nComponent\nPoints\n\n\n\n\nPart 1 Qu 1\n2\n\n\nPart 1 Qu 2\n3\n\n\nPart 2 Qu 1\n2\n\n\nPart 2 Qu 2\n2\n\n\nPart 2 Qu 3\n1\n\n\nPart 2 Qu 4\n2\n\n\nPart 2 Qu 5 Pt 1\n1\n\n\nPart 2 Qu 5 Pt 2\n1\n\n\nPart 2 Qu 6\n3\n\n\nPart 2 Qu 7\n4\n\n\nPart 2 Qu 8\n4"
  },
  {
    "objectID": "hw/hw-3.html",
    "href": "hw/hw-3.html",
    "title": "HW - 3 Diamonds dataset",
    "section": "",
    "text": "For this homework assignment we will be exploring the Diamonds dataset. You will find this document which introduces R commands necessary for this homework indispensable. Please review it thoroughly.\n\n\nIn this assignment, you will…\n\nFind confidence intervals and perform hypothesis tests for means in R\nInterpret confidence intervals and hypothesis tests for the mean.\nPerform a \\(\\chi^2\\) test and determine which cells contribute most to a test statistic.\nPerform paired tests and interpret them."
  },
  {
    "objectID": "hw/hw-3.html#getting-started",
    "href": "hw/hw-3.html#getting-started",
    "title": "HW - 3 Diamonds dataset",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to RStudio\nClick on your Stat1010.Rproj that we made the first day of class.\nGo to File ➛ New File ➛ Quarto Document and name the document hw-3 click create."
  },
  {
    "objectID": "hw/hw-3.html#packages",
    "href": "hw/hw-3.html#packages",
    "title": "HW - 3 Diamonds dataset",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used in this assignment:\n\nlibrary(tidyverse) # for data manipulation and data visualization\nlibrary(here) # to organize files"
  },
  {
    "objectID": "hw/hw-3.html#data-diamonds",
    "href": "hw/hw-3.html#data-diamonds",
    "title": "HW - 3 Diamonds dataset",
    "section": "Data: Diamonds",
    "text": "Data: Diamonds\nThe diamonds dataset, from the tidyverse() set of packages, contains information on 53,940 round diamonds from the Loose Diamonds Search Engine.\n\nIn R, compute a 92% confidence interval for the average Price for a diamond and interpret it in context.\nIn R, test the hypothesis that the average Price of a diamond is greater than $3500 and interpret it. Include all hypotheses, and the test statistic.\nThe variable cut has 5 levels: fair, good, very good, premium, and ideal. Choose one level to compare to Fair and find the 95% confidence interval for the differences in prices between the level-of-your-choice and Fair diamonds. Interpret it in context.\nPerform a \\(\\chi^2\\) test on the variables cut and clarity in diamonds and interpret it in context. Include all hypotheses, expected counts, degrees of freedom, and the test statistic. Which cell(s) contributed to the value of the test statistic?\nPerform 3 hypothesis tests to see if any of the variables length (\\(x\\)), width (\\(y\\)), and depth (\\(z\\)) of a diamond come from populations with the same mean. State all hypotheses, p-values, and test statistics, then interpret the results in context. Draw a suitable plot that shows the distribution of the 3 variables and comment on it."
  },
  {
    "objectID": "hw/hw-3.html#section",
    "href": "hw/hw-3.html#section",
    "title": "HW - 3 Diamonds dataset",
    "section": "",
    "text": "Component\nPoints\n\n\n\n\n1\n2\n\n\n2\n3\n\n\n3\n3\n\n\n4\n9\n\n\n5\n15"
  },
  {
    "objectID": "hw/hw-6.html",
    "href": "hw/hw-6.html",
    "title": "HW 6 - Statistics Experience",
    "section": "",
    "text": "The world of statistics and data science is vast and continually growing! The goal of the statistics experience assignments is to help you engage with the statistics and data science communities outside of the classroom.\nYou may submit the statistics experience assignment anytime between now and the deadline.\nEach experience has two parts:\n1️⃣ Have a statistics experience\n2️⃣ Make a slide summarizing on your experience\nYou must complete both parts to receive credit."
  },
  {
    "objectID": "hw/hw-6.html#part-1-experience-statistics-outside-of-the-classroom",
    "href": "hw/hw-6.html#part-1-experience-statistics-outside-of-the-classroom",
    "title": "HW 6 - Statistics Experience",
    "section": "Part 1: Experience statistics outside of the classroom",
    "text": "Part 1: Experience statistics outside of the classroom\nComplete an activity in one of the categories below. Under each category are suggested activities. You do not have to do one these suggested activities. You are welcome to find other activities as long as they are related to statistics/data science and they fit in one of the six categories. If there is an activity you’d like to do but you’re not sure if it qualifies for the statistics experience, just ask!\n\nCategory 1: Attend a talk or conference\nAttend an talk, panel, or conference related to statistics or data science. If you are attending a single talk or panel, it must be at least 30 minutes to count towards the statistics experience. The event can be in-person or online.\n\n\nCategory 2: Talk with a statistician/ data scientist\nTalk with someone who uses statistics in their daily work. This could include a professional in industry, a speaker from the RLadies directory, or other local data science groups.\n\n\nCategory 3: Listen to a podcast / watch video\nListen to a podcast or watch a video about statistics and data science. The podcast or video must be at least 30 minutes to count towards the statistics experience. A few suggestions are below:\n\nStats + Stories Podcast\nCausal Inference Podcast\nFiveThirtyEight Model Talk\nrstudio::conf 2022\nrstudio::global 2021 talks\nrstudio::conf 2020 talks\n\nThis list is not exhaustive. You may listen to other podcasts or watch other statistics/data science videos not included on this list. Ask your professor if you are unsure whether a particular podcast or video will count towards the statistics experience.\n\n\nCategory 4: Participate in a data science competition or challenge\nParticipate in a statistics or data science competition. You can participate individually or with a team. Kaggle and Drivendata both lead online competitions. This is a comprehensive list of hackathons, and more information can be found here or here.\n\n\nCategory 5: Read a book on statistics/data science\nThere are a lot of books about statistics, data science, and related topics. A few suggestions are below. If you decide to read a book that isn’t on this list, ask your professor to make sure it counts toward the experience. Many of these books are available through the Van Pelt Library.\n\nWeapons of Math Destruction by Cathy O’Neil\nHow Charts Lie: Getting Smarter about Visual Information by Alberto Cairo\nThe Theory that Would Not Die by Sharon Bertsch McGrayne\nThe Art of Statistics: How to learn from data by David Spiegelhalter\nThe Signal and the Noise: Why so many predictions fail - but some don’t by Nate Silver\nHow Charts Lie by Alberto Cairo\nList of books about data science ethics\n\n\n\nCategory 6: TidyTuesday\nYou may also participate in a TidyTuesday challenge. New data sets are announced on Monday afternoons.You can find more information about TidyTuesday and see the data in the TidyTuesday GitHub repo.\nA few guidelines:\n✅ Create a project for your TidyTuesday submission - The Quarto file with all the code needed to reproduce your visualization. - A pdf that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n✅ The visualization should include features or customization that are beyond what we’ve done in class ."
  },
  {
    "objectID": "hw/hw-6.html#part-2-summarize-your-experience",
    "href": "hw/hw-6.html#part-2-summarize-your-experience",
    "title": "HW 6 - Statistics Experience",
    "section": "Part 2: Summarize your experience",
    "text": "Part 2: Summarize your experience\nMake one slide summarizing your experience. Submit the slide as a PDF on Gradescope.\nInclude the following on your slide:\n\nName and brief description of the event/podcast/competition/etc.\nSomething you found new, interesting, or unexpected\nHow the event/podcast/competition/etc. connects to something we’ve done in class.\nCitation or link to web page for event/competition/etc.\n\nClick here to see a template to help you get started on your slide. Your slide does not have to follow this exact format; it just needs to include the information mentioned above and be easily readable (i.e. use a reasonable font size!). Creativity is encouraged!"
  },
  {
    "objectID": "hw/hw-6.html#submission",
    "href": "hw/hw-6.html#submission",
    "title": "HW 6 - Statistics Experience",
    "section": "Submission",
    "text": "Submission\nSubmit the reflection as a PDF under the HW 6 - Statistics Experience assignment on Convas by Tuesday, Nov 22 at 5 pm ET. It must be submitted by the deadline on Canvas to be considered for grading."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 1010: Introduction to Business Statistics",
    "section": "",
    "text": "Week\nDate\nTopic\nPrepare\nSlides\nAE\nHW\nExam\nProject\n\n\n\n\n\n1\nMon, 29 Aug\nNo class - opening exercises\n\n\n\n\n\n\n\n\n\n\nWed, 31 Aug\nStarting with R\n\n🖥️\n📋\n\n\n\n\n\n\n2\nMon, 5 Sept\nNo class - Labor Day\n\n\n\n\n\n\n\n\n\n\nWed, 7 Sept\nContinuation of Intro to R\n📖\n🖥️\n\n\n\n\n\n\n\n3\nMon, 12 Sept\nCategorical variables\n📖\n🖥️\n📋\n\n\n\n\n\n\n\nWed, 14 Sept\nNumeric variables\n\n🖥️\n📋\n✍️\n\n\n\n\n\n4\nMon, 19 Sept\nIntro to probability\n📖\n🖥️\n📋\n✍️\n\n\n\n\n\n\nWed, 21 Sept\nMore probability rules\n\n🖥️\n📋\n\n\n\n\n\n\n5\nMon, 26 Sept\nRevision of AE 1-4\n📖\n\n📋\n\n\n\n\n\n\n\nWed, 28 Sept\nRevision of HW-1\n📖\n\n📋\n\n\n\n\n\n\n6\nMon, 3 Oct\nExam 1\n\n\n\n\n✅\n\n\n\n\n\nWed, 5 Oct\nRandom variables\n📖\n🖥️\n📋\n\n\n\n\n\n\n7\nMon, 10 Oct\nAssociation between quantitative variables\n📖\n🖥️\n\n\n\n\n\n\n\n\nWed, 12 Oct\nAssociation between random variables\n\n🖥️\n📋\n✍️\n\n\n\n\n\n8\nMon, 17 Oct\nProbability models for counts\n📖\n🖥️\n\n\n\n\n\n\n\n\nWed, 19 Oct\nThe normal probability model\n\n🖥️\n\n\n\n\n\n\n\n9\nMon, 24 Oct\nSamples and surveys\n📖\n🖥️\n\n\n\n\n\n\n\n\nWed, 26 Oct\nSampling variation and quality\n\n🖥️\n📋\n\n\n\n📂\n\n\n10\nMon, 31 Oct\nRevision for Exam 2\n\n🖥️\n\n\n\n\n\n\n\n\nWed, 2 Nov\nExam 2\n\n\n\n\n✅\n\n\n\n\n11\nMon, 7 Nov\nConfidence intervals\n\n🖥️\n📋\n\n\n\n\n\n\n\nWed, 9 Nov\nStatistical tests\n\n🖥️\n\n\n\n\n\n\n\n12\nMon, 14 Nov\nComparison\n\n🖥️\n\n\n\n\n\n\n\n\nWed, 16 Nov\nInference for counts\n\n🖥️\n📋\n✍️\n\n\n\n\n\n13\nMon, 21 Nov\nLinear patterns\n\n🖥️\n\n\n\n\n\n\n\n\nWed, 22 Nov\nNo class - Thanksgiving\n\n\n\n\n\n\n\n\n\n14\nMon, 28 Nov\nRevision\n\n\n\n\n\n\n\n\n\n\nWed, 30 Nov\nExam 3\n\n\n\n\n\n✅\n\n\n\n15\nMon, 5 Dec\nPresentations & Revision\n\n\n\n\n\n\n📂\n\n\n\nWed, 7 Dec\nPresentations & Revision\n\n\n\n\n\n\n📂\n\n\n16\nMon, 12 Dec\nLast midterm\n\n\n\n\n\n✅"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "STA 1010: Introduction to Business Statistics",
    "section": "",
    "text": "This course introduces students to the discipline of statistics as a science of understanding and analyzing data. Throughout the semester, students will learn how to effectively make use\nof data in the face of uncertainty: how to collect data, how to analyze data, and how to use data to make inferences and conclusions about real world phenomena.\n\nThe course goals are as follows:\n\nRecognize the importance of data collection, identify limitations in data collection methods, and determine how they affect the scope of inference.\nUse statistical software to summarize data numerically and visually, and to perform data analysis.\nHave a conceptual understanding of the unified nature of statistical inference.\nApply estimation and testing methods to analyze single variables or the relationship between two variables in order to understand natural phenomena and make data-based\ndecisions.\nModel numerical response variables using a single or multiple explanatory variables.\nInterpret results correctly, effectively, and in context without relying on statistical jargon.\nCritique data-based claims and evaluate data-based decisions.\nComplete a research project demonstrating mastery of statistical data analysis from exploratory analysis to inference to modeling."
  },
  {
    "objectID": "repo-structure/presentation/presentation.html",
    "href": "repo-structure/presentation/presentation.html",
    "title": "Presentation title",
    "section": "",
    "text": "Our project will demonstrate that the praise package is by far the best R package."
  },
  {
    "objectID": "repo-structure/proposal/proposal.html",
    "href": "repo-structure/proposal/proposal.html",
    "title": "Project proposal",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)"
  },
  {
    "objectID": "repo-structure/proposal/proposal.html#introduction",
    "href": "repo-structure/proposal/proposal.html#introduction",
    "title": "Project proposal",
    "section": "1. Introduction",
    "text": "1. Introduction"
  },
  {
    "objectID": "repo-structure/proposal/proposal.html#data",
    "href": "repo-structure/proposal/proposal.html#data",
    "title": "Project proposal",
    "section": "2. Data",
    "text": "2. Data"
  },
  {
    "objectID": "repo-structure/proposal/proposal.html#data-analysis-plan",
    "href": "repo-structure/proposal/proposal.html#data-analysis-plan",
    "title": "Project proposal",
    "section": "3. Data analysis plan",
    "text": "3. Data analysis plan"
  },
  {
    "objectID": "project-description.html#data",
    "href": "project-description.html#data",
    "title": "Showcase your inner data scientist",
    "section": "Data",
    "text": "Data\nIn order for you to have the greatest chance of success with this project it is important that you choose a manageable dataset. This means that the data should be readily accessible and large enough that multiple relationships can be explored. As such, your dataset must have at least 50 observations and between 10 to 20 variables (exceptions can be made but you must speak with me first). The dataset’s variables should include categorical variables, discrete numerical variables, and continuous numerical variables.\nIf you are using a dataset that comes in a format that we haven’t encountered in class, make sure that you are able to load it into R as this can be tricky depending on the source. If you are having trouble ask for help before it is too late.\nNote on reusing datasets from class: Do not reuse datasets used in examples, homework assignments, or labs in the class.\nBelow are a list of data repositories that might be of interest to browse. You’re not limited to these resources, and in fact you’re encouraged to venture beyond them. But you might find something interesting there:\n\nTidyTuesday\nNHS Scotland Open Data\nEdinburgh Open Data\nOpen access to Scotland’s official statistics\nBikeshare data portal\nUK Gov Data\nKaggle datasets\nOpenIntro datasets\nAwesome public datasets\nYouth Risk Behavior Surveillance System (YRBSS)\nPRISM Data Archive Project\nHarvard Dataverse\nIf you know of others, let me know, and we’ll add here…"
  },
  {
    "objectID": "project-description.html#deliverables",
    "href": "project-description.html#deliverables",
    "title": "Showcase your inner data scientist",
    "section": "Deliverables",
    "text": "Deliverables\n\nProposal - due 26 October\nPresentation - due 5 & 7 Dec\nExecutive summary - due 9 Dec\n\n\nProposal\nThis is a draft of the introduction section of your project as well as a data analysis plan and your dataset.\n\nSection 1 - Introduction: The introduction should introduce your general research question and your data (where it came from, how it was collected, what are the cases, what are the variables, etc.).\nSection 2 - Data: Place your data in the `/data` folder, and add dimensions and codebook to the README in that folder. Then print out the output of and codebook to the README in that folder. Then print out the output of glimpse() or skim() of your data frame. Add the README for the data, the codebook, and the output of glimpse() or skim() to the end of your proposal.\nSection 3 - Data analysis plan:\n\nAny outcomes (response, Y) and predictor (explanatory, X) variables you will use to answer your question.\nThe comparison groups you will use, if applicable.\nVery preliminary exploratory data analysis, including some summary statistics and visualizations, along with some explanation on how they help you learn more about your data. (You can add to these later as you work on your project.)\nThe method(s) that you believe will be useful in answering your question(s). (You can update these later as you work on your project.)\nWhat results from these specific statistical methods are needed to support your hypothesized answer?\n\n\nEach section should be no more than 1 page (excluding figures). You can check a print preview to confirm length.\nThe grading scheme for the project proposal is as follows. Note that after you receive feedback for your proposal you can improve it based on the feedback and re-submit it. If you re-submit, your final score for the proposal will be the average of two scores you receive (first and second submission).\n\n\n\nTotal\n10 pts\n\n\n\n\nData\n3 pts\n\n\nProposal\n5 pts\n\n\nWorkflow, organization, code quality\n1 pt\n\n\nTeamwork\n1 pt\n\n\n\n\n\nPresentation\n5 minutes maximum, and each team member should say something substantial. You can either present live during your workshop or pre-record and submit your video to be played during the workshop.\nPrepare a slide deck using the template I will give to you. This template uses quarto, and allows you to make presentation slides using R Markdown syntax. There isn’t a limit to how many slides you can use, just a time limit (5 minutes total). Each team member should get a chance to speak during the presentation. Your presentation should not just be an account of everything you tried (“then we did this, then we did this, etc.”), instead it should convey what choices you made, and why, and what you found.\nBefore you finalize your presentation, make sure your chunks are turned off with echo = FALSE.\nPresentations will take place on 5 and 7 December. You can choose to do your presentation live or pre-record it. During your workshop you will watch presentations from other teams in your workshop and provide feedback in the form of peer evaluations. The presentation line-up will be generated randomly.\nThe grading scheme for the presentation is as follows:\n\n\n\n\n\n\n\nTotal\n50 pts\n\n\n\n\nTime management: Did the team divide the time well amongst themselves or got cut off going over time?\n4 pts\n\n\nContent: Is the research question well designed and is the data being used relevant to the research question?\n5 pts\n\n\nProfessionalism: How well did the team present? Does the presentation appear to be well practiced? Did everyone get a chance to say something meaningful about the project?\n5 pts\n\n\nTeamwork: Did the team present a unified story, or did it seem like independent pieces of work patched together?\n6 pts\n\n\nContent: Did the team use appropriate statistical procedures and interpretations of results accurately?\n10 pts\n\n\nCreativity and Critical Thought: Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n10 pts\n\n\nSlides: Are the slides well organized, readable, not full of text, featuring figures with legible labels, legends, etc.?\n10 pts\n\n\n\n\n\nExecutive summary\nAlong with your presentation slides, we want you to provide a brief summary of your project.\nThis executive summary should provide information on the dataset you’re using, your research question(s), your methodology, and your findings.\nThe executive summary is worth 15 points and will be evaluated based on whether it follows guidance and whether it’s concise but detailed enough.\n\n\nProject organization\nThe following folders and files in your project. Please download the template here:\n\npresentation.qmd + presentation.html: Your presentation slides\nREADME.Rmd + README.md: Your write-up\n/data: Your dataset in CSV or RDS format and your data dictionary\n/proposal: Your project proposal\n\nStyle and format does count for this assignment, so please take the time to make sure everything looks good and your data and code are properly formatted."
  },
  {
    "objectID": "project-description.html#tips",
    "href": "project-description.html#tips",
    "title": "Showcase your inner data scientist",
    "section": "Tips",
    "text": "Tips\n\nYou’re working in the same google doc as your teammates now, so make sure that the coding is in the correct order.\nReview the marking guidelines below and ask questions if any of the expectations are unclear.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution.\nSet aside time to work together and apart (physically).\nWhen you’re done, review the documents to make sure you’re happy with the final state of your work. Then go get some rest!\nCode: In your presentation your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your quarto file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcomed to show that portion.\n\nTeamwork: You are to complete the assignment as a team. All team members are expected to contribute equally to the completion of this assignment and team evaluations will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-description.html#marking",
    "href": "project-description.html#marking",
    "title": "Showcase your inner data scientist",
    "section": "Marking",
    "text": "Marking\n\n\n\nTotal\n100 pts\n\n\n\n\nProposal\n10 pts\n\n\nPresentation\n50 pts\n\n\nExecutive summary\n15 pts\n\n\nReproducibility and organization\n10 pts\n\n\nTeam peer evaluation\n10 pts\n\n\nClassmates’ evaluation\n5 pts\n\n\n\n\nCriteria\nYour project will be assessed on the following criteria:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100% - Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89% - Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79% - Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69% - Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60% - Student is not making a sufficient effort.\n\n\n\nTeam peer evaluation\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member out of 10 points. You will additionally report a contribution percentage for each team member. Filling out the survey is a prerequisite for getting credit on the team member evaluation.If you are suggesting that an individual did less than 20% of the work, please provide some explanation. If any individual gets an average peer score indicating that they did less than 10% of the work, this person will receive half the grade of the rest of the group.\n\n\nLate work policy\n\nThere is no late submission / make up for the presentation. You must be in class on the day of the presentation to get credit for it or pre-record and submit your presentation by 9am in the morning of the presentations.\nThe late work policy for the write-up is 5% of the maximum obtainable mark per calendar day up to seven calendar days after the deadline. If you intend to submit work late for the project, you must notify the course organizer before the original deadline as well as soon as the completed work is submitted."
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures",
    "href": "course-support.html#lectures",
    "title": "Course support",
    "section": "Lectures",
    "text": "Lectures\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone’s office hours here."
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that can’t wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Ed discussion as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Ed discussion), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email dr. gwynn sturdevant at gwynnc@wharton.upenn.edu.\nIf there is a question that’s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include “STAT 1010” and only “STAT 1010” in the subject line. You can also contact me privately on Canvas. Barring extenuating circumstances, I will respond to STAT 1010 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Weingarten learning center. The Weingarten Center offers a variety of resources to support all Penn students in reaching their academic goals. They offer multiple workshops throughout the semester and students can request workshops. All services are free and confidential. To contact the Weingarten Center, call 215-573-9235. The office is located in Stouffer Commons at 3702 Spruce Street, Suite 300.\nLearning Consultations offers individual consultations and group workshops that support students in developing more efficient and effective study skills and learning strategies. Learning specialists work with students to address time and project management, academic reading and writing, note-taking, problem-solving, exam preparation, test-taking, self-regulation, and flexibility.\n\nTutoring offers free access to on-campus tutors for many Penn courses in both drop-in and weekly contract format. Tutoring may be individual or in small groups. Tutors will assist with applying course information, understanding key concepts, and developing course-specific strategies. Tutoring support is available throughout the term but is best accessed early in the semester. First-time users must meet with a staff member; returning users may submit their requests online.\n\nAdditionally, Marks Family Writing Center provides expert help in writing for undergraduate and graduate students. Communication Within the Curriculum helps students express themselves orally with clarity and confidence. Language Direct provides tutoring for foreign languages. Van\nPelt Library supports students in research and instructional technologies through a range of workshops and consultations."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nCounseling and Psychological Services (CAPS) provides professional psychological and psychiatric services to students who need support in fulfilling their academic, social, and personal objectives. CAPS directly supports student mental health through counseling, crisis management, consultation, education and outreach, and training. CAPS has a satellite office in Huntsman Hall, and students with urgent concerns can talk with a CAPS clinician 24/7 at 215.898.7021 (press 1) or visit CAPS' main office at 3624 Market Street during business hours.\n\nStudent Health Service provides students with accessible, cost-effective, culturally-sensitive, and student-focused healthcare, including care for acute and chronic health problems, preventive health services, and health and wellness education.\n\nAlcohol and Other Drug Program Initiatives (AOD) works to reduce harm related to alcohol and other drug use at Penn.\n\nPenn Violence Prevention (PVP) engages the Penn community in the prevention of sexual violence, relationship violence, stalking, and sexual harassment on campus. PVP provides education and outreach and the staff serves as confidential resources for students.\n\nPublic Safety: From riding your bike on campus, to preventing unattended theft, the Division of Public Safety wants to make sure you have the information you need to protect your safety and your belongings.\n\nPenn Recreation: Penn offers many opportunities for students to participate in competitive team sports and stay physically fit at state-of-the-art, world-class training centers.\n\nWharton Wellness is a division-sponsored student organization that works to implement initiatives targeted at specific wellness issues in the Wharton community by creating experiences, fostering a positive culture of well-being, and connecting clubs/students to wellness resources.\n\nIt is important to me that you have the resources you need to be able to focus on learning in this course – this includes both the necessary academic materials as well as taking care of your day-to-day needs. Students experiencing difficulty affording the course materials should reach out to the Penn First Plus office (pennfirstplus@upenn.edu). Students who are struggling to afford sufficient food to eat every day and/or lack a safe and suitable space to live should contact Student Intervention Services (vpul-sisteam@pobox.upenn.edu). Students may also wish to contact their Financial Aid Counselor or Academic Advisor about these concerns. You are welcome to notify me if any of these challenges are affecting your success in this course, as long as you are comfortable doing so – I may have resources to support you."
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited. Additional support is also available through Student Intervention Services."
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-canvas",
    "href": "course-support.html#assistance-with-zoom-or-canvas",
    "title": "Course support",
    "section": "Assistance with Zoom or Canvas",
    "text": "Assistance with Zoom or Canvas\nFor technical help with Canvas, Gradescope, or Zoom, contact Wharton Student Computing. You can also access the self-service help documentation for Zoom here, Canvas here, and for Gradescope here.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Open your Stat1010 project then go to file and open a quarto document. You can add code chunks by clicking on the green button in the IDE and answer some questions directly in the document, and add further code chunks. Hit render and if there are no errors in your code, the pdf should pop up. You can then upload to either Canvas or Gradescope"
  },
  {
    "objectID": "weeks/week-02.html",
    "href": "weeks/week-02.html",
    "title": "Week 2",
    "section": "",
    "text": "📖 Read the syllabus\n📖 Read the support resources"
  },
  {
    "objectID": "weeks/week-02.html#participate",
    "href": "weeks/week-02.html#participate",
    "title": "Week 2",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 2"
  },
  {
    "objectID": "weeks/week-02.html#practice",
    "href": "weeks/week-02.html#practice",
    "title": "Week 2",
    "section": "Practice",
    "text": "Practice\n📋 diamonds"
  },
  {
    "objectID": "weeks/week-02.html#perform",
    "href": "weeks/week-02.html#perform",
    "title": "Week 2",
    "section": "Perform",
    "text": "Perform\n⌨️ Diagnostic test\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-08.html",
    "href": "weeks/week-08.html",
    "title": "Week 8",
    "section": "",
    "text": "📖 Read Chpts 11 & 12 from Stine and Foster\n📖 Read FGLI: Chpts 11 & 12 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-08.html#participate",
    "href": "weeks/week-08.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 10\n🖥️ Lecture 11"
  },
  {
    "objectID": "weeks/week-08.html#perform",
    "href": "weeks/week-08.html#perform",
    "title": "Week 8",
    "section": "Perform",
    "text": "Perform\n✍️ HW 2 - Titanic\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "📖 Read Chpts 19 from Stine and Foster\n📖 Read FGLI: Chpts 19 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-13.html#participate",
    "href": "weeks/week-13.html#participate",
    "title": "Week 13",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 18\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-07.html",
    "href": "weeks/week-07.html",
    "title": "Week 7",
    "section": "",
    "text": "📖 Read Chpts 6 & 10 from Stine and Foster\n📖 Read FGLI: Chpts 6 & 10 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-07.html#participate",
    "href": "weeks/week-07.html#participate",
    "title": "Week 7",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 8\n🖥️ Lecture 9"
  },
  {
    "objectID": "weeks/week-07.html#practice",
    "href": "weeks/week-07.html#practice",
    "title": "Week 7",
    "section": "Practice",
    "text": "Practice\n📋 AE 9"
  },
  {
    "objectID": "weeks/week-07.html#perform",
    "href": "weeks/week-07.html#perform",
    "title": "Week 7",
    "section": "Perform",
    "text": "Perform\n✍️ HW 2 - Titanic\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-15.html",
    "href": "weeks/week-15.html",
    "title": "Week 14",
    "section": "",
    "text": "📖 Read Chpts 19 from Stine and Foster\n📖 Read FGLI: Chpts 19 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-15.html#participate",
    "href": "weeks/week-15.html#participate",
    "title": "Week 14",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 18\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-05.html",
    "href": "weeks/week-05.html",
    "title": "Week 5",
    "section": "",
    "text": "📖 Read Solutions to AEs\n📖 Read Solutions to HW-1"
  },
  {
    "objectID": "weeks/week-05.html#practice",
    "href": "weeks/week-05.html#practice",
    "title": "Week 5",
    "section": "Practice",
    "text": "Practice\n📋 Revision 1 for exam 1\n📋 Revision 2 for exam 1"
  },
  {
    "objectID": "weeks/week-05.html#perform",
    "href": "weeks/week-05.html#perform",
    "title": "Week 5",
    "section": "Perform",
    "text": "Perform\n⌨️ Course evaluation\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-16.html",
    "href": "weeks/week-16.html",
    "title": "Week 14",
    "section": "",
    "text": "📖 Read Chpts 19 from Stine and Foster\n📖 Read FGLI: Chpts 19 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-16.html#participate",
    "href": "weeks/week-16.html#participate",
    "title": "Week 14",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 18\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-14.html",
    "href": "weeks/week-14.html",
    "title": "Week 14",
    "section": "",
    "text": "🖥️ Lecture 18\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-09.html",
    "href": "weeks/week-09.html",
    "title": "Week 9",
    "section": "",
    "text": "📖 Read Chpts 13 & 14 from Stine and Foster\n📖 Read FGLI: Chpts 13 & 14 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-09.html#participate",
    "href": "weeks/week-09.html#participate",
    "title": "Week 9",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 12: Samples and survey\n🖥️ Lecture 13: Sampling variation and quality"
  },
  {
    "objectID": "weeks/week-09.html#perform",
    "href": "weeks/week-09.html#perform",
    "title": "Week 9",
    "section": "Perform",
    "text": "Perform\n✍️ HW 2 - Titanic\n📂 Proposal for project\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-06.html",
    "href": "weeks/week-06.html",
    "title": "Week 6",
    "section": "",
    "text": "📖 Read Chpt 9 from Stine and Foster\n📖 Read FGLI: Chpt 9 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-06.html#participate",
    "href": "weeks/week-06.html#participate",
    "title": "Week 6",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 7"
  },
  {
    "objectID": "weeks/week-06.html#practice",
    "href": "weeks/week-06.html#practice",
    "title": "Week 6",
    "section": "Practice",
    "text": "Practice\n📋 Random variables"
  },
  {
    "objectID": "weeks/week-06.html#perform",
    "href": "weeks/week-06.html#perform",
    "title": "Week 6",
    "section": "Perform",
    "text": "Perform\n✅ Exam 1\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-03.html",
    "href": "weeks/week-03.html",
    "title": "Week 3",
    "section": "",
    "text": "📖 Read chapter 4 of Introduction to Modern Statistics\n📖 Read chapter 5 of Introduction to Modern Statistics"
  },
  {
    "objectID": "weeks/week-03.html#participate",
    "href": "weeks/week-03.html#participate",
    "title": "Week 3",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 3\n🖥️ Lecture 4"
  },
  {
    "objectID": "weeks/week-03.html#practice",
    "href": "weeks/week-03.html#practice",
    "title": "Week 3",
    "section": "Practice",
    "text": "Practice\n📋 Exploring qualitative variables\n📋 Exploring numeric variables"
  },
  {
    "objectID": "weeks/week-03.html#perform",
    "href": "weeks/week-03.html#perform",
    "title": "Week 3",
    "section": "Perform",
    "text": "Perform\n⌨️ Diagnostic test - week 3\n✍️ HW 6 - Statistics Experience\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "📖 Read Chpts 17 & 18 from Stine and Foster\n📖 Read FGLI: Chpts 17 & 18 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-12.html#participate",
    "href": "weeks/week-12.html#participate",
    "title": "Week 12",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 16\n🖥️ Lecture 17"
  },
  {
    "objectID": "weeks/week-12.html#practice",
    "href": "weeks/week-12.html#practice",
    "title": "Week 12",
    "section": "Practice",
    "text": "Practice\n📋 AE - 11\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-01.html",
    "href": "weeks/week-01.html",
    "title": "Week 1",
    "section": "",
    "text": "📖 Read the syllabus\n📖 Read the support resources"
  },
  {
    "objectID": "weeks/week-01.html#participate",
    "href": "weeks/week-01.html#participate",
    "title": "Week 1",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 1 - Download R and plotting your first graph"
  },
  {
    "objectID": "weeks/week-01.html#practice",
    "href": "weeks/week-01.html#practice",
    "title": "Week 1",
    "section": "Practice",
    "text": "Practice\n📋 diamonds"
  },
  {
    "objectID": "weeks/week-01.html#perform",
    "href": "weeks/week-01.html#perform",
    "title": "Week 1",
    "section": "Perform",
    "text": "Perform\n⌨️ Diagnostic test\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "🖥️ Happy Halloween"
  },
  {
    "objectID": "weeks/week-10.html#practice",
    "href": "weeks/week-10.html#practice",
    "title": "Week 10",
    "section": "Practice",
    "text": "Practice\n📋 Revision for exam 2"
  },
  {
    "objectID": "weeks/week-10.html#perform",
    "href": "weeks/week-10.html#perform",
    "title": "Week 10",
    "section": "Perform",
    "text": "Perform\n✅ Exam 2\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "📖 Read Chpts 15 & 16 from Stine and Foster\n📖 Read FGLI: Chpts 15 & 16 from Stine and Foster"
  },
  {
    "objectID": "weeks/week-11.html#participate",
    "href": "weeks/week-11.html#participate",
    "title": "Week 11",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 14\n🖥️ Lecture 15"
  },
  {
    "objectID": "weeks/week-11.html#practice",
    "href": "weeks/week-11.html#practice",
    "title": "Week 11",
    "section": "Practice",
    "text": "Practice\n📋 AE - 10\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-04.html",
    "href": "weeks/week-04.html",
    "title": "Week 4",
    "section": "",
    "text": "📖 Read chapter 14 of Introduction to Data Science\n📖 Read chapter 2 of Probability, Statistics, and Data: A fresh approach using R"
  },
  {
    "objectID": "weeks/week-04.html#participate",
    "href": "weeks/week-04.html#participate",
    "title": "Week 4",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 5\n🖥️ Lecture 6"
  },
  {
    "objectID": "weeks/week-04.html#practice",
    "href": "weeks/week-04.html#practice",
    "title": "Week 4",
    "section": "Practice",
    "text": "Practice\n📋 Exploring probabilities\n📋 The birthday simulation"
  },
  {
    "objectID": "weeks/week-04.html#perform",
    "href": "weeks/week-04.html#perform",
    "title": "Week 4",
    "section": "Perform",
    "text": "Perform\n⌨️ Diagnostic test - week 3\n✍️ HW 1 - Titanic\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "Discussion forum\n🔗 on Ed discussion\n\n\n\n\nLecture streaming and recordings (email for permission)\n🔗 on Canvas\n\n\nSubmit AE\n🔗 on Canvas\n\n\nSubmit HWK\n🔗 on Gradescope\n\n\nGradebook\n🔗 on Canvas"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download a PDF copy of the syllabus."
  },
  {
    "objectID": "course-syllabus.html#course-info",
    "href": "course-syllabus.html#course-info",
    "title": "Syllabus",
    "section": "Course info",
    "text": "Course info\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nSection 1\nMon & Wed\n8:30 am - 10:00 am\nSHDH 1206\n\n\nSection 2\nMon & Wed\n10:15 am - 11:45 am\nSHDH 1206\n\n\nSection 3\nMon & Wed\n1:45 pm - 3:15 pm\nSHDH 1206"
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the semester, you will be able to…\n\nunderstand data manipulation, and find basic summaries\nreason under uncertainity\nmake predictions\nunderstand probability\ncommunicate results from statistical analyses to a general audience."
  },
  {
    "objectID": "course-syllabus.html#community",
    "href": "course-syllabus.html#community",
    "title": "Syllabus",
    "section": "Community",
    "text": "Community\n\nWharton’s Code of Conduct\nAs a student in this course, you have agreed to uphold the Wharton’s Student Code of Conduct as well as the practices specific to this course.\n\n\nInclusive community\nMy goal as your lecturer is to help you accomplish or discover your passion and find your spark. You all belong here. It is also my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Diversity, Inclusion and Belonging at the Wharton School. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups. Please feel free to leave anonymous comments in my mailbox on the 4th floor of the Academic Research Building.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean is an excellent resource.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Weingarten Center is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the the Weingarten Center to request or update accommodations under these circumstances; it can take up to 4 weeks to review documents and get approval.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website at stats1010-f22.github.io/website.\nI will regularly send course announcements via email and canvas, make sure to check one or the other of these regularly. If an announcement is sent Monday through Thursday, I will assume that you have read the announcement by the next day. If an announcement is sent on a Friday or over the weekend, I will assume that you have read it by Monday.\n\n\nWhere to get help\n\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. Please use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the Ed discussion forum. There is a chance another student has already asked a similar question, so please check the other posts in Conversations before adding a new question. If you know the answer to a question posted in the discussion forum, I encourage you to respond!\nEmails should be reserved for questions not appropriate for the public forum. If you email me, please include “STAT 1010” in the subject line and ONLY STAT 1010. Barring extenuating circumstances, I will respond to STAT 1010 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.\n\nCheck out the Support page for more resources."
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nWhile there is no official textbook for the course, we will be assigning readings from the following textbooks.\n\nR for Data Science by Garret Grolemund and Hadley Wickham\nIntroduction to Modern Statistics by Mine Çetinkaya-Rundel and Johanna Hardin\nStatistics for Business by Robert Stine and Dean Foster"
  },
  {
    "objectID": "course-syllabus.html#lectures",
    "href": "course-syllabus.html#lectures",
    "title": "Syllabus",
    "section": "Lectures",
    "text": "Lectures\nThe goal of the lectures is for them to be as interactive as possible. My role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. You are expected to attend all lecture and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature application exercises that will be graded. In addition to application exercises will be periodic activities to help build a learning community. These will be short, fun activities that will help everyone in the class connect throughout the semester.\nYou are expected to bring a laptop to each class so that you can take part in the in-class exercises. Please make sure your laptop is fully charged before you come to class as the number of outlets in the classroom will not be sufficient to accommodate everyone. If you are in need of a loaner laptop please seek support here."
  },
  {
    "objectID": "course-syllabus.html#teams",
    "href": "course-syllabus.html#teams",
    "title": "Syllabus",
    "section": "Teams",
    "text": "Teams\nYou will be assigned to a team at the beginning of the semester. You are encouraged to sit with your teammates in lecture. All team members are expected to contribute equally to the completion of the labs and project and you will be asked to evaluate your team members throughout the semester. Failure to adequately contribute to an assignment will result in a penalty to your mark relative to the team’s overall mark."
  },
  {
    "objectID": "course-syllabus.html#assessment",
    "href": "course-syllabus.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\nAssessment for the course is comprised of five components: application exercises, homework assignments, exams, projects, and teamwork.\n\nApplication exercises\nParts of some lectures will be dedicated to working on Application Exercises (AEs). These exercises give you an opportunity to apply the statistical concepts and code introduced in the readings and lectures. Due dates for AEs will be announced as they are assigned.\nBecause these AEs are for practice, they will be graded based on completion, i.e., a good-faith effort has been made in attempting all parts. Successful on-time completion of at least 80% of AEs will result in full credit for AEs in the final course grade.\n\n\nHomework\nIn homework, you will apply what you’ve learned during lecture and lab to complete data analysis tasks. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. Similar to lab assignments, homework must be typed up using Quarto and submitted as a PDF in Canvas.\nOne homework assignment will be dedicated to a statistics experience. The statistics experience is an opportunity to engage with statistics and data science outside of the classroom through podcasts, books, seminars, data analysis competitions, and other activities. As you complete these experiences, the goal is to consider how the material you’re learning in the course connects with society more broadly.\nThe lowest homework grade will be dropped at the end of the semester.\n\n\nExams\nThere will be four exams consisting of multiple choice questions. Through these exams you have the opportunity to demonstrate what you’ve learned in the course thus far. The exams will focus on the conceptual understanding of the content. The content of the exam will be related to the content in the prepare, practice, and perform assignments. More detail about the exams will be given during the semester.\nThe lowest exam grade will be dropped at the end of the semester.\n\n\nProject\nThe purpose of the project is to apply what you’ve learned throughout the semester to analyze an interesting, data-driven research question. The project will be completed with your teams, and each team will present their work. More information about the project will be provided during the semester."
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nApplication exercises\n7%\n\n\nHomework\n35% (7 x 5%)\n\n\nProject\n15%\n\n\nExam 01\n10%\n\n\nExam 02\n10%\n\n\nExam 03\n10%\n\n\nTeamwork\n13%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n>= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n< 60"
  },
  {
    "objectID": "course-syllabus.html#five-tips-for-success",
    "href": "course-syllabus.html#five-tips-for-success",
    "title": "Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success on this course depends very much on you and the effort you put into it. The course has been organized so that the burden of learning is on you. Your TA and I will help you be providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask the TA, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask us why. If you’re not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings.\nDo the homework and lab.The earlier you start, the better. It’s not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example.)\nDon’t procrastinate. If something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, and eventually you won’t know where to begin asking questions. Don’t let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours, and let me help you identify a good (re)starting point."
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic integrity\nTL;DR: Don’t cheat!\nAll students must adhere to Wharton’s Code of Academic Integrity: Wharton is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nRegardless of course delivery format, it is your responsibility to understand and follow Wharton policies regarding academic integrity, including doing one’s own work, following proper citation of sources, and adhering to guidance around group work projects. Ignoring these requirements is a violation of the Wharton Community Standard. If you have any questions about how to follow these requirements, please contact Julie Nettleton, Director of the Center for Community Standards and Accountability (CSA).\n\n\nCollaboration policy\nOnly work that is clearly assigned as team work should be completed collaboratively.\n\nThe homework assignments must be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what’s the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to homework questions (including any code) with anyone other than myself and the teaching assistants.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\n\n\n\nPolicy on sharing and reusing code\nI am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course’s policy is that you may make use of any online resources (e.g. RStudio Community, StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. On individual assignments you may not directly share code with another student in this class, and on team assignments you may not directly share code with another team in this class.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework and lab assignment will be dropped to accommodate such circumstances.\n\nHomework and labs may be submitted up to 3 days late. There will be a 5% deduction for each 24-hour period the assignment is late.\nThere is no late work accepted for application exercises, since these are designed to help you prepare for labs and homework.\nThe late work policy for the project will be provided with the project instructions.\n\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a lab or homework assignment by the stated due date, you may email dr. sturdevant before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used for once in the semester, so only use it for a truly extenuating circumstance.\nIf there are circumstances that are having a longer-term impact on your academic performance, please let your academic dean know, as they can be a resource. Please let dr. sturdevant know if you need help contacting your academic dean.\n\n\nRegrade request policy\nRegrade requests must be submitted on Gradescope within a week of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the final project presentations.\n\n\nAttendance policy\nResponsibility for class attendance rests with individual students. Since regular and punctual class attendance is expected, students must accept the consequences of failure to attend. More details on Wharton attendance policies are available here.\nHowever, there may be many reasons why you cannot be in class on a given day, particularly with possible extra personal and academic stress and health concerns this semester. All course lectures will be recorded and available to enrolled students after class. If you miss a lecture, make sure to watch the recording and review the material before the next class session. Given the technologies we use in the course, this is straightforward to do asynchronously. If you know you’re going to miss a class and you’re feeling well enough to do so, notify your teammates ahead of time. Additionally, please fill out a Course Absence Notice (CAN) so that I am notified in case of an attendance quiz. Overall these policies are put in place to ensure communication between team members, respect for each others’ time, and also to give you a safety net in the case of illness or other reasons that keep you away from attending class.\n\n\nAttendance policy related to COVID symptoms, exposure, or infection\nStudent health, safety, and well-being are the university’s top priorities. To help ensure your well-being and the well-being of those around you, please do not come to class if you have symptoms related to COVID-19, have had a known exposure to COVID-19, or have tested positive for COVID-19. If any of these situations apply to you, you must follow university guidance related to the ongoing COVID-19 pandemic and current health and safety protocols. If you are experiencing any COVID-19 symptoms, contact student health at 215-898-0300. To keep the university community as safe and healthy as possible, you will be expected to follow these guidelines. Please reach out to me and your academic dean as soon as possible if you need to quarantine or isolate so that we can discuss arrangements for your continued participation in class. For the safety of all students, I require that students wear masks in class and prefer N95 or KN95 masks.\n\n\nInclement weather policy\nIn the event of inclement weather or other connectivity-related events that prohibit class attendance, I will notify you how we will make up missed course content and work. This might entail holding the class on Zoom synchronously or watching a recording of the class.\n\n\nPolicy on video recording course content\nLectures will be recorded and available on Canvas, so students should not need to create their own recordings of lectures. If you feel that you need record the lectures yourself, you must get written permission from me ahead of time and these recordings should be used for personal study only, not for distribution. The full policy on recording of lectures falls under the University of Pennsylvania’s V.L. Policy on Unauthorized Copying of Copyrighted Media, available at https://catalog.upenn.edu/faculty-handbook/v/v-l/. Unauthorized distribution may result in a civil suite, criminal charges, and/or penalties and fines."
  },
  {
    "objectID": "course-syllabus.html#learning-during-a-pandemic",
    "href": "course-syllabus.html#learning-during-a-pandemic",
    "title": "Syllabus",
    "section": "Learning during a pandemic",
    "text": "Learning during a pandemic\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don’t hesitate to ask.\n\nYou never owe me personal information about your health (mental or physical) but you’re always welcome to talk to me. If I can’t help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded during this crisis.\n\nNote: If you’ve read this far in the syllabus, send me an email with a picture of your pet if you have one or your favorite memes with STAT1010 in the subject line!"
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nAugust 30: First day of classes\nSeptember 5: Labor Day holiday, no classes are held\nSeptember 13: Course Selection Period ends\nOctober 6 - 9: Fall term break, no classes are held\nOctober 10: Drop period ends\nOctober 10: Indigenous People’s Day (classes in session)\nOctober 28: Grade Type Change Deadline\nNovember 7: Last day to withdraw from a course\nNovember 22-23: Thur-Fri class schedule on Tue-Wed\nNovember 24-27: Thanksgiving Break\nDecember 12: Last day of classes\nDecember 13-14: Reading Days\nDecember 15 - 22: Final exams\nDecember 22: Fall term ends\n\nClick here for the full University of Pennsylvania academic calendar."
  },
  {
    "objectID": "supplemental/exam-1-studyguide.html",
    "href": "supplemental/exam-1-studyguide.html",
    "title": "Study list for exam 1",
    "section": "",
    "text": "The R functions we have discussed so far these few weeks:\n\n\n\nfunctions\noperators\npipes\n\n\nfilter()\n%/%\n%>%\n\n\ndistinct()\n%%\n“+”\n\n\ncount()\n!=\n\n\n\nselect()\nrowSums()\n\n\n\nmutate()\ncolSums()\n\n\n\nView()\n\n\n\n\nglimpse()\n\n\n\n\npivot_wider()\n\n\n\n\ngroup_by()\n\n\n\n\nchisq.test()\n\n\n\n\nggplot()\n\n\n\n\ngeom_hist()\n\n\n\n\ngeom_bar()\n\n\n\n\ngeom_point()\n\n\n\n\nlabs()\n\n\n\n\nfacet_wrap()\n\n\n\n\n\nBenefits and downsides of these visualization methods: histogram and boxplot.\nConditional probability tables - which variable is it conditioned on.\nMean, median, mode and which is best in skewed distributions. Identify in a plot\nMeasures of spread: standard deviation, variance, quartiles, percentiles, interquartile range.\nChi-squared expected counts given a contingency table of counts.\nIndependent events\nDisjoint events\nVenn diagrams\nCompliment\nThe three laws of probability - Kolomogorov’s axioms\nClassification of variables"
  },
  {
    "objectID": "supplemental/poisson.html",
    "href": "supplemental/poisson.html",
    "title": "Limits of the binomial distribution is the poisson",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. James Tanton. They are provided for students who want to dive deeper into the mathematics behind the Poisson distribution. Additional supplemental notes will be added throughout the semester.\nI don’t know statistics! I’ve only ever taken one—very basic—introductory course on the subject from which I feel I learned very little. The rest of my understanding of the subject has been from personal reading and from teaching my own course on the subject! But I’ve secretly been longing for understanding the history and theoretical underpinnings of the topic for a very long time: Why is the central limit theorem true – mathematically and intuitively? How does one actually figure out the chi-squared distribution? Why, really, do you divide by \\(n-1\\) in the formula for standard deviation? And so on. (I figured out one answer to that last question here).\nI was reminded of my lack of knowledge of statistics last week at a university reception. An astronomer was chatting with his graduate student and said “Everything is the Poisson distribution” to then look at me, the mathematician, to say “Right?”\nIt might be shocking to learn that I don’t know what the Poisson distribution is! Well, I didn’t at that moment.\nI have a maxim: It is okay not to know. But it is not okay not to want to find out.\nSo I decided this month to find out about the Poisson distribution and think about its mathematical meaning.In looking up the formula one retrieves:\n\\[P(X=k)=\\frac{\\lambda^k}{k!} e^{-\\lambda}\\] for \\(k\\in\\{0,1, 2,3,\\ldots\\}.\\)\nThis reads: Some random variable \\(X\\) can take on non-negative integer values and the chances that you’ll actually see it adopt a particular value \\(k\\) is given by a crazy formula that depends on some constant parameter \\(\\lambda\\).\nGot it? Is it now clear that everything is Poisson?\nStatisticians and mathematicians do not come up with formulas out of the air. What is the story behind this bizarreness?"
  },
  {
    "objectID": "supplemental/poisson.html#perhaps-everything-is-binomial",
    "href": "supplemental/poisson.html#perhaps-everything-is-binomial",
    "title": "Limits of the binomial distribution is the poisson",
    "section": "Perhaps Everything is Binomial?",
    "text": "Perhaps Everything is Binomial?\nIf I roll a die three times, what are the chances I’ll get exactly two sixes?\nMaybe I’ll first roll a non-six and then two sixes:\n\\[N \\ 6 \\ 6\\] The chances of seeing this are \\(\\frac{5}{6}\\times \\frac{1}{6}\\times\\frac{1}{6}\\). Or, following the implied notation, I could roll \\(6\\ N\\ 6\\) or \\(6\\ 6\\ N\\) with probabilities\n\\(\\frac{1}{6}\\times \\frac{5}{6}\\times\\frac{1}{6}\\) and \\(\\frac{1}{6}\\times \\frac{1}{6}\\times\\frac{5}{6}\\), respectively.\nSo we see that there are \\(3\\) ways I could see two sixes and one non-six in a roll of three, each with the same probability. The chances of me seeing exactly two sixes is thus\n\\[ 3\\left(\\frac{1}{6}\\right)^2\\left(\\frac{5}{6}\\right) \\]\nIn general, if in an experiment I have a probability \\(p\\) of seeing a success, and hence a probability \\(1-p\\) of seeing a failure, and I run the experiment \\(n\\) times in a row, the probability that I will see exactly \\(k\\) successes is \\[\\binom{n}{k}p^k(1-p)^{n-k}\\]\nHere \\(\\binom{n}{k}=\\frac{n!}{k!(n-k)!}\\)is “\\(n\\) choose \\(k\\) ,” the number of ways \\(k\\) successes can be arranged among a string \\(n\\) units long. For example, the chances of seeing exactly two sixes after rolling a die three times is \\[\\frac{3!}{2!1!}\\left(\\frac{1}{6}\\right)^2\\left(1-\\frac{1}{6}\\right)=3\\left(\\frac{1}{6}\\right)^2\\left(\\frac{5}{6}\\right) \\] as we saw.\nWe have here a binomial distribution. In general, for each success probability \\(p\\) and run length \\(n\\) we have the distribution \\[\\mathbb{P}(X=k) = \\binom{n}{k}p^k(1-p)^{n-k}\\] for \\(k\\in\\{0,1, 2,\\ldots,n \\}.\\) Here \\(X\\) is the random variable given as the number of times I see a success in running an experiment \\(n\\) times in a row.”\nIn the 1700s scholars became interested in questions about runs of events that seem to occur in an ongoing fashion and counting the number of occurrences of the event you might expect to see in any selected time period. French mathematicians Siméon Denis Poisson and Abraham de Moivre were the first to develop the mathematics for this.\nI’ve been sitting on my porch for many hours of my life, counting the number of cars that pass by each hour. After many hours of observation I can say that the flow of cars is more or less regular (there is no difference in traffic flow in the day versus the night) and that, on average, \\(12\\) cars pass my house each hour.\nI am about to go sit out on my porch for an hour. What is the probability I will see \\(15\\) cars?\nOne possible start to this question is to treat it as a run of an experiment, not over an hour but, instead, one experiment per minute for \\(60\\) minutes. If I know the chances \\(p\\) of seeing a car in a given minute, then the chances of seeing \\(15\\) cars over a course of \\(60\\) minutes would be \\[\\binom{60}{15} p^{15}(1-p)^{45}.\\] How might I estimate \\(p\\)?\nWell, I expect to see an average of \\(12\\) cars over the course an hour. So, on average, I expect to see one car in any five-minute period. So chances of me selecting a minute with the appearance of a car in it should be \\(p=\\frac{1}{5}\\) That’s it. We have a formula for the chances of seeing \\(15\\) cars over the course of an hour. \\[ \\binom{60}{15}0.2^{15}0.8^{45}\\approx 0.076.\\] There is a problem with our work here: we’ve assumed that cars are spaced apart so that I’ll never see two cars within the same minute. To obviate this objection, let’s work instead with the \\(3600\\) seconds in an hour: I’ve never seen two cars go by within the same second.\nNow, on average, over the \\(3600\\) seconds of an hour, \\(12\\) of them will have a car “in them,” and so the chances of me seeing a car in any particular second is \\(p=\\frac{12}{3600}\\)\nThis now gives \\[\\binom{3600}{15}\\left(\\frac{12}{3600}\\right)^{15} \\left(1-\\frac{12}{3600}\\right)^{3555}\\approx0.080\\] for the chances of me seeing exactly \\(15\\) cars.\nWell, actually, I realise now that I think I have seen two cars go by within the same second: two cars going in opposite directions passed each other in front of my house.\nSo maybe I should repeat this analysis not over every minute or every second, but instead over every nanosecond or every picosecond. I need a very short length of time I can be sure will never have two cars appearing in it.\nLet’s break the hour into \\(n\\), almost instantaneous, units of time. The chances of me seeing a car within any one of those units is \\(p=\\frac{12}{n}\\) and so the probability of me seeing \\(15\\) cars over a run of \\(n\\) of these units of time is \\[\\binom{n}{15}\\left(\\frac{12}{n}\\right)^{15}\\left(1-\\frac{12}{n}\\right)^{n-15}.\\] Ideally, we should compute this for larger and larger values of \\(n\\). That is, we should take the limit as \\(n\\) grows infinitely large."
  },
  {
    "objectID": "supplemental/poisson.html#taking-the-limit",
    "href": "supplemental/poisson.html#taking-the-limit",
    "title": "Limits of the binomial distribution is the poisson",
    "section": "Taking the Limit",
    "text": "Taking the Limit\nThe formula we have reads \\[\\frac{n(n-1)(n-14)}{15!}\\cdot\\frac{12^{15}}{n^{15}}\\cdot\\left(1-\\frac{12}{n}\\right)^n\\cdot\\frac{1}{\\left(1-\\frac{12}{n}\\right)^{15}}.\\] I can see how parts of this formula change as \\(n\\) grows larger and larger.\nFor instance \\[ \\frac{1}{\\left(1-\\frac{12}{n}\\right)^{15}}\\to\\frac{1}{(1-0)^{15}}=1\\] as \\(n\\) grows.\nAlso, \\[ \\frac{n(n-1)\\cdots(n-14)}{n^{15}}=\\left(1-\\frac{1}{n}\\right)\\left(1-\\frac{2}{n}\\right)\\cdots\\left(1-\\frac{14}{n}\\right)\\to1\\cdot\\cdots 1 = 1\\] as \\(n\\) grows.\nAnd the formula \\(\\left(1-\\frac{12}{n}\\right)^{n}\\) is reminiscent of the famous compound-interest formula \\[ \\lim_{n\\to\\infty}\\left(1+\\frac{1}{n}\\right)^n=e\\] or more generally, \\[ \\lim_{n\\to\\infty}\\left(1+\\frac{x}{n}\\right)^n=e^x\\] So we have that \\[ \\left(1-\\frac{12}{n}\\right)^n\\to e^{-12}\\] as \\(n\\) grows.\nSo we see that our formula, \\[\n\\left(\\begin{array}{c}\nn \\\\\n15\n\\end{array}\\right)\\left(\\frac{12}{n}\\right)^{15}\\left(1-\\frac{12}{n}\\right)^{n-15}\n\\] in the absolute ideal of looking at finer and finer intervals over the hour, becomes the formula \\[\n1 \\cdot \\frac{12^{15}}{15 !} \\cdot e^{-12} \\cdot 1=\\frac{12^{15}}{15 !} e^{-12}.\n\\] This has value \\(\\approx 0.072\\). There is about a \\(7\\)% chance I will see \\(15\\) cars over any given hour.\nMost important, this work has led us to the Poisson probability distribution formula!\nIf you have a phenomenon that seems to occur at a more-or-less steady rate, with the number occurrences during any set time length seeming to be more-or-less the same over all periods of that length, and no two events can occur at exactly the same instant, then, if you have good reason to believe that, for a given period length, on average \\(\\lambda\\) events occur, the probability that you will see \\(k\\) events during any particular time period of that length is \\[\\frac{\\lambda^k}{k !} e^{-\\lambda}.\\]\nThis is the Poisson distribution.\nI am guessing that many astronomical phenomena occur at more-or-less steady rates, on average—meteor strikes to the Earth, appearance of Sun spots—and so maybe close to everything in astronomy is indeed Poisson?\nAside: From \\(\\lim_{n\\to\\infty}\\left(1+\\frac{1}{n}\\right)^n=e\\) we see that \\[\n\\left(1+\\frac{x}{n}\\right)^n=\\left(\\left(1+\\frac{1}{n / x}\\right)^{n / x}\\right)^x \\rightarrow(e)^x\n\\]\nif \\(x\\) is positive (since \\(n/x\\) grows large and positive if \\(n\\) does). We also establish \\[\\lim _{n \\rightarrow \\infty}\\left(1-\\frac{1}{n}\\right)^n=e^{-1}\\]\nby observing that \\[\\begin{aligned}\\left(1-\\frac{1}{n}\\right)^n &=\\left(\\frac{n-1}{n}\\right)^n=\\frac{1}{\\left(\\frac{n}{n-1}\\right)^n} \\\\ &=\\frac{1}{\\left(1+\\frac{1}{n-1}\\right)^{n-1} \\cdot\\left(1+\\frac{1}{n-1}\\right)} \\\\ & \\rightarrow \\frac{1}{e \\cdot(1+0)}=\\frac{1}{e} \\end{aligned}\\]\nas \\(n\\) grows.\nFollowing similar algebra, we see that for negative \\(x\\), setting \\(y=-x\\), \\[ \\left(1+\\frac{x}{n}\\right)^n=\\left(1-\\frac{y}{n}\\right)^n=\\left(\\left(1-\\frac{1}{n/y}\\right)^{n/y}\\right)^y\\to e^{-y}=e^{x}.\\] Therefore \\[\n\\lim_{n\\to\\infty}\\left(1+\\frac{x}{n}\\right)^n=e^x\n\\] even if \\(x\\) is negative.\n© 2017 James Tanton tanton.math@gmail.com"
  },
  {
    "objectID": "supplemental/exam-3-studyguide.html",
    "href": "supplemental/exam-3-studyguide.html",
    "title": "Study list for exam 3",
    "section": "",
    "text": "The R functions we have discussed in these weeks:\n\n\n\nfunctions\n\n\nqnorm()\n\n\npnorm()"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html",
    "href": "supplemental/exam-2-studyguide.html",
    "title": "Study list for exam 2",
    "section": "",
    "text": "The R functions we have discussed in these weeks:"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#addition-rules",
    "href": "supplemental/exam-2-studyguide.html#addition-rules",
    "title": "Study list for exam 2",
    "section": "Addition rules",
    "text": "Addition rules\n-\\(E(X \\pm c) = E(X) \\pm c\\)\n-\\(SD(X \\pm c) = SD(X)\\)\n-\\(Var(X \\pm c) = Var(X)\\)"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#multiplication-rules",
    "href": "supplemental/exam-2-studyguide.html#multiplication-rules",
    "title": "Study list for exam 2",
    "section": "Multiplication rules",
    "text": "Multiplication rules\n-\\(E(cX) = cE(X)\\)\n-\\(SD(cX) = |c|SD(X)\\)\n-\\(Var(cX) = c^2Var(X)\\)\n-\\(E(cX \\pm a)=cE(X) \\pm a\\)\n- \\(Var(cX \\pm a)=c^2Var(X)\\)"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#which-variable-on-which-axis-and-what-are-the-names-for-each-variable",
    "href": "supplemental/exam-2-studyguide.html#which-variable-on-which-axis-and-what-are-the-names-for-each-variable",
    "title": "Study list for exam 2",
    "section": "Which variable on which axis and what are the names for each variable?",
    "text": "Which variable on which axis and what are the names for each variable?"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#describing-scatter-plots",
    "href": "supplemental/exam-2-studyguide.html#describing-scatter-plots",
    "title": "Study list for exam 2",
    "section": "Describing scatter plots",
    "text": "Describing scatter plots\n\nHomo and heteroskedastic"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#covariance-look-at-a-plot-and-know-if-positive-or-negative",
    "href": "supplemental/exam-2-studyguide.html#covariance-look-at-a-plot-and-know-if-positive-or-negative",
    "title": "Study list for exam 2",
    "section": "Covariance look at a plot and know if positive or negative",
    "text": "Covariance look at a plot and know if positive or negative\n\\(cov(x, y) = \\frac{(x_1 - \\bar{x})(y_1 - \\bar{y}) + (x_2 - \\bar{x})(y_2 - \\bar{y}) + \\ldots + (x_n - \\bar{x})(y_n - \\bar{y})}{n-1}\\)"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#correlation---look-at-a-plot-and-approximate",
    "href": "supplemental/exam-2-studyguide.html#correlation---look-at-a-plot-and-approximate",
    "title": "Study list for exam 2",
    "section": "Correlation - look at a plot and approximate",
    "text": "Correlation - look at a plot and approximate\n\\[corr(x, y) = \\frac{cov(x, y)}{s_x \\cdot s_y}\\]\n\nReferred to as \\(r\\)\nStrength of linear association\n\\(r\\) is always between \\(-1\\) and \\(+1\\), \\(-1 \\leq r \\leq 1\\).\n\\(r\\) does not have units"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#fitting-a-line",
    "href": "supplemental/exam-2-studyguide.html#fitting-a-line",
    "title": "Study list for exam 2",
    "section": "Fitting a line",
    "text": "Fitting a line\n\\(m = \\frac{r \\cdot s_y}{s_x}\\)\n\\(b = \\bar{y} - m \\bar{x}\\)"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#predicting",
    "href": "supplemental/exam-2-studyguide.html#predicting",
    "title": "Study list for exam 2",
    "section": "Predicting",
    "text": "Predicting\n\nlibrary(tidymodels)\n\n## Assign the least\n## squares line\nleast_squares_fit <- \n  linear_reg() %>% \n  set_engine(\"lm\") %>% \n  fit(price ~ carat, data = diamonds) \n## NOTE: outcome first, predictor second\n\n## Find the prediction\npredict(least_squares_fit, tibble(carat = c(1, 2, 2.5, 4)))\n\n# A tibble: 4 × 1\n   .pred\n   <dbl>\n1  5500.\n2 13256.\n3 17135.\n4 28769."
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#spurious-correlations",
    "href": "supplemental/exam-2-studyguide.html#spurious-correlations",
    "title": "Study list for exam 2",
    "section": "Spurious correlations",
    "text": "Spurious correlations\nbirth order associated with increased risk of downs syndrome, etc…"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#the-sharpe-ratio",
    "href": "supplemental/exam-2-studyguide.html#the-sharpe-ratio",
    "title": "Study list for exam 2",
    "section": "The Sharpe ratio",
    "text": "The Sharpe ratio\n\\(S(X) = \\frac{\\mu_X - r_f}{\\sigma_X}\\) - higher better - how to compute with a calculator from a pdf and also with just \\(\\mu\\) and \\(\\sigma\\) - computing in R"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#joint-pdfs",
    "href": "supplemental/exam-2-studyguide.html#joint-pdfs",
    "title": "Study list for exam 2",
    "section": "Joint pdfs",
    "text": "Joint pdfs\nFind probability given values Are they independent or not? What do increasing and decreasing joint pdfs look like?\n\\(E(X+Y) = E(X) + E(Y)\\) regardless of independence"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#rules-of-independence",
    "href": "supplemental/exam-2-studyguide.html#rules-of-independence",
    "title": "Study list for exam 2",
    "section": "3 Rules of independence",
    "text": "3 Rules of independence"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#covariance-of-rv",
    "href": "supplemental/exam-2-studyguide.html#covariance-of-rv",
    "title": "Study list for exam 2",
    "section": "Covariance of RV",
    "text": "Covariance of RV\n\\(Cov(X, Y) = E((X - \\mu_X)(Y - \\mu_Y))\\) \\(Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)\\) If \\(X\\),\\(Y\\) are independent, then \\(Cov(X, Y) = 0\\), opposite not true If \\(X\\), \\(Y\\) are independent, \\(Cov(X, Y) = 0\\), so \\(Var(X+ Y) = Var(X) + Var(Y)\\)"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#correlation",
    "href": "supplemental/exam-2-studyguide.html#correlation",
    "title": "Study list for exam 2",
    "section": "Correlation",
    "text": "Correlation\n\\(corr(x, y) = \\frac{cov(x, y)}{s_x \\cdot s_y}\\) \\[\\rho = Corr(X, Y) = \\frac{Cov(X, Y)}{\\sigma_X \\cdot \\sigma_Y}\\] ## IID variables \\(E(aX + bY + c) = aE(X) + bE(Y) + c\\) \\(Var(aX + bY + c) = a^2Var(X) + b^2Var(Y) + 2abCov(X, Y)\\)"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#bernoulli-trial",
    "href": "supplemental/exam-2-studyguide.html#bernoulli-trial",
    "title": "Study list for exam 2",
    "section": "Bernoulli trial",
    "text": "Bernoulli trial\nexpectation and variance"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#binomial",
    "href": "supplemental/exam-2-studyguide.html#binomial",
    "title": "Study list for exam 2",
    "section": "Binomial",
    "text": "Binomial\n\\(Y = B_1 + B_2 + ... + B_n\\), where \\(B_1, B_2, ..., B_n\\) Bernoulli trials FIST expectation and variance binomial pdf limiting of \\(p\\) approaches Poisson distribution use R to find these probabilities"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#poisson",
    "href": "supplemental/exam-2-studyguide.html#poisson",
    "title": "Study list for exam 2",
    "section": "Poisson",
    "text": "Poisson\nRIPS expectation and variance use R to find these probabilities"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#shifts-and-scales-of-normal-distribution",
    "href": "supplemental/exam-2-studyguide.html#shifts-and-scales-of-normal-distribution",
    "title": "Study list for exam 2",
    "section": "Shifts and scales of normal distribution",
    "text": "Shifts and scales of normal distribution\nZ score and standardization Use symmetry to find probabilities, finding them in R Percentiles to Z values, finding them in R"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#rule",
    "href": "supplemental/exam-2-studyguide.html#rule",
    "title": "Study list for exam 2",
    "section": "68 - 95 - 99.7 rule",
    "text": "68 - 95 - 99.7 rule"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#normality-tests-and-qqplots",
    "href": "supplemental/exam-2-studyguide.html#normality-tests-and-qqplots",
    "title": "Study list for exam 2",
    "section": "Normality tests and qqplots",
    "text": "Normality tests and qqplots"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#skewness-and-kurtosis",
    "href": "supplemental/exam-2-studyguide.html#skewness-and-kurtosis",
    "title": "Study list for exam 2",
    "section": "Skewness and Kurtosis",
    "text": "Skewness and Kurtosis"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#vocabulary-related-to-sampling",
    "href": "supplemental/exam-2-studyguide.html#vocabulary-related-to-sampling",
    "title": "Study list for exam 2",
    "section": "Vocabulary related to sampling",
    "text": "Vocabulary related to sampling\nsample, population, bias, representative, random, inference, cluster, strata, sampling frame, nonresponse rate, interviewer affects, survivor bias, …"
  },
  {
    "objectID": "supplemental/exam-2-studyguide.html#sampling-methods",
    "href": "supplemental/exam-2-studyguide.html#sampling-methods",
    "title": "Study list for exam 2",
    "section": "Sampling methods",
    "text": "Sampling methods\nSRS, stratified, cluster, census, voluntary response, convenience benefits and drawbacks of all and where to apply each"
  },
  {
    "objectID": "supplemental/chi-squared.html",
    "href": "supplemental/chi-squared.html",
    "title": "Chi-squared test expected values",
    "section": "",
    "text": "Information about the Chi-Squared test can be found here. We have two hypotheses:\n\\(H_0\\) = the variables are independent, there is no relationship between the two categorical variables.\n\\(H_A\\) = Knowing the value of one variable helps to predict the value of the other variable.\nUnder the null hypothesis, the distribution of one variable is independent to the distribution of the other. This means that the expected count for one cell, can be found using the column total times the row total divided by the total number of observations."
  },
  {
    "objectID": "supplemental/exam-revision-3.html",
    "href": "supplemental/exam-revision-3.html",
    "title": "Study list for exam 3",
    "section": "",
    "text": "The R functions we have discussed in these weeks:\n\n\n\nqt()\n\n\nt.test()\n\n\npchisq()\n\n\nchisq.test()\n\n\nlm()\n\n\n\n\nLecture 15 Confidence Intervals\nDistribution of proportion estimate\nConfidence Intervals for Proportions\nSRS Assumption\nCI manipulations\nConfidence Interval for Mean\nt-distributions\nMargin of Error\nSRS condition and Sample size condition\n\n\nLecture 16 Statistical Tests\nNull and alternative hypotheses\nType I and II Errors for Tests Testing a proportion: \\[ \\hat p\\sim \\mathcal{N}\\left(p_0, \\frac{p_0(1-p_0)}{n}\\right)\\] Multiple testing, impact and motivation\n\n\nLecture 17 Comparison\nTwo sample z-test for proportions\nStandard Error formula\nAssumptions for two-sample z-test for proportions\nConfidence Interval for Difference between Means\n\\[(\\bar X_1 - X_2 - t_{\\alpha/2}\\text{se}(\\bar X_1-\\bar X_2),\\bar X_1 - X_2 + t_{\\alpha/2}\\text{se}(\\bar X_1-\\bar X_2))\\]\n\\[\\text{se}(\\bar X_1-\\bar X_2) = \\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}.\\]\nCode for paired t-test in R\nPaired comparisons\n\n\nLecture 18 Inference for Counts\nTesting for independence using \\(\\chi^2\\) test\nTesting for goodness of fit using \\(\\chi^2\\) test\nDegrees of freedom: \\[\\text{df}=(r-1)(c-1)\\]\nR code to perform \\(\\chi^2\\) test and get p-value from test statistic\n\n\nLecture 19 Linear Patterns\nResponse and explanatory variables\n\\(R^2\\) for linear models\nSlope and intercept for linear models\nMathematical formulas for slope and intercept\nResiduals of linear model\nR code for fitting linear models"
  },
  {
    "objectID": "supplemental/tests_R.html",
    "href": "supplemental/tests_R.html",
    "title": "Confidence intervals, statistical tests, and comparison in R",
    "section": "",
    "text": "Note\n\n\n\nThese supplemental notes addends the following lectures: Confidence intervals, Statistical tests, and Comparison. It demonstrates how to do some of the in class examples in R, and how to do the same tests but with data instead of calculations. This will be useful for homework and projects."
  },
  {
    "objectID": "supplemental/tests_R.html#confidence-intervals",
    "href": "supplemental/tests_R.html#confidence-intervals",
    "title": "Confidence intervals, statistical tests, and comparison in R",
    "section": "confidence intervals",
    "text": "confidence intervals\n\nfor the proportion\nFor confidence intervals for the proportion use prop.test():\nThe solution to example 1 is:\n\n## The expected counts (x) is .14*35\nprop.test(x = .14*350, n = 350, correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  0.14 * 350 out of 350, null probability 0.5\nX-squared = 181.44, df = 1, p-value < 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.1075436 0.1802730\nsample estimates:\n   p \n0.14 \n\n\nThe solution to example 2 is:\n\nprop.test(x = 35, n = 225, correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  35 out of 225, null probability 0.5\nX-squared = 106.78, df = 1, p-value < 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.1140250 0.2086502\nsample estimates:\n        p \n0.1555556 \n\n\nThe solutions in the slides are not exactly those given by R. The slides are using a normal approximation to compute the confidence interval while R uses the Wilson interval. The interval from R is more accurate because it does two things:\n1. It’s wider around \\(0.5\\) than away from \\(0.5\\). This is important because an estimate for the proportion can not be below \\(0\\) or above \\(1\\). It makes sense that it should be shorter at the edges and wider in the middle.\n2. It lets the variance change along the interval. At p_low it uses \\(p_{low}(1-p_{low})\\) and at \\(p_{high}\\) it uses \\(p_{high}(1-p_{high})\\) where as the approximation we use has fixed variance.\n\n\nfor the mean\nTo find confidence intervals for the mean use t.test():\nExample 1 and example 2 do not have a built-in function in R to automagically compute them. Instead, I show you how to compute them directly from the data:\n\n## default confidence level is 95%\nt.test(diamonds$price)\n\n\n    One Sample t-test\n\ndata:  diamonds$price\nt = 228.95, df = 53939, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 3899.132 3966.467\nsample estimates:\nmean of x \n   3932.8 \n\n## a 72% confidence level\nt.test(diamonds$price, conf.level = 0.72)\n\n\n    One Sample t-test\n\ndata:  diamonds$price\nt = 228.95, df = 53939, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n72 percent confidence interval:\n 3914.243 3951.357\nsample estimates:\nmean of x \n   3932.8"
  },
  {
    "objectID": "supplemental/tests_R.html#statistical-tests",
    "href": "supplemental/tests_R.html#statistical-tests",
    "title": "Confidence intervals, statistical tests, and comparison in R",
    "section": "statistical tests",
    "text": "statistical tests\n\nfor proportions\nThe solution for example 3 follows:\n\nprop.test(x = 17, n = 36, p = 9/24, alternative = \"greater\")\n\n\n    1-sample proportions test with continuity correction\n\ndata:  17 out of 36, null probability 9/24\nX-squared = 1.0667, df = 1, p-value = 0.1508\nalternative hypothesis: true p is greater than 0.375\n95 percent confidence interval:\n 0.3294798 1.0000000\nsample estimates:\n        p \n0.4722222 \n\n\nAgain, this is not the same as our calculations because R is more accurate.\n\n\nfor means, directly from the data\nThe following code does this directly from the data:\n\n## alternative: true mean is not equal to 2000\nt.test(diamonds$price, mu = 2000, alternate = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  diamonds$price\nt = 112.52, df = 53939, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 2000\n95 percent confidence interval:\n 3899.132 3966.467\nsample estimates:\nmean of x \n   3932.8 \n\n## alternative: true mean is greater than 2000\nt.test(diamonds$price, mu = 2000, alternate = \"greater\")\n\n\n    One Sample t-test\n\ndata:  diamonds$price\nt = 112.52, df = 53939, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 2000\n95 percent confidence interval:\n 3899.132 3966.467\nsample estimates:\nmean of x \n   3932.8 \n\n## a 72% confidence level\nt.test(diamonds$price, conf.level = 0.72)\n\n\n    One Sample t-test\n\ndata:  diamonds$price\nt = 228.95, df = 53939, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n72 percent confidence interval:\n 3914.243 3951.357\nsample estimates:\nmean of x \n   3932.8"
  },
  {
    "objectID": "supplemental/tests_R.html#comparison",
    "href": "supplemental/tests_R.html#comparison",
    "title": "Confidence intervals, statistical tests, and comparison in R",
    "section": "comparison",
    "text": "comparison\n\ntwo sample \\(z\\) test for proportions\nExample 2 can almost be done in R. This does not specify the difference, just that the \\(H_A:p_s>p_i\\): the population average estimated by \\(\\hat{p_s}\\) input first (280/809) is greater than the population average estimated by \\(\\hat{p_i}\\) the fraction input second (197/646))\n\n## alternative: true mean is not equal to 2000\nprop.test(x = c(280, 197), n = c(809, 646), alternative = \"greater\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(280, 197) out of c(809, 646)\nX-squared = 2.5769, df = 1, p-value = 0.05422\nalternative hypothesis: greater\n95 percent confidence interval:\n -0.0007927079  1.0000000000\nsample estimates:\n   prop 1    prop 2 \n0.3461063 0.3049536 \n\n\n\n\ntwo sample \\(z\\) test for proportions\nexample 3. Again, the calculation in R is more accurate.\n\n## default is 95% confidence interval\nprop.test(x = c(280, 197), n = c(809, 646))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(280, 197) out of c(809, 646)\nX-squared = 2.5769, df = 1, p-value = 0.1084\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.008561667  0.090867154\nsample estimates:\n   prop 1    prop 2 \n0.3461063 0.3049536 \n\n\n\n\ntwo sample \\(t\\) test for means\n\nd.color <- diamonds %>% \n  filter(color == \"D\") %>% \n  select(carat)\n  \nj.color <- diamonds %>% \n  filter(color == \"J\") %>% \n  select(carat)\n\n## 2 - sided test\n## Alternative hypothesis: x != y\nt.test(x = d.color, y = j.color, \n       alternative = \"two.sided\")\n\n\n    Welch Two Sample t-test\n\ndata:  d.color and j.color\nt = -41.811, df = 3683.7, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5279915 -0.4806923\nsample estimates:\nmean of x mean of y \n0.6577948 1.1621368 \n\n## Alternative hypothesis: x < y by 0.1\nt.test(x = d.color, y = j.color, \n       alternative  = \"less\", mu = .1)\n\n\n    Welch Two Sample t-test\n\ndata:  d.color and j.color\nt = -50.101, df = 3683.7, p-value < 2.2e-16\nalternative hypothesis: true difference in means is less than 0.1\n95 percent confidence interval:\n       -Inf -0.4844961\nsample estimates:\nmean of x mean of y \n0.6577948 1.1621368 \n\n\n\n\nCI for the difference between two means\n\nt.test(x = d.color, y = j.color, conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  d.color and j.color\nt = -41.811, df = 3683.7, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5279915 -0.4806923\nsample estimates:\nmean of x mean of y \n0.6577948 1.1621368 \n\n\nexample 5 is already done in R\n\n\nPaired comparisons\n\n# Weight of mice before treatment\nbefore <-c(200.1, 190.9, 192.7, 213, 241.4, 196.9, 172.2, 185.5, 205.2, 193.7)\n# Weight of mice after treatment\nafter <-c(392.9, 393.2, 345.1, 393, 434, 427.9, 422, 383.9, 392.3, 352.2)\n\n# A tibble\nmice <- tibble(\n  group = rep(c(\"before\", \"after\"), each = 10),\n  weight = c(before, after)\n  )\n\nt.test(weight ~ group, data = mice, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  weight by group\nt = 20.883, df = 9, p-value = 6.2e-09\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 173.4219 215.5581\nsample estimates:\nmean difference \n         194.49"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\tag{1}\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Equation 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation 1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\tag{2}\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Equation 2, i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\tag{3}\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\tag{4}\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Equation 4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike’s Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\tag{5}\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\tag{6}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Equation 1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\tag{1}\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Equation 1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation 2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\tag{2}\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Equation 3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\tag{3}\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Equation 4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\tag{4}\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that’s more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{5}\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\tag{6}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Equation 5 and Equation 6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\tag{7}\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Equation 7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\tag{8}\\]"
  },
  {
    "objectID": "ae/ae-9.html",
    "href": "ae/ae-9.html",
    "title": "Associations AE-9",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 21 Oct at 2:00pm.\n\n\n\nlibrary(tidyverse) # for data manipulation and plots\nlibrary(praise) # for good vibes\n\nChapter 6: Association between quantitative variables\n\nIf the covariance between \\(x\\) and \\(y\\) is \\(0\\), then the correlation between \\(x\\) and \\(y\\) is \\(0\\) as well\n\nTrue\nFalse\n\nA retailer calculated the correlation line between the price of an item (\\(x\\)) and the amount sold (\\(y\\)). The correlation line is the same if the \\(x\\) and \\(y\\) variables are exchanged.\n\nTrue\nFalse\n\n\n\npraise()\n\n\nIf the correlation between number of customers and sales in dollars in retail stores is \\(r=0.6\\), then what would be the correlation if the sales were measured in thousands of dollars? In euros? (1 euro is worth about \\(\\$US0.97\\))\n\nChapter 10: Association between random variables\n\nMix and match\n\n\n\n\n\n\n\n1. Consequence of positive covariance\na. \\(p(x, y)\\)\n\n\n\n\n\nCovariance between X and Y\n\nb. \\(\\rho\\)\n\n\n\nProperty of uncorrelated random variables\n\nc. \\(p(x, y) = p(x)p(y)\\)\n\n\n\nWeighted sum of two random variables\n\nd. \\(\\rho \\sigma_X \\sigma_Y\\)\n\n\n\nSharpe ratio of a random variable\n\ne. \\(Var(X+Y) > Var(X) +Var(Y)\\)\n\n\n\nImplies \\(X\\) and \\(Y\\) are independent random variables\n\nf. \\(p(x) = p(y)\\)\n\n\n\nImplies \\(X\\) and \\(Y\\) are identically distributed\n\ng. \\(X_1, X_2, X_3\\)\n\n\n\nSymbol for correlation between random variables\n\nh. \\(Var(X, Y) = Var(X) +Var(Y)\\)\n\n\n\nSymbol for a joint probability distribution\n\ni. \\(S(Y)\\)\n\n\n\nSequence of iid random variables\n\nj. \\(3X - 2Y\\)\n\n\n\nIndependent random variables \\(X\\) and \\(Y\\) have the means and standard deviations as given in the following table. Use these parameters to find the expected value and SD of the following random variables that are derived from \\(X\\) and \\(Y\\)\n\n\\(2X - 100\\)\n\\(0.5Y\\)\n\\(X+Y\\)\n\\(X-Y\\)\n\n\n\n\nMean\nSD\n\n\n\\(X\\)\n1000\n200\n\n\n\\(Y\\)\n2000\n600\n\n\n\n\n\n\npraise()\n\n\nRepeat the calculations above but now \\(X\\) and \\(Y\\) are not independent and have \\(Cov(X, Y) = 12,500\\).\nWhat’s the covariance between a random variable \\(X\\) and a constant?\nA student budgets $60 weekly for gas and quick meals off-campus. Let \\(X\\) denote the amount spent for gas and \\(Y\\) the amount spent for quick meals in a typical week. Assume the student sticks to the budget.\n\nCan we model \\(X\\) and \\(Y\\) as independent random variables? Why or why not?\nSuppose we assume \\(X\\) and \\(Y\\) are dependent. What is the effect of this dependence on the variance of \\(X+Y\\)?"
  },
  {
    "objectID": "ae/ae-5.html",
    "href": "ae/ae-5.html",
    "title": "The birthday simulation (AE-5)",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 14 Oct at 2:00pm.\n\n\nComputers let you assemble, manipulate, and visualize data sets, all at speeds that would have wowed yesterday’s scientists. In short, computers give you superpowers! But if you wish to use them, you’ll need to pick up some programming skills. Steve Job said that “computers are bicycles for our minds” because the efficiency rating of humans on bicycles is so incredible, and computers give us similar powers.\nOne reason computers are so incredible is that they allow us to simulate a multitude of events. Today we will simulate the probability that two of you in this section of Stat1010 have the same birthdays.\n\nlibrary(tidyverse) # for data manipulation\nlibrary(vctrs) # to find the length of a tibble\n\nSuppose you are in a classroom with 100 people. If we assume this is a randomly selected group of 100 people, what is the chance that at least two people have the same birthday? Although it is somewhat advanced, we can deduce this mathematically. We will do this later. Here we use a Monte Carlo simulation. For simplicity, we assume nobody was born on February 29. This actually doesn’t change the answer much.\nFirst, note that birthdays can be represented as numbers between 1 and 365, so a sample of 100 birthdays can be obtained like this:\n\nall_bdays <- as_tibble(1:365) # data from where we can sample\n\nbdays_class <- \n  all_bdays %>% # data from where to sample\n  slice_sample(n = 100, replace = TRUE) # sample of size 100 with replacement\n\nTo check if in this particular set of 100 people we have at least two with the same birthday, we can use the functions group_by and filter, which returns a tibble with a vector of duplicated dates. Here is an example:\n\nbdays_class %>% \n  group_by(value) %>% # for each birthday\n  count() %>% # count them\n  filter(n > 1) # those have more than 1\n\nTo estimate the probability of a shared birthday in the group, we repeat this experiment by sampling sets of 100 birthdays over and over. Prior to replicating, we need to write this as a function.\n\nbdays_dups <- \n  all_bdays %>% # data from where to sample\n  slice_sample(n = 100, replace = TRUE) %>% # take a sample of n 100\n  group_by(value) %>% # for each birthday\n  count() %>% # count them\n  filter(n > 1) %>% # those have more than 1\n  vec_n(.) # the number that are duplicated\n\nA function has an input (in this case \\(n\\))\n\nWhat does \\(n\\) represent in this coding?\n\n\nnum_of_same_birthdays <- function(n){\n  all_bdays %>% # data from where to sample\n  slice_sample(n = n, replace = TRUE) %>% # take a sample\n  group_by(value) %>% # for each birthday\n  count() %>% # count them\n  filter(n > 1) %>% # those have more than 1\n  vec_size(.)  # the number that are duplicated\n}\n\nTo run this function, we do this:\n\nnum_of_same_birthdays(10) # run the function\n\n\nHow many students are there in class today?\nRun the coding above but include the number of students in class today.\n\nThe law of large numbers says that if we do this many times we should have an estimate of the number of people that have the same birthdays in class today. The function replicate can be used to run functions multiple times.\n\nB <- 500 # the number of times to run\nresults <- replicate(B, # replicate this number of times\n          ifelse( # if there are some duplicated birthdays\n            num_of_same_birthdays(50) >= 1, # our function\n            1, # give me a 1\n            0)) # if not, give me a 0\n\nmean(results) # take the mean\n\nWere you expecting the probability to be this high?\nPeople tend to underestimate these probabilities. To get an intuition as to why it is so high, think about what happens when the group size is close to 365. At this stage, we run out of days and the probability is one.\nSay we want to use this knowledge to bet with friends about two people having the same birthday in a group of people. When are the chances larger than 50%? Larger than 75%?\nLet’s create a look-up table. We can create a function to compute this for any group size. This function runs our function \\(500\\) times for every value of $n$. Statisticians will usually do this \\(10000\\) times, but this will take a very very very very looooooooooonnnnnggggg time, so we are simplifying.\n\ncompute_prob <- # name the function\n  function(n, B = 500){ # function inputs\n  results <- # store results\n    replicate(B, # run num_of_same_birthdays B times \n              ifelse( # if there are some duplicated birthdays\n            num_of_same_birthdays(n) >= 1, # our function\n            1, # give me a 1\n            0)) # if not, give me a 0\n  \n  mean(results) # find the mean\n}\n\nUsing the function map_dbl, we can perform element-wise operations on any function. Note that this may take awhile to run.\n\nn <- seq(1, 60) # which values of n are important\nprob <- # save the results as prob\n  map_dbl(n,  # for each value of n \n          compute_prob) # run the function\n\nWe can now make a plot of the estimated probabilities of two people having the same birthday in a group of size n:\n\nas_tibble(n, prob) %>% # the format for ggplot\n  ggplot() + # draw a graph\n  geom_point(aes(x = n, y = prob)) # of points\n\nNow let’s compute the exact probabilities rather than use Monte Carlo approximations. Not only do we get the exact answer using math, but the computations are much faster since we don’t have to generate experiments.\nTo make the math simpler, instead of computing the probability of it happening, we will compute the probability of it not happening, or the compliment. For this, we use the multiplication rule.\nLet’s start with the first person. The probability that person 1 has a unique birthday is 1. The probability that person 2 has a unique birthday, given that person 1 already took one, is 364/365. Then, given that the first two people have unique birthdays, person 3 is left with 363 days to choose from. We continue this way and find the chances of all \\(n\\) people having a unique birthday is:\n\\[1×\\frac{364}{365}×\\frac{363}{365}…\\frac{365−n+1}{365}\\] We can write a function that does this for any number:\n\nexact_prob <- function(n){\n  1 - \n    prod(365:(365-n+1))/ # the product from the numerator above\n    365^(n) # the denominator from above\n}\n\n\nRun this coding for the number of people that are in class today.\n\nNow, we run this on multiple values of \\(n\\)\n\neprob <- map_dbl(n, exact_prob) # run on multiple values of n\n\nNext we plot the results.\n\nas_tibble(n, eprob) %>% # the format for ggplot\n  ggplot() + # draw a graph\n  geom_point(aes(x = n, y = eprob)) # of points\n\nOn mercury, the year is only 88 days. Update the coding above to simulate the expected number of people with the same birthdays on Mercury.\nAssuming we have the same number of people, would you expect the number of people with the same birthdays to be higher or lower than those on earth?"
  },
  {
    "objectID": "ae/ae-11.html",
    "href": "ae/ae-11.html",
    "title": "Confidence intervals: AE - 10",
    "section": "",
    "text": "Match each item on the left with its correct description on the right.\n1.\\(y \\pm 2se(\\bar{y})\\) (a)  Sampling distribution of X\n2. \\(\\hat{p} \\pm se(\\hat{p})\\) (b)  Margin of error\n3. \\(2se (\\bar{X})\\) (c)  100% confidence interval for p\n4. \\(N(\\mu,\\sigma^2/n)\\) (d)  Estimated standard error of \\(\\bar{Y}\\)\n5. \\(s/\\sqrt{n}\\) (e)  Estimated standard error of \\(\\hat{p}\\)\n6. \\(\\sigma/\\sqrt{n}\\) (f)  An interval with about 95% coverage\n7. \\(1/(0.05)^2\\) (g)  Actual standard error of \\(\\bar{Y}\\)\n8. [0, 1] (h) About 2 for moderate sample sizes\n9. \\(\\sqrt{\\hat{p}(1 -\\hat{p})/n}\\) (i)   An interval with 68% coverage\n10. \\(t_{0.025, n-1}\\) (j)   Sample size needed for 0.05 margin of error\n11. True or False: By increasing the sample size from \\(n = 100\\) to \\(n = 400\\), we can reduce-the margin of error by \\(50\\%\\).\n12. If the 95% confidence interval for the average purchase of customers at a department store is $50 to $110, then $100 is a plausible value for the population mean at this level of confidence.\n13. If zero lies inside the 95% confidence interval for \\(\\mu\\), then zero is also inside the 99% confidence interval for \\(\\mu\\).\n14. The clothing buyer for a department store wants to order the right mix of sizes. As part of a survey, she measured the height (in inches) of men who bought suits at this store. Her software reported the following confidence interval:\nWith 95.00% confidence, 70.8876 < \\(\\mu\\) < 74.4970\n(a) Explain carefully what the software output means.\n(b) What’s the margin of error for this interval?\n(c) How should the buyer round the endpoints of the interval to summarize the result in a report for store managers?\n(d) If the researcher had calculated a 99% confidence interval, would the output have shown a longer or shorter interval?\n15. Hoping to lure more shoppers downtown, a city builds a new public parking garage in the central business district. The city plans to pay for the structure through parking fees. During a two-month period (44 weekdays), daily fees collected averaged $1,264 with a standard deviation of $150.\n(a) What assumptions must you make in order to use these statistics for inference?\n(b) Write a 90% confidence interval for the mean daily income this parking garage will generate, rounded appropriately.\n(c) The consultant who advised the city on this project predicted that parking revenues would average $1,300 per day. On the basis of your confidence interval, do you think the consultant was correct? Why or why not?\n(d) Give a 90% confidence interval for the total revenue earned during five weekdays."
  },
  {
    "objectID": "ae/ae-0-first_line_of_code.html",
    "href": "ae/ae-0-first_line_of_code.html",
    "title": "Introduction to the diamonds dataset",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due at noon on 6 Sept at 4:59pm.\n\n\n\nScenario\nYou own a jewelry business and are working to price your diamonds. To facilitate, you explore the diamond dataset in R and pay special attention to the cost of the diamond. Let’s explore together.\nTo accomplish this, please paste each line of code into your RStudio instance, and answer the accompanying questions.\n\n\nCoding basics\nGood coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.\n\nWhen writing code, load all libraries first.\n“##” is used to write comments in R and should be used to justify next steps and anything unusual or unexpected steps\nComments are really important and are notes to your future self to remind you about steps that you took and decisions that you made.\n\n\n## loading library\nlibrary(tidyverse) # for data analysis and visualisation\n\n\n\nData\nThe diamonds dataset includes basic information about 53940 diamonds including price and other characteristics.\n\n## View the dataset in a separate tab\nglimpse(diamonds)\n\nThe ___ dataset has ___ observations and ___ variables.\n\n## What are the variables in this dataset?\n?diamonds\n\n\nWhat do the “4 c’s” of every diamond mean? What kind of variables are they? (hint: This video may help, copy this into your notes under the title data types)\n\nggplot is an R package that is used to create data visualizations. We will be using it throughout this course. The basic format for every ggplot command is:\n\n## DO NOT COPY INTO YOUR QUARTO DOCUMENT\n## THIS IS TO SHOW YOU THE BASIC FORMAT OF ALL ggplots\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\n\n\n## What is the distribution of price?\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = price), binwidth = 1) \n\n\nWhat kind of variable is price? What happens if you draw a histogram with another kind of variable?\n\n\n## What happens if you draw a histogram with a different type of variable?\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = OTHER_VAR), binwidth = 1) \n\n\nWhat does the binwidth in the geom_histogram() function do? (hint: draw multiple plots changing this value) What binwidth value is most appropriate for this data?\n\n\n## Find an appropriate binwidth\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = price), binwidth = OTHER_VALUE) \n\nThe distribution of diamond prices is right skewed (long right tail).\n\nIn your notes, copy these histograms and the words used to describe them.\n\nOne interesting feature of this graph is the dip in diamonds priced at about $1000. Let’s explore this a little further and look at some basic dplyr commands.\nThe pipe operator “%>%” tells R to take the output from one function and sends it to the input in the next function.\n\n## Filtering on price\ndiamonds %>% \n  filter(price > 750 & price < 1250) %>% \n  View()\n\n\nWhat does the filter function do?\n\n\ndiamonds %>% \n  filter(price > 750 & price < 1250) %>% ## Filtering on price\n  mutate(price_hundred = price %/% 100) %>% ## Finding price_hundred\n  View()\n\n\nWhat does this “%/%” operator do? (hint: run 10 %/% 3, 10 %/% 5, 7 %/% 3, 7 %/% 5 in the console)\nWhat does the mutate function do? (hint: pay special attention to the dimensions of the data)\n\n\ndiamonds %>% \n  filter(price > 750 & price < 1250) %>% ## Filtering on price\n  mutate(price_hundred = price %/% 100) %>% ## Finding price_hundred\n  select(price, price_hundred) %>% ## Select vars\n  View()\n\n\nWhat does the select function do?\n\n\ndiamonds %>% \n  filter(price > 750 & price < 1250) %>% ## Filtering on price\n  mutate(price_hundred = price %/% 100) %>% ## Finding price_hundred\n  select(price, price_hundred) %>% ## Select vars\n  count(price_hundred) %>% ## Count\n  View()\n\n\nCan you identify the values where the dip is using the information above? Why or why not?\n\n\ndiamonds %>% \n  filter(price > 750 & price < 1250) %>% ## Filtering on price\n  mutate(price_hundred = price %/% 100) %>% ## Finding price_hundred\n  select(price, price_hundred) %>% ## Select vars\n  mutate(price_remainder = price %% 100) %>% ## Finding price_remainder\n  count(price_hundred, price_remainder) %>% ## Count\n  View()\n\n10. What does the “%%” operator do?\n\nCan you identify the values where the dip is using the information above? Why or why not?\n\nCut may impact upon price\n\n## Cut and price\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = price, color = cut, fill = cut),\n                 binwidth = 100) \n\n\nWhich cut do you suspect is the most common in the dataset? How many diamonds with that cut are in the dataset?\n\n\n## Most common cute\ndiamonds %>% \n  INSERT_FUNCTION_HERE(cut)\n\n\nThe variables color or clarity might impact upon the price. Draw a histogram and color it using one of them.\n\n\n## Impact of color and clarity\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = price, color = VAR_NAME, \n                               fill = VAR_NAME), \n                 binwidth = YOUR_BIN_WIDTH) \n\n\nWhat do the fill and color inputs do in the above coding?\nWhat kind of variables should we use for the fill and color inputs? What happens if another type of variable is used?\n\n\n## Color and fill variables\nggplot(data = diamonds) +\n      geom_histogram(mapping = aes(x = price, color = VAR_NAME, \n                                   fill = VAR_NAME), \n                     binwidth = YOUR_BIN_WIDTH) \n\nSo far we have been looking at only 1 continuous variable. To look at two continous variables, we draw scatterplots.\n\n## Color and fill variables\nggplot(data = diamonds) +\n      geom_point(mapping = aes(x = price, y = depth)) \n\n\nWhat kinds of variables do you think you could use to color this plot? Please use an appropriate variable from diamonds dataset and do so.\n\n\n## Color and fill variables\nggplot(data = diamonds) +\n      geom_point(mapping = aes(x = price, y = depth))"
  },
  {
    "objectID": "ae/ae-12.html",
    "href": "ae/ae-12.html",
    "title": "Chi-squared test: AE - 11",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 21 Nov at 5:00pm.\n\n\nMatch each item on the left with its correct description on the right.\n\nNumber of degrees of freedom in the chi-squared test of independence in a 2 × 2 table\nNumber of constraints on frequencies in the chi-squared test of goodness of fit of a binomial distribution\nP-value if \\(\\chi^2\\) = 9.488 when testing for independence in a 3 × 3 table\nP-value if \\(\\chi^2\\) = 16.812 when testing the null hypothesis that a categorical variable with 7 levels has a uniform distribution\nSmallest possible value of \\(\\chi^2\\)\nImpossible value for \\(\\chi^2\\)\nReject the null hypothesis of independence in a 3 × 4 contingency table if \\(\\chi^2\\) is larger than this value and \\(\\alpha = 0.05\\)\nReject the null hypothesis of independence in a 6 × 3 contingency table if \\(\\chi^2\\) is larger than this value and \\(\\alpha = 0.01\\)\nThe expected cell count in a table should be larger than this number when using \\(\\chi^2\\)\nNumber of degrees of freedom if using chi-squared to test whether four proportions are the same\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na. 0\nb. 0.01\nc. 23.209\nd. 10\ne. 1\nf. 0.05\ng. 3\nh. 2\ni. 12.592\nj. -1\n\n\n\nTrue/False\n\nThe chi-squared test of independence only detects linear association between the variables in a contingency table.\nA statistically significant \\(\\chi^2\\) in the test of independence implies a causal relationship between the variables that define a contingency table.\nThe expected size of the chi-squared statistic \\(\\chi^2\\) increases with the number of observations \\(n\\) in the table.\nA stock market analyst recorded the number of stocks that went up or went down each day for 5 consecutive days, producing a contingency table with two rows (up or down) and five columns (Monday through Friday). Are these data suitable for applying the chi-squared test of independence?\nThe human resources group regularly interviews prospective clerical employees and tests their skill at data entry. The following table shows the number of errors made by 60 prospective clerks when entering a form with 80 numbers. (Five clerks made four errors.) Test the null hypothesis that the number of errors follows a Poisson distribution. Find the\n\n\n\nerrors\nClerks\n\n\n\n\n0\n12\n\n\n1\n20\n\n\n2\n9\n\n\n3\n14\n\n\n4 or more\n5\n\n\n\na. Rate (or mean) of the Poisson distribution\nb. Expected number of employees who do not make an error\nc. Degrees of freedom of \\(\\chi^2\\)\nd. \\(\\chi^2\\)\ne. p-value for testing H0."
  },
  {
    "objectID": "ae/ae-8.html",
    "href": "ae/ae-8.html",
    "title": "Random variables AE-8",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 14 Oct at 2:00pm.\n\n\n\nlibrary(tidyverse)\nlibrary(praise)\n\n\nGiven that the random variable \\(X\\) has mean \\(\\mu = 120\\) and SD \\(\\sigma = 15\\), find the mean and SD of each of these random variables that are defined by \\(X\\).\n\n\\(X/3\\)\n\\(2X - 100\\)\n\\(X+2\\)\n\\(X-X\\)\n\nAn investor buys the stock of two companies, investing \\(\\$10000\\) in each. The stock of each company either goes up by \\(80%\\) after a month (rising to \\(\\$18000\\)) with probability \\(\\frac{1}{2}\\) or drops by \\(60%\\) (falling to \\(\\$4000\\)) with probability \\(\\frac{1}{2}\\). Assume that the changes in each are independent. Let the random variable \\(X\\) denote the value of the amounted invested after one month.\n\nFind the probability distribution of \\(X\\).\nFind the mean value of \\(X\\)\nDoe the mean value represent the experience of the typical investor?\n\n\n\npraise()\n\n\nA law firm takes cases on a contingent fee basis. If the case goes to trial, the firm expects to earn \\(\\$25000\\) as part of the settlement if it wins and nothing if it does not. The firm wins one-third of the cases that go to trial. If the case does not go to trial, the firm earns nothing. Half of the cases do not go to trial.\n\nDefine a random variable to model the earning of taking a case of this type.\nWhat is the expected value of such a case to the firm?\nWhat is the standard deviation of the earnings?\n\nThe maintenance staff of a large office building regularly replaces fluorescent ceiling lights that have gone out. During a visit to a typical floor, the staff may have to replace several lights. The manager of this staff has given the following probabilities to the number of lights (identified by the random variable \\(Y\\)) that need to be replaced on the floor:\n\n\n\n\\(Y\\)\n0\n1\n2\n3\n4\n\n\n\\(P(Y = y)\\)\n0.2\n0.15\n0.2\n0.3\n0.15\n\n\n\n\nHow many lights should the manager expect to replace on a floor?\nWhat is the standard deviation of the number of lights on a floor that are replaced?\nIf a crew takes six lights to a floor, how many should it expect to have left after replacing those that are out?\nIf a crew takes six lights to a floor, find the standard deviation of the number of lights that remain after replacing those that are out on a floor.\nIf it takes 10 minutes to replace each light, how long should the manager expect the crew to take when replacing the lights on a floor.\n\nSuppose that you’ve just bought a \\(\\$4000\\) TV. Should you also buy the \\(\\$50\\) surge protector that guarantees to protect your TV from electric surges caused by lightning?\n\nLet \\(p\\) denote the probability that your home is hit by lightning during the time that you own this TV, say five years. In order for the purchase of the surge protector to have positive long-term values, what must the chance of being hit by lightning be?\nThere are about 100 million households in the United States, and fewer than 10,000 get hit by lightening each year. Do you think the surge protector is a good deal on average? Be sure to note any assumptions that you make.\n\n\n\npraise()\n\n\nThe ATM at a local convenience store allows customers to make withdrawals of \\(\\$10\\), \\(\\$20\\), \\(\\$50\\), or \\(\\$100\\). Let \\(X\\) denote a random variable that indicates the amount withdrawn by a customer. The probability distribution of \\(X\\) is:\n\\(p(10) = 0.2\\)\n\\(p(20) = 0.5\\)\n\\(p(50) = 0.2\\)\n\\(p(100) = 0.1\\)\n\nPlot the probability distribution of \\(X\\) in R.\nWhat is the probability that a customer withdraws more than \\(\\$20\\)?\nWhat is the expected amount of money withdrawn by a customer?\nThe expected value is not a possible value value of the amount withdrawn. Interpret the expected value for a manager.\nFind the variance and standard deviation of \\(X\\)."
  },
  {
    "objectID": "ae/ae-7.html",
    "href": "ae/ae-7.html",
    "title": "Revision 2 for exam 1",
    "section": "",
    "text": "Find the matching item from the second column\n\n\n\n\n\n\n\n1. Independent events\na. \\(P(A \\cap B) \\neq P(A) \\times P(B)\\)\n\n\n\n\n\nDisjoint events\n\nb. \\(0\\leq p \\leq 1\\)\n\n\n\nUnion\n\nc. \\(A, B\\) disjoint \\(P(A\\cup B) = P(A) +P(B)\\)\n\n\n\nIntersection\n\nd. \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\n\nComplement of A\n\ne. \\(P(A \\cup B) \\leq P(A) + P(B)\\)\n\n\n\nSample space\n\nf. \\(A \\cap B\\)\n\n\n\nAddition rule\n\ng. \\(P(A \\cap B) = 0\\)\n\n\n\nComplement rule\n\nh. \\(P(S) = 1\\)\n\n\n\nBoole’s inequality\n\ni. \\(P(A^c) = 1 - P(A)\\)\n\n\n\nDependent events\n\nj. \\(P(A \\cap B) = P(A) \\times P(B)\\)\n\n\n\nKolmogorov’s axiom\n\nk. \\(A \\cup B\\)\n\n\n\nl. \\(A^c\\)\n\n\n\nm. \\(S\\)\n\n\n\n\n\n\nWhen \\(A \\subset B\\), then \\(A \\cup B = A\\)\n\nTrue\nFalse\n\nWhen \\(A \\subset B\\), then \\(A \\cap B = A\\)\n\nTrue\nFalse\n\nOne ball will be drawn at random from a box containing: 4 cyan balls, 3 magenta balls, and 5 yellow balls. What is the probability that the ball will be cyan?\nWhat is the probability that the ball will not be cyan?\nInstead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\nNow repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling with replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\nTwo events A and B are independent if Pr(A and B)=Pr(A)P(B). Under which situation are the draws independent?\n\nYou don’t replace the draw.\nYou replace the draw.\nNeither\nBoth\n\nSay you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one is yellow?\nIf you roll a 6-sided die six times, what is the probability of seeing neither a 5 or 6?\nPlease match the following venn diagrams with the corresponding set theory notation.\n\n\n\n22.\na.\\(B \\subset A\\)\n\n\n\n\n23.\nb. \\(A\\) & \\(B\\) disjoint\n\n\n24.\nc. \\(A\\)\n\n\n25.\nd. \\(A \\cup B\\)\n\n\n26.\ne. \\(A \\cap B\\)\n\n\n27.\nf. \\(A^c\\)\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n\n1\nj\n\n\n\n\n2\ng\n\n\n3\nk\n\n\n4\nf\n\n\n5\nl\n\n\n6\nm\n\n\n7\nd\n\n\n8\ni\n\n\n9\ne\n\n\n10\na\n\n\n11\nh, c, b\n\n\n12\nFalse\n\n\n13\nTrue\n\n\n14\n\\(\\frac{4}{4+3+5} = \\frac{4}{12}\\)\n\\(= \\frac{1}{3}\\)\n\n\n15\n\\(1 - \\frac{1}{3} = \\frac{2}{3}\\)\n\n\n16\nLet \\(C_1 = \\{1st\\:draw \\: is \\:cyan\\}\\) and\n\\(C_2 = \\{2nd\\: draw\\: is\\: cyan\\}\\)\nWe want :\\(P(C_1 \\cap C_2^c) = P(C_1) \\times P(C_2^c \\vert C_1)\\)\nThis is \\(\\frac{1}{3} \\times (1 -\\frac{3}{11}) = \\frac{8}{33}\\)\n\n\n17\nBecause they are independent:\\(P(C_1 \\cap C_2^c) = P(C_1) \\times P(C_2^c)\\)\n= \\(\\frac{1}{3} \\times \\frac{2}{3} = \\frac{2}{9}\\)\n\n\n18\nb\n\n\n19\n\\(\\frac{5}{12}\\)\n\n\n20\nLet \\(A = \\{5 \\: or \\: 6 \\: pips\\: facing\\: up\\}\\)\nWe want:\\(P(A^c \\cap A^c \\cap A^c \\cap A^c \\cap A^c \\cap A^c) = P(A^c)^6\\) because of independence of rolling a die.\n\\(= (\\frac{2}{3})^6 = \\frac{64}{729}\\)\n\n\n21\nNo question\n\n\n22\nc\n\n\n23\nf\n\n\n24\nb\n\n\n25\ne\n\n\n26\nd\n\n\n27\na"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html",
    "href": "ae/ae-4-exam-1-review.html",
    "title": "AE 4: Exam 1 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-4-exam-1-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#packages",
    "href": "ae/ae-4-exam-1-review.html#packages",
    "title": "AE 4: Exam 1 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggfortify)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "href": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "title": "AE 4: Exam 1 Review",
    "section": "Restaurant tips",
    "text": "Restaurant tips\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St. Olaf who worked at a local restaurant.1\nThe variables we’ll focus on for this analysis are\n\nTip: amount of the tip\nParty: number of people in the party\n\nView the data set to see the remaining variables.\n\ntips <- read_csv(\"data/tip-data.csv\")"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "href": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "title": "AE 4: Exam 1 Review",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\n\nVisualize, summarize, and describe the relationship between Party and Tip.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#modeling",
    "href": "ae/ae-4-exam-1-review.html#modeling",
    "title": "AE 4: Exam 1 Review",
    "section": "Modeling",
    "text": "Modeling\nLet’s start by fitting a model using Party to predict the Tip at this restaurant.\n\nWrite the statistical model.\nFit the regression line and write the regression equation. Name the model tips_fit and display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret the slope.\nDoes it make sense to interpret the intercept? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#inference",
    "href": "ae/ae-4-exam-1-review.html#inference",
    "title": "AE 4: Exam 1 Review",
    "section": "Inference",
    "text": "Inference\n\nInference for the slope\n\nThe following code can be used to create a bootstrap distribution for the slope (and the intercept, though we’ll focus primarily on the slope in our inference). Describe what each line of code does, supplemented by any visualizations that might help with your description.\n\n\nset.seed(1234)\n\nboot_dist <- tips %>%\n  specify(Tip ~ Party) %>%\n  generate(reps = 100, type = \"bootstrap\") %>%\n  fit()\n\n\nUse the bootstrap distribution created in Exercise 6, boot_dist, to construct a 90% confidence interval for the slope using bootstrapping and the percentile method and interpret it in context of the data.\n\n\n# add your code here\n\n\nConduct a hypothesis test at the equivalent significance level using permutation. State the hypotheses and the significance level you’re using explicitly. Also include a visualization of the null distribution of the slope with the observed slope marked as a vertical line.\n\n\n# add your code here\n\n\nCheck the relevant conditions for Exercises 7 and 8. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\nNow repeat Exercises 7 and 8 using approaches based on mathematical models.\n\n\n# add your code here\n\n\nCheck the relevant conditions for Exercise 9. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\n\nInference for a prediction\n\nBased on your model, predict the tip for a party of 4.\n\n\n# add your code here\n\n\nSuppose you’re asked to construct a confidence and a prediction interval for your finding in Exercise 11. Which one would you expect to be wider and why? In your answer clearly state the difference between these intervals.\nNow construct the intervals from Exercise 12 and comment on whether your guess is confirmed.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#model-diagnostics",
    "href": "ae/ae-4-exam-1-review.html#model-diagnostics",
    "title": "AE 4: Exam 1 Review",
    "section": "Model diagnostics",
    "text": "Model diagnostics\n\nLeverage (Outliers in x direction)\n\nWhat is the threshold used to identify observations with high leverage? Calculate the threshold and save the value as leverage_threshold.\n\n\n# add your code here\n\n\nMake a plot of the standardized residuals vs. leverage (you can do this with ggplot() or with autoplot(which = 5)). Use geom_vline() to add a vertical line to help identify points with high leverage.\n\n\n# add your code here\n\n\nLet’s dig into the data further. Which observations have high leverage? Why do these points have high leverage?\n\n\n# add your code here\n\n\n\nIdentifying outliers (outliers in y direction)\n\nMake a plot of the residuals vs. fitted values and a plot of the square root of the absolute value of standardized residuals vs. fitted (You can use autoplot(which = c(1, 3)) to display the plots side-by-side).\n\n\nHow are the plots similar? How do they differ?\nWhat is an advantage of using the plot of the residuals vs. fitted to check conditions and model diagnostics?\nWhat is an advantage of using the plot of the \\(\\sqrt{|\\text{standardized residuals}|}\\) vs. fitted to check conditions and model diagnostics?\n\n\n# add your code here\n\n\nAre there any observations that are outliers?\n\n\n# add your code here\n\n\n\nCook’s distance\n\nMake a plot to check Cook’s distance (autoplot(which = 4)). Based on this plot, are there any points that have a strong influence on the model coefficients?\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#adding-another-variable",
    "href": "ae/ae-4-exam-1-review.html#adding-another-variable",
    "title": "AE 4: Exam 1 Review",
    "section": "Adding another variable",
    "text": "Adding another variable\n\nAdd another variable, Alcohol, to your exploratory visualization. Describe any patterns that emerge.\n\n\n# add your code here\n\n\nFit a multiple linear regression model predicting Tip from Party and Alcohol. Display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret each of the slopes.\nDoes it make sense to interpret the intercept? Explain your reasoning.\nAccording to this model, is the rate of change in tip amount the same for various sizes of parties regardless of alcohol consumption or are they different? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-10.html",
    "href": "ae/ae-10.html",
    "title": "Exam 2 revision",
    "section": "",
    "text": "Section A:\n\n\n\n\n\n\n\n\nExpected value of \\(X\\)\n\na. \\(E(X - \\mu)\\)\n\n\n\nVariance of \\(X\\)\n\nb. \\(10X\\)\n\n\n\nStandard deviation of \\(X\\)\n\nc. \\(\\frac{(X-0.04)}{\\sigma}\\)\n\n\n\nShorthand notation for \\(P(X=x)\\)\n\nd. \\(X+10\\)\n\n\n\nHas 10 times the standard deviation of \\(X\\)\n\ne. \\(E(X-\\mu)^2\\)\n\n\n\nIs always equal to zero\n\nf. \\(\\sqrt{VarX}\\)\n\n\n\nIncreases the mean of \\(X\\) by 10\n\ng. \\(\\mu\\)\n\n\n\nHas standard deviation 1\n\nh. \\(p(x)\\)\n\n\n\nPlease review AE-8, the solutions are on the Canvas page.\nSection B:\n\n\n\n\n\n\n\n\nConsequence of covariance\n\na. \\(p(x,y)\\)\n\n\n\nCovariance between \\(X\\) and \\(Y\\)\n\nb. \\(\\rho\\)\n\n\n\nProperty of uncorrelated random variables\n\nc. \\(p(x, y) = p(x)p(y)\\)\n\n\n\nWeighted sum of two random variables\n\nd. \\(\\rho\\sigma_x\\sigma_y\\)\n\n\n\nSharpe ratio of random variable\n\ne. \\(Var(X+Y) >Var(X) + Var(Y)\\)\n\n\n\nImplies \\(X\\) and \\(Y\\) are independent random variables\n\nf. \\(p(x) = p(y)\\)\n\n\n\nImplies \\(X\\) and \\(Y\\) are identically distributed\n\ng. \\(X_1, X_2, X_3\\)\n\n\n\nSymbol for the correlation between random variables\n\nh. \\(Var(X+Y) = Var(X) + Var(Y)\\)\n\n\n\nSymbol for joint probability distribution\n\ni. \\(S(Y)\\)\n\n\n\nSequence of iid random variables\n\nj. \\(3X-2Y\\)\n\n\n\nPlease review AE-9, the solutions are on the Canvas page.\nSection C:\n\\(Y\\) is a binomial random variable with parameters \\(n\\) and \\(p\\), \\(X\\) is a Poisson random variable with parameter \\(\\lambda\\) and \\(B_1\\) and \\(B_2\\) are Bernoulli trials with probability of success \\(p\\).\n\n\n\n\n\n\n\n1. Expresssion for the variance of \\(Y\\)\na. \\(np\\)\n\n\n\n\n\nProbability that the first trial \\(B_1\\) fails\n\nb. \\(e^{-\\lambda}\\)\n\n\n\nExpression for the mean of \\(X\\)\n\nc. \\(e^{-\\lambda} \\frac{\\lambda^2}{2}\\)\n\n\n\nProbability that \\(Y\\) is zero\n\nd. \\(p\\)\n\n\n\nProbability that \\(X\\) is 2\n\ne. \\(0\\)\n\n\n\nCovariance between \\(B_1\\) and \\(B_2\\)\n\nf. \\(p^n\\)\n\n\n\nExpression for the expected value of \\(X\\)\n\ng. \\(1-p\\)\n\n\n\nThe expected value of \\(B_1\\)\n\nh. \\((1-p)^n\\)\n\n\n\nProbability that \\(Y\\) is \\(n\\)\n\ni. \\(np(1-p)\\)\n\n\n\nProbability that \\(X\\) is \\(0\\).\n\nj. \\(\\lambda\\)\n\n\n\n\nAn auditor inspects 25 transactions processed by the business office of a company. The auditor selects these transactions at random from the thousands that were processed in the most recent three months. In the past, the auditor has found 10% of transactions at this type of company to have been processed incorrectly. True or False: A binomial model would be more appropriate for this problem if the auditor picked the first 25 transactions during the three-month period.\nIs the binomial model suited to these applications?\n\nThe next five cars that enter a gasoline filling stations get a fill-up.\nA poll of the 15 members of the board of directors indicates that 6 are in favor of a proposal to change the salary of the CEO.\nA company realizes that 10% of its packages are not being sealed properly. When examining a case of 24, it counts the number that are unsealed.\n\nEvery now and then even a good diamonds cutter has a problem and the diamond shatters when being cut. For one cutter, the chance of such errors is 0.1%.\n\nWhat is the probability model seems well suited to this problem? Why?\nIf this cutter works on 75 stones, what is the probability that he breaks 2 or more?\n\nA dairy farmer accidentally allowed some of his cows to graze in a pasture containing weeds that would contaminate the milk from his herd. The farmer estimates that there’s a 10% chance of a cow grazing on some of the flavorful weeds.\n\nUnder these conditions, what is the probability that none of the 12 animals in this herd ate the weeds.\nDoes the poisson model give a good estimate of the probability that no animal ate the weeds?\n\n\nSection D:\n\\(X\\) denotes a normally distributed random variable: \\(X \\sim N(\\mu, \\sigma^2)\\). A googol, the namesake of Google, is \\(10^{100}\\). The random variable \\(Z\\) denotes a standard normal random variable, \\(Z \\sim N(0,1)\\)\n\n\n\n\n\n\n\n\nMean of \\(X\\)\n\na. \\(\\frac{1}{2}\\)\n\n\n\nVariance of \\(X\\)\n\nb. \\(P(Z<1)\\)\n\n\n\nProbability of \\(X\\) being less than its mean\n\nc. \\(0.05\\)\n\n\n\nProbability of \\(X\\) being less than \\(\\mu +\\sigma\\)\n\nd. \\(\\frac{2}{3}\\)\n\n\n\nStandard deviation of \\(Z\\)\n\ne. \\(\\frac{1}{1 googol}\\)\n\n\n\nProbability that a \\(z-score\\) based on \\(X\\) is less than \\(1\\) in magnitude\n\nf. \\(\\mu\\)\n\n\n\nProportion of a normal distribution that is more than \\(20\\sigma\\) from \\(\\mu\\)\n\ng. \\(\\sigma^2\\)\n\n\n\nDifference between value of \\(P(Z<-x)\\) and value of \\(P(Z>x)\\)\n\nh. \\(1\\)\n\n\n\nDistribution of the random variable \\(\\mu +\\sigma Z\\)\n\ni. \\(0\\)\n\n\n\nProbability that \\(Z>1.96\\) plus the probability that \\(Z<-1.96\\)\n\nj. \\(N(\\mu, \\sigma^2)\\)\n\n\n\n\nThe currently age (in years) of 400 clerical employees at an insurance claims processing center is normally distributed with mean 38 and SD 6. True or False: A training program for employees under the age of 30 at the center would be expected to attract about 36 employees.\nIf \\(X_1 \\sim N(\\mu, \\sigma)\\) and \\(X_2 \\sim N(\\mu, \\sigma)\\) are iid, then what is the distribution of \\(\\frac{X_1-X_2}{\\sqrt{2}\\sigma}\\)\nA contractor built 30 similar homes in a suburban development. The homes have comparable size and amenities, but each has features that customize the appearance, landscape, and interior. The contractor expects the homes to sell for $450,000. He expects that one-third of the homes will sell either for less than $400,000 or more than $500,000.\n\nWould a normal model be appropriate to describe the distribution of sale prices?\nWhat data would help you decide if a normal model is appropriate? (These homes are unsold, their prices are unavailable)\nWhat normal model has properties that are consistent with the intuition of the contractor?\n\nA hurricane bond pays the holder a face amount, say $1 million, if a hurricane causes major damage in the United States. Suppose that the chance for such a storm is 5% per year.\n\nIf a financial firm sells these bonds for $60,000, what is the chance that the firm loses money if it only sells one?\nIf the firm sells 1,000 of these policies, each for $60,000, what is the probability that it loses money.\nHow does the difference between the probabilities of parts a and b compare to the situation of an insurance company that writes coverage to homeowners who have accidents independently of one another?\n\n\nSection E:\n\n\n\n\n\n\n\n\nSample\n\na. A complete collection of items desired to be studied\n\n\n\nCensus\n\nb. A list of all the items in the population\n\n\n\nTarget population\n\nc. A subset of a larger collection of items\n\n\n\nStatistic\n\nd. A homogeous subset of the population\n\n\n\nParameter\n\ne. A characteristic of a sample\n\n\n\nSampling frame\n\nf. Occurs if a sampling method distorts a property of the population\n\n\n\nSimple random sample\n\ng. A comprehensive study of every item of the population\n\n\n\nStratum\n\nh. The result if a respondent chooses not to answer a question\n\n\n\nBias\n\ni. A characteristic of a population\n\n\n\nNonresponse\n\nj. Sample chosen so that all subset of size \\(n\\) are equally likely.\n\n\n\n\nTrue or False: Bias due to the wording of questions causes different samples to present different impressions of the population.\nA school district has requested a survey be conducted on the socioeconomic status of their students. Their budget only allows them to conduct the survey in some of the schools, hence they need to first sample a few schools. Students living in this district generally attend a school in their neighbourhood. The district is broken into many distinct and unique neighbourhoods, some including large single-family homes and others with only low-income housing. What kind of sampling did they employ?\n\nSection F:\n\nWhich of the following \\(X\\)-bar charts indicate a process that is out of control?\n\nWhich of the following \\(X\\)-bar charts show that a process went out of control?\n\n\nWhich, if any, of these combinations of an \\(X\\)-bar and an \\(S\\)-chart suggest a problem? If there’s a problem, in which chart did you find the problem?\n\nWhich, if any, of these combinations of an \\(X\\)-bar and an \\(S\\)-chart suggest a problem? If there’s a problem, in which chart did you find the problem?\n\nThe manager of a warehouse monitors shipments. A random sample of 25 packages is selected and weighed every day. The mean weight should be \\(\\mu = 22\\) pounds, and \\(\\sigma = 5\\) pounds. True or False: An \\(X\\)-bar chart with control limits 12 pounds and 32 pounds has a 5% chance of a Type I error.\nRather than stop the production when a mean crosses the control limits in the \\(X\\)-bar chart, a manager has decided to wait until two consecutive means lie outside the control limits before stopping the process. The control limits are \\(\\mu \\pm 2\\sigma/ \\sqrt{n}\\).\n\nBy waiting for two consecutive sample means to lie outside the control limits, has the manager increased or decreased the chance for a Type I error?\nWhat is the probability of a Type I error if this procedure is used?\n\nWhere should the control limits for an \\(X\\)-bar chart be placed if the design of the process sets \\(\\alpha = 0.0027\\) with the following parameters (assume that the sample size condition for control charts has been verified)?\n\n\\(\\mu = 10, \\sigma = 5,\\) and \\(n = 18\\) cases per batch\n\\(\\mu = -4, \\sigma = 2,\\) and \\(n = 12\\) cases per batch"
  },
  {
    "objectID": "ae/ae-6.html",
    "href": "ae/ae-6.html",
    "title": "Revision 1 for exam 1",
    "section": "",
    "text": "What tidyverse function would you use to extract rows?\na. filter()\n\n\n\ncount()\nselect()\ndistinct()\n\n\n\nWhat tidyverse function would you use to extract colums?\n\n\n\nfilter()\ncount()\nc. select()\ndistinct()\n\n\n\nTo find the maximum number of times that a categorical variable appears which function should I use?\n\n\n\nfilter()\nb. count()\nselect()\ndistinct()\n\n\n\nWhat does the “%>%” operator do?\n\na. used in tidyverse between layers of dplyr coding\nb. used in ggplot to add more layers of coding\n\n\nWill this code run without an error? diamonds %>% mutate(price_hundred = price %/% 100)\n\n\n\nIt will produce an error\nb. It will not produce an error\n\n\n\nWhat does the binwidth input in ggplot do?\n\n\n\nIt controls the opacity of the plot\nIt can be used to update the labels\nc. It controls the smoothness of a density plot\nIt controls the size of dots\n\n\n\nWhat punctuation must be placed before a function or dataset to access more information?\n\n\n\n“.”\n“+”\nc. “?”\n“!”\n\n\n\nWhere should the variable names go in the following line of code: ggplot(data = A) + B(mapping = aes(C))\n\n\n\nA\nB\nc. C\n\n\n\nWhat does the here package do?\n\n\n\nfor data manipulation and display\nb. to organize files\nnone of the above\n\n\n\nWhich of the following variables are discrete?\na. number of employees in a company.\n\n\n\ndistance traveled to and from work\ntaxes paid in 2019\npurchases from 2018\n\n\n\nWhat is the difference between distinct() and count()?\n\n\n\nthey are the same\nb. distinct() gives the levels in a variable, count() gives the marginal distribution\ndistinct() gives the marginal distribution, count() gives the levels in a variable\n\n\n\nWhat operator is used in ggplot?\n\n\n\n“.”\nb. “+”\n“?”\n“!”\n\nCategorical variables\n\nFind the expected count for clarity VS1 and color I for a \\(\\chi^2\\) distribution assuming independence.\n\n\\(8171\\times 5422/53940\\)\n\n\n# A tibble: 8 × 10\n  clarity     D     E     F     G     H     I     J marginal_clarity marginal_…¹\n  <ord>   <int> <int> <int> <int> <int> <int> <int>            <dbl>       <dbl>\n1 I1         42   102   143   150   162    92    50              741        6775\n2 SI2      1370  1713  1609  1548  1563   912   479             9194        9797\n3 SI1      2083  2426  2131  1976  2275  1424   750            13065        9542\n4 VS2      1697  2470  2201  2347  1643  1169   731            12258       11292\n5 VS1       705  1281  1364  2148  1169   962   542             8171        8304\n6 VVS2      553   991   975  1443   608   365   131             5066        5422\n7 VVS1      252   656   734   999   585   355    74             3655        2808\n8 IF         73   158   385   681   299   143    51             1790       53940\n# … with abbreviated variable name ¹​marginal_color\n\n\n[1] 821.3415\n\n\n\nWhat proportion of the very good cut diamonds are colored I?\n0.0997\n\n\n\n# A tibble: 5 × 8\n  cut            D      E      F      G      H      I      J\n  <ord>      <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 Fair      0.0241 0.0229 0.0327 0.0278 0.0365 0.0323 0.0424\n2 Good      0.0977 0.0952 0.0953 0.0771 0.0845 0.0963 0.109 \n3 Very Good 0.223  0.245  0.227  0.204  0.220  0.222  0.241 \n4 Premium   0.237  0.239  0.244  0.259  0.284  0.263  0.288 \n5 Ideal     0.418  0.398  0.401  0.433  0.375  0.386  0.319 \n\n\n\n\n# A tibble: 5 × 8\n# Groups:   cut [5]\n  cut           D     E     F     G     H      I      J\n  <ord>     <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl>\n1 Fair      0.101 0.139 0.194 0.195 0.188 0.109  0.0739\n2 Good      0.135 0.190 0.185 0.178 0.143 0.106  0.0626\n3 Very Good 0.125 0.199 0.179 0.190 0.151 0.0997 0.0561\n4 Premium   0.116 0.169 0.169 0.212 0.171 0.104  0.0586\n5 Ideal     0.132 0.181 0.178 0.227 0.145 0.0971 0.0416\n\n\n\nWhich of the lines below is the mean, median, or mode?\n\n\n\n\n\n\n\nred is median, green is mean\nred is mean, green is mode\nred is mode, green is mean\nd. red is mean, green is median\n\n\n\nIs the mean or median a better measure of center in this distribution.\n\nThe mean is a better estimate in a a symmetric distribution, median in a skewed distribution\n\n\n\n\n\n\nWhich plot is best at displaying outliers:\n\n\n\nhistogram\ndotplot\nplot of proportions\nd. boxplot"
  },
  {
    "objectID": "ae/ae-4.html",
    "href": "ae/ae-4.html",
    "title": "Introduction to probability (AE-4)",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 23 Sept at 2:00pm.\n\n\n\nOne ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan?\nWhat is the probability that the ball will not be cyan?\nInstead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\nNow repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling with replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\nTwo events A and B are independent if Pr(A and B)=Pr(A)P(B). Under which situation are the draws independent?\n\nYou don’t replace the draw.\nYou replace the draw.\nNeither\nBoth\n\nSay you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one is yellow?\nIf you roll a 6-sided die six times, what is the probability of not seeing a 6?"
  },
  {
    "objectID": "ae/ae-2-exploring-categorical-vars.html",
    "href": "ae/ae-2-exploring-categorical-vars.html",
    "title": "Exploring categorical data AE-2",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 16 Sept at 2:00pm.\n\n\n\nLoad packages, data\n\n## loading libraries\nlibrary(tidyverse) # for data analysis and visualisation\nlibrary(here) # to organize files\nlibrary(praise) # for ocassional good vibes!\n\nLet’s look at the data on the web prior to importing it and also to ensure that we know what it should look like when it is imported into R.\nNow press “raw”\n\nThen copy the url:\n\nDownload the file by inserting the URL below:\n\nfs::dir_create(here(\"data\")) # create a data folder \ndownload.file(\n  url = URL_HERE, # url of file to download\n  destfile = here(\"data/comics.csv\") # directory/name_of_file\n)\n\nThen click on the data folder in the “Files” panel.\n\nDouble click on “comics.csv” and click on “Import Dataset…”\n\nCheck the data in the “Data Preview” pane and ensure that it looks as it should.\n\nNext, click on the clipboard icon to copy the code from the “Code Preview” pane, then click “Import”.\n\n#PASTE THE CODE FROM \"Code Preview\" HERE\n\n\n\nData\nThe comics dataset includes basic information scraped from the superheroDb for a kaggle competition.\n\n## An overview of the dataset and type of variables\nglimpse(comics)\n\nThe first superhero is named “A-Bomb” he is male, with yellow eyes, human, with no hair. Marvel Comics created him and his skin color is not listed, but his height and weight are.\n\nAre there any differences between the data on the website above and the one in R? What are they? (Hint: which function can you use to see the whole dataset?)\n\npraise()\n\nThe ___ dataset has ___ observations and ___ variables.\nClassify all variables in the dataset as either: quantitative discrete, quantitative continuous, qualitative nominal, qualitative ordinal. Which of these variables is also categorical?\n\n\n\nCounts\n\nHow many publishers are there in this dataset?\n\ncomics %>% \n  distinct(Publisher) # finds the levels of publisher\n\nHow many levels of the variable Alignment are there in this dataset?\n\ncomics %>% \n  INSERT_FUNCTION(INSERT_VARIABLE)\n\n\nA common way to represent the number of cases that fall into each combination of levels of two categorical variables, such as these, is with what’s called a “contingency table.” Note that each superhero appears in exactly one cell, statisticians call this mutually exclusive.\n\npraise()\n\n\nWhich is the most common category in this contingency table? How many superheros are in that category? (note: please look at both of the next two blocks of code.)\n\ncomics %>% \n  count(Publisher, Alignment) %>% # counts all combinations\n  View()\n\n\nThe long format above is difficult to see clearly. The format we normally use for contingency tables is the wide format.\nThe third line of coding below takes a long table and makes it wide by moving the Alignment variable to the columns, and filling the values of the table with the count variable (n). The names_from argument tells R where the names of the new columns are coming from (i.e. what variable), and the values_from argument tells R where the values in the table are coming from. Here, the values we want in our table are stored as a variable labeled n in our table.\n\ncomics %>% \n  count(Publisher, Alignment) %>%  # counts all combinations\n  pivot_wider(names_from = Alignment, \n              values_from = n) # pivots from long to wide \n\n\nThere are about ___ good superheros for each bad superhero.\n\nggplot(data = comics) + # the dataset\n  geom_bar(mapping = aes(x = Alignment)) # a bar chart of Alignment\n\nLook at the next two plots and find the difference between them. What would make you use one plot over the other? Which plot shows that females are much more likely to be good superheros than bad? Which one shows that good superheros are more likely to be missing gender? And which that females are more likely to be good superheros than males? All of these mean that the variables gender and alignment are associated, the value of one impacts upon the value of the other.\n\nggplot(data = comics) + # add the data\n  geom_bar(mapping = aes(x = Gender, # bar chart of gender\n                         fill = Alignment)) +  # colored by alignment\n  labs(title = \"Gender colored with alignment\") # add title\n\n\nggplot(data = comics) + # add the data\n  geom_bar(mapping = aes(x = Alignment, # bar chart of alignment\n                         fill = Gender)) + # colored by gender\n  labs(title = \"Alignment colored with gender\")  # add title\n\n\nWe note that the commonly held belief that gender is binary is no longer the scientific consensus and that gender is a complex spectrum.\n\nStatisticians want to discuss the strength of association of variables. They want to ensure that this association is not the result of random noise, but instead is a function of the underlying population of interest (all superheros).\n\npraise()\n\n\n\nChi-Squared test\nTo test for independence in 2 categorical variables, statisticians use a Chi-squared test. This is a hypothesis test so it has both a null hypothesis (\\(H_0\\)) and an alternative hypothesis (\\(H_A\\)).\n\\(H_0\\) = the variables are independent, there is no relationship between the two categorical variables.\n\\(H_A\\) = Knowing the value of one variable helps to predict the value of the other variable\nJust like a fair court case, where a defendant is assumed to be innocent until proven guilty, here we assume \\(H_0\\) until proven otherwise. The proof that we present at the case is a formula relating the actual values in our sample to what we would expect to get under the independence assumption.\nTo perform a Chi-squared test of independence on gender and alignment we must find the marginal distributions of both gender and alignment.\n\n\nMarginal distribution\n\nThe marginal distribution is the count of each variable.\n\ncomics %>% \n  count(Alignment, Gender) %>% \n  pivot_wider(names_from = Alignment, \n              values_from = n)\n\n\nWe take the original two-way table (with two categorical variables) and added up the cells across each level of align (ie \\(1+7+19+2 = 29\\)) to get the marginal distribution for each variable.\n\ncomics %>% # the dataset\n  count(Gender) # and marginal distribution of gender\n\n\npraise()\n\n\ncomics %>% # the dataset\n  count(Alignment) # and marginal distribution of the Alignment\n\nIf these are independent, we would expect \\(\\frac{200}{29+200+505}\\) of the \\(207\\) (about \\(56.4\\)) bad superheros to be female, and the same proportion of \\(496\\) (about \\(135.1\\)) good superheros to be female, and so on.\n\nAssuming independence of Alignment and Gender, how many good superheros would we expect to be male? (hint: use the “$$” to make\\(\\frac{1}{2}\\))\n\nMore information, including mathematical notation can be found here.\nFortunately, `R` does all of this for us.\n\n# the \"$\" sign pulls out the Gender values and Alignment value from the comics dataset\nchisq.test(comics$Gender, comics$Alignment)\n\nWe get a warning because the missing values (“-”) expected counts are probably quite small. You can safely use the chi-square test with critical values from the chi-square distribution when no more than 20% of the expected counts are less than \\(5\\) and all individual expected counts are 1 or greater. In particular, all four expected counts in a \\(2 \\times 2\\) table should be 5 or greater.\n\npraise()\n\nThe \\(p-value\\) for this test \\(0.0003298\\) is very small which means that we have very strong evidence against \\(H_0\\) — that there is independence between Alignment and Gender — and we can reject the null hypothesis. This suggests that the association between Gender and Alignment is not likely due to chance.\n\nThere are a few characters with missing data “-” in the Alignment and Gender variables? How many are in each?\n\ncomics %>% \n  INSERT_FUNCTION(INSERT_VARIABLE)\n\n\ncomics %>% \n  INSERT_FUNCTION(INSERT_VARIABLE)\n\nSince there are only a few missing values (<10%) in each category, there is no reason to keep them especially since our analysis is largely about these two variables.\nWhat does the “!=” operator do?\n\ncomics_filtered <- # assign a name to a new dataset\n  comics %>% # tell R which dataset to start with\n  INSERT_FUNCTION(Alignment != \"-\", # remove \"-\" values in Alignment\n                  Gender != \"-\") # remove \"-\" values in Gender\n\nRemake the “Alignment colored with gender” plot and the “Gender colored with alignment” plots with this new dataset.\nRedo the Chi-Squared test for independence between Alignement and Gender. Include all hypotheses, and computations for at least \\(2\\) expected values. Does this change our conclusion?\n\npraise()\n\nSide-by-side barcharts allow us to represent the counts from a contingency table graphically. Telling R to make a side-by-side barchart involves adding the words position = \"dodge” to the coding from above. Now created the other bar chart side-by-side, the “Gender colored with alignment.” use the comics_filter dataset.\n\nggplot(data = INSERT) + # add the data\n  geom_bar(mapping = aes(x = INSERT, # bar chart of alignment\n                         fill = INSERT), # colored by gender\n           position = INSERT) + # side_by_side\n  labs(title = \"Alignment colored with gender\")  # add title\n\n\n\n\nProportions\nWe have been focusing mostly on counts. Proportional data is also interesting to explore and it is easy to get R to produce those for us.\n\nWhat combination of Gender and Alignment has the highest proportion? What is the value? What is the sum of all of the proportions in the table below? (Note: Use the View() function to go through each line of code and ensure you know what each step does.)\n\ncomics %>% # which dataset to use \n  count(Gender, Alignment) %>% # counts all combinations\n  mutate(prop = n / sum(n)) %>% # finds proportions\n  select(Gender, Alignment, prop) %>% # selecting which cols to pivot\n  pivot_wider(names_from = Alignment, # the variable to pivot\n              values_from = prop) # which values to pivot\n\n\n\n\nConditional proportions\n\nIf we’re curious about systematic associations between variables, we should look to conditional proportions. An example of a conditional proportion is the proportion of female superheroes that are good. To build a table of these conditional proportions, we need to specify a grouping variable before we calculate the proportions. What proportion of female characters are good? What do the rows sum to? (Note: Use the View() function to go through each line of code and ensure you know what each step does.)\n\ncomics %>% # which dataset to use \n  count(Gender, Alignment) %>% # counts all combinations\n  group_by(Gender) %>% # conditions on gender\n  mutate(prop = n / sum(n)) %>% # finds proportions\n  select(Gender, Alignment, prop) %>%  # selecting which cols to pivot\n  pivot_wider(names_from = Alignment, # the variable to pivot\n              values_from = prop) # which values to pivot\n\nWhat proportion of good characters are male? What do the columns sum to?\n\ncomics %>% # which dataset to use \n  count(Gender, Alignment) %>% # counts all combinations\n  group_by(Alignment) %>% # conditions on gender\n  mutate(prop = n / sum(n)) %>% # finds proportions\n  select(Gender, Alignment, prop) %>%  # selecting which cols to pivot\n  pivot_wider(names_from = Alignment, # the variable to pivot\n              values_from = prop) # which values to pivot\n\n\npraise()\n\nPlotting proportions is similar to the side-by-side bar chart completed earlier. Instead of position = \"dodge\" we use position = \"fill\" . Draw one such plot below using Gender for the x axis and color it using Alignment.\n\nggplot(data = INSERT) + # add the data\n  geom_bar(mapping = aes(x = INSERT, # bar chart of ?\n                         fill = INSERT), # colored by ?\n           position = INSERT) # fill\n\nNow, change the label of the y axis from count by including the coding\nlabs(y = \"Proportion of superheroes\")\n\npraise()\n\nDoes the above plot condition on gender or alignment? How do you know? (hint: what sums to 1?)\nCondition on the other variable.\n\nggplot(data = INSERT) + # add the data\n  geom_bar(mapping = aes(x = INSERT, # bar chart of ?\n                         fill = INSERT), # colored by ?\n           position = INSERT)  + # fill\n  lab(y = \"INSERT Y AXIS TITLE HERE\")\n\nAnother way to view the differences in variables is to facet. What is the advantage of a facet plot over a stacked barchart?\n\nggplot(comics) + # which dataset\n  geom_bar(mapping = aes(x = Gender)) + # bar chart of Gender\n  facet_wrap(~Alignment) # broken down by Alignment"
  },
  {
    "objectID": "ae/ae-3.html",
    "href": "ae/ae-3.html",
    "title": "Exploring numeric data (AE-3)",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is due on 20 Sept at 2:00pm."
  },
  {
    "objectID": "ae/ae-3.html#dotplot",
    "href": "ae/ae-3.html#dotplot",
    "title": "Exploring numeric data (AE-3)",
    "section": "Dotplot",
    "text": "Dotplot\n\nLook at the coding below and one at a time remove each of the dotsize, alpha, and stackdir from the code. What does each of these inputs do? How does each input impact upon the plot?\n\n\nggplot(data = diamonds) + # the data\n  geom_dotplot(mapping = aes(x = price), # plot of price\n               dotsize = 0.005, # make the dots tiny\n               alpha = 0.3, # less opaque\n               stackdir = \"center\") #"
  },
  {
    "objectID": "ae/ae-3.html#dotplot-vs-boxplot",
    "href": "ae/ae-3.html#dotplot-vs-boxplot",
    "title": "Exploring numeric data (AE-3)",
    "section": "Dotplot vs boxplot",
    "text": "Dotplot vs boxplot\nA boxplot provides summary information for the distribution of a variable. Here are some explanations (using different data) showing the purpose of the box portion of the boxplot.\nThe left line of the box is the quartile 1 (\\(Q_1\\)) or Q1 \n\npraise()\n\nThe middle line of the box is the median (\\(Q_2\\)) \nThe right line of the box is the quartile 3 (\\(Q_3\\)) \n\nLooking at the above plots, count the percentage of yellow dots in each of the dotplots. Is there anything interesting about these values?\nCompare and contrast a dotplot and a boxplot? What are the advantages and disadvantages of each? Think carefully about bimodal data and which plot you would use to show that aspect of a variable. Also think about outliers."
  },
  {
    "objectID": "ae/ae-3.html#boxplots",
    "href": "ae/ae-3.html#boxplots",
    "title": "Exploring numeric data (AE-3)",
    "section": "Boxplots",
    "text": "Boxplots\n\nDraw a boxplot of the price data from the diamonds dataset.\n\n\nggplot(data = INSERT) + # the data\n  INSERT(aes(x = INSERT)) # the variable\n\n\nBoxplot descriptions\n\n\nUse the information above to describe the boxplot of price.\n\n\n\nInterquartile range (IQR)\nThe IQR of a variable is the length of the box in a boxplot. \\[ IQR = Q_3 - Q_1\\]\n\npraise()"
  },
  {
    "objectID": "ae/ae-3.html#density-plot",
    "href": "ae/ae-3.html#density-plot",
    "title": "Exploring numeric data (AE-3)",
    "section": "Density plot",
    "text": "Density plot\n\n ggplot(data = diamonds) + # the data\n       geom_density(aes(x = price), # plot of price\n                    bw = 1) # plot of price\n\n\nWhat does the “bw” input in the geom_density function do? What is an appropriate value of “bw” for this data?\n\n\nggplot(data = diamonds) + # the data\n  geom_density(aes(x = price), # plot of price\n               bw = INSERT) + # binwidth\n  facet_wrap(~clarity) # faceting\n\n\nWhat does the above plot suggest about the prices of diamonds for all levels of the variable clarity?\n\n\npraise()\n\n\nDraw a density plot for each value of cut. Describe the distribution of diamond price for each value of cut.\n\n\nggplot(data = diamonds) + # the data\n  geom_density(aes(x = price), # plot of price\n               bw = INSERT) + # binwidth\n  facet_wrap(~INSERT) # faceting"
  },
  {
    "objectID": "ae/ae-3.html#measures-of-center",
    "href": "ae/ae-3.html#measures-of-center",
    "title": "Exploring numeric data (AE-3)",
    "section": "Measures of center",
    "text": "Measures of center\nWe use the summarise() function when we expect the result to be one number.\n\nMean\n\ndiamonds %>% # the dataset\n  summarise(mean = mean(price)) # function that is used to compute mean of price\n\n9, Now compute the means of the variables \\(x\\), \\(y\\), \\(z\\), and \\(carat\\). (hint: You will need to insert a code chunk for each of these variables.)\n\npraise()\n\nLast class we learned about the group_by() and used it to compute conditional probabilities. Today we will use it to compare the means of different subgroups.\n\ndiamonds %>% # the data\n  group_by(cut) %>% # for each value of cut\n  summarise(mean = mean(price), # find the mean\n            median = median(price)) # and the median\n\n\nCompare the average values of price for each level of the cut variable. Considering the distributions of price for each level of cut in question 8, is the grouped mean a good measure of center for each level of cut? Why or why not?\n\n\n\nMedian\n\nHow would you find the overall median of price?"
  },
  {
    "objectID": "ae/ae-3.html#measures-of-spread",
    "href": "ae/ae-3.html#measures-of-spread",
    "title": "Exploring numeric data (AE-3)",
    "section": "Measures of spread",
    "text": "Measures of spread\n\nVariance\nR also computes measures of spread.\n\ndiamonds %>% # the data\n  summarise(var = var(price)) # find the variance\n\nBesides the direct method above, we can also do this computationally, by using the definition and finding the mean, subtracting, and squaring, then dividing.\n\ndiamonds %>% # the data\n  mutate(\n    mean_price = mean(price), # mean of price\n    deviation = price - mean_price, # subtraction from notes\n    deviation_sq = deviation^2) %>% # then square it\n  summarise(\n    mean_sq_deviation = sum(deviation_sq)/ # sum them\n      (nrow(diamonds) - 1)) # divide by n - 1\n\n\nRun each line of code above and ensure that you understand how this computation was developed. This shows us how to find \\(s_{price}^2\\) directly.\nFind \\(s_{x}^2\\), \\(s_{y}^2\\), and \\(s_{z}^2\\) both directly, and computationally: having R do the computation. (hint: you will need to insert 6 code chunks)\n\n\npraise()\n\n\n\nStandard deviation\n\nR also computes the standard deviation \\(s\\) directly:\n\n\n    diamonds %>% # the data\n      summarise(sd = sd(price)) # find the sd\n\nCan you use the computational approach (finding the mean_sq_deviation) from above to find the standard deviation of price \\(s_{price}\\)? (hint: you will need to use the sqrt() operator)\n\n\nIQR\n\nThe \\(IQR\\) can also be computed directly or indirectly:\n\n\ndiamonds %>% # the data\n  summarise(\n    q1 = quantile(price, 0.25), # find Q1\n    q3 = quantile(price, 0.75), # find Q3\n    iqr = q3 - q1 # now the IQR\n  )\n\n\n\nRange\n\ndiamonds %>% # the data\n  summarise(\n    min = min(price), # find min\n    max = max(price), # find max\n    range = max - min # and range\n  )\n\n\nOf all of the measures of spread, which do you think are sensitive to skewed data and outliers? Why?"
  },
  {
    "objectID": "exams/exam-1.html",
    "href": "exams/exam-1.html",
    "title": "Exam 1",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 4 at 9 am ET and must be completed by Mon, Feb 7 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-1- repo to complete Part 3 of your exam. Add your answers to the exam-1.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 7 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀\n\n\n\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am).\n\n\n\n\n\nPart 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#tips",
    "href": "project-tips-resources.html#tips",
    "title": "Project tips + resources",
    "section": "Tips",
    "text": "Tips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that’s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-tips-resources.html#formatting-communication-tips",
    "href": "project-tips-resources.html#formatting-communication-tips",
    "title": "Project tips + resources",
    "section": "Formatting + communication tips",
    "text": "Formatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\n\nHeaders\n\nUse headers to clearly label each section.\nInspect the document outline to review your headers and sub-headers.\n\n\n\nReferences\n\nInclude all references in a section called “References” at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called “Appendix”.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\nResize plots and figures, so you have more space for the narrative.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf you’re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\n\n❌ NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n✅ YES! Names are readable\n\nggplot(data = mpg, aes(y = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg %>%\n  count(manufacturer) %>%\n  mutate(manufacturer = str_to_title(manufacturer)) %>%\n  ggplot(aes(y = fct_reorder(manufacturer,n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_minimal() \n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel <- lm(mpg ~ hp, data = mtcars)\ntidy(model) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STAT 1010.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document."
  },
  {
    "objectID": "project-tips-resources.html#additional-resources",
    "href": "project-tips-resources.html#additional-resources",
    "title": "Project tips + resources",
    "section": "Additional resources",
    "text": "Additional resources\n\nR for Data Science\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "dr. gwynn sturdevant (she/her/ella) is a Lecturer at Wharton. gwynn’s work focuses on revamping introductory statistics and data science curriculum, connecting recent advances in human computer interaction to statistical programming languages, and community-led research involving those facing entrenched inequalities.\n\n\n\nOffice hours\nLocation\n\n\n\n\nTues 11:15 - 12:15\nARB 327\n\n\nFri 11:00 - 12:00 & Tues 6 - 7pm\nZoom\n\n\n\nIf these times don’t work for you or you’d like to schedule a one-on-one meeting, you can do so by emailing gwynnc@wharton.upenn.edu. Please put STATS 1010 in the subject line."
  },
  {
    "objectID": "slides/lec-1.html#meet-the-professor",
    "href": "slides/lec-1.html#meet-the-professor",
    "title": "Welcome to STA 210!",
    "section": "Meet the professor",
    "text": "Meet the professor\n\n\n\n\n\nDr. Mine Çetinkaya-Rundel (she/her)\n\n\n\n\nProfessor of the Practice & Director of Undergraduate Studies, Department of Statistical Science\nAffiliated Faculty, Computational Media, Arts & Cultures\nFind out more at mine-cr.com"
  },
  {
    "objectID": "slides/lec-1.html#meet-the-tas",
    "href": "slides/lec-1.html#meet-the-tas",
    "title": "Welcome to STA 210!",
    "section": "Meet the TAs",
    "text": "Meet the TAs\n\nMartha Aboagye (she/her, UG)\nRich Fremgen (he/him, MS)\nEmily Gentles (she/her, MS)\nSara Mehta (she/her, UG)\nRick Presman (he/him, PhD)\nShari Tian (she/her, UG)\nAaditya Warrier (he/him, UG)"
  },
  {
    "objectID": "slides/lec-1.html#check-out-conversations",
    "href": "slides/lec-1.html#check-out-conversations",
    "title": "Welcome to STA 210!",
    "section": "Check out Conversations",
    "text": "Check out Conversations\n\nGo to Conversations 💬\nAnswer the discussion question: How are you doing?"
  },
  {
    "objectID": "slides/lec-1.html#what-is-regression-analysis",
    "href": "slides/lec-1.html#what-is-regression-analysis",
    "title": "Welcome to STA 210!",
    "section": "What is regression analysis",
    "text": "What is regression analysis\n\n\n“In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or predictors). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or ‘criterion variable’) changes when any one of the independent variables is varied, while the other independent variables are held fixed.”\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lec-1.html#course-faq",
    "href": "slides/lec-1.html#course-faq",
    "title": "Welcome to STA 210!",
    "section": "Course FAQ",
    "text": "Course FAQ\n\n\nWhat background is assumed for the course? Introductory statistics or probability course.\nWill we be doing computing? Yes. We will use R.\nWill we learn the mathematical theory of regression? Yes and No. The course is primarily focused on application; however, we will discuss some of the mathematics of simple linear regression. The 1-credit course STA 211: Mathematics of Regression you can take simultaneously / after dives into more of the mathematics."
  },
  {
    "objectID": "slides/lec-1.html#course-learning-objectives",
    "href": "slides/lec-1.html#course-learning-objectives",
    "title": "Welcome to STA 210!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\n\nAnalyze real-world data to answer questions about multivariable relationships.\nFit and evaluate linear and logistic regression models.\nAssess whether a proposed model is appropriate and describe its limitations.\nUse Quarto to write reproducible reports and GitHub for version control and collaboration.\nCommunicate results from statistical analyses to a general audience."
  },
  {
    "objectID": "slides/lec-1.html#examples-of-regression-in-practice",
    "href": "slides/lec-1.html#examples-of-regression-in-practice",
    "title": "Welcome to STA 210!",
    "section": "Examples of regression in practice",
    "text": "Examples of regression in practice\n\n\nNew Yorkers Will Pay $56 A Month To Trim A Minute Off Their Commute\nHow FiveThirtyEight’s 2020 Presidential Forecast Works — And What’s Different Because Of COVID-19\nEffect of Forensic Evidence on Criminal Justice Case Processing\nWhy it’s so freaking hard to make a good COVID-19 model"
  },
  {
    "objectID": "slides/lec-1.html#homepage",
    "href": "slides/lec-1.html#homepage",
    "title": "Welcome to STA 210!",
    "section": "Homepage",
    "text": "Homepage\nsta210-s22.github.io/website\n\nAll course materials\nLinks to Sakai, GitHub, RStudio containers, etc.\nLet’s take a tour!"
  },
  {
    "objectID": "slides/lec-1.html#course-toolkit",
    "href": "slides/lec-1.html#course-toolkit",
    "title": "Welcome to STA 210!",
    "section": "Course toolkit",
    "text": "Course toolkit\nAll linked from the course website:\n\nGitHub organization: github.com/sta210-s22\nRStudio containers: cmgr.oit.duke.edu/containers\nDiscussion forum: Conversations\nAssignment submission and feedback: Gradescope\n\n\n\n\n\n\n\nImportant\n\n\nReserve an RStudio Container (titled STA 210) before lab on Monday!"
  },
  {
    "objectID": "slides/lec-1.html#activities-prepare-participate-practice-perform",
    "href": "slides/lec-1.html#activities-prepare-participate-practice-perform",
    "title": "Welcome to STA 210!",
    "section": "Activities: Prepare, Participate, Practice, Perform",
    "text": "Activities: Prepare, Participate, Practice, Perform\n\nPrepare: Introduce new content and prepare for lectures by completing the readings (and sometimes watching the videos)\nParticipate: Attend and actively participate in lectures and labs, office hours, team meetings\nPractice: Practice applying statistical concepts and computing with application exercises during lecture, graded for completion\nPerform: Put together what you’ve learned to analyze real-world data\n\nLab assignments x 7 (first individual, later team-based)\nHomework assignments x 5 (individual)\nThree take-home exams\nTerm project presented during the final exam period"
  },
  {
    "objectID": "slides/lec-1.html#cadence",
    "href": "slides/lec-1.html#cadence",
    "title": "Welcome to STA 210!",
    "section": "Cadence",
    "text": "Cadence\n\n\nLabs: Start and make large progress on Monday in lab section, finish up by Friday 5pm of that week\nHWs: Posted Friday morning, due following Friday 5pm\nExams: Exam review Thursday in class, exam posted Friday morning, no lab on Monday of following week, due Monday 11:59pm\nProject: Deadlines throughout the semester, with some lab and lecture time dedicated to working on them, and most work done in teams outside of class"
  },
  {
    "objectID": "slides/lec-1.html#teams",
    "href": "slides/lec-1.html#teams",
    "title": "Welcome to STA 210!",
    "section": "Teams",
    "text": "Teams\n\nTeam assignments\n\nAssigned by me\nApplication exercises, labs, and project\nPeer evaluation during teamwork and after completion\n\nExpectations and roles\n\nEveryone is expected to contribute equal effort\nEveryone is expected to understand all code turned in\nIndividual contribution evaluated by peer evaluation, commits, etc."
  },
  {
    "objectID": "slides/lec-1.html#grading",
    "href": "slides/lec-1.html#grading",
    "title": "Welcome to STA 210!",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nApplication exercises\n3%\n\n\nHomework\n35% (7% x 5)\n\n\nProject\n15%\n\n\nLab\n14% (2% x 7)\n\n\nExam 01\n10%\n\n\nExam 02\n10%\n\n\nExam 03\n10%\n\n\nTeamwork\n3%\n\n\n\nSee course syllabus for how the final letter grade will be determined."
  },
  {
    "objectID": "slides/lec-1.html#support",
    "href": "slides/lec-1.html#support",
    "title": "Welcome to STA 210!",
    "section": "Support",
    "text": "Support\n\nAttend office hours\nAsk and answer questions on the discussion forum\nReserve email for questions on personal matters and/or grades\nRead the course support page"
  },
  {
    "objectID": "slides/lec-1.html#announcements",
    "href": "slides/lec-1.html#announcements",
    "title": "Welcome to STA 210!",
    "section": "Announcements",
    "text": "Announcements\n\nPosted on Sakai (Announcements tool) and sent via email, be sure to check both regularly\nI’ll assume that you’ve read an announcement by the next “business” day\nI’ll (try my best to) send a weekly update announcement each Friday, outlining the plan for the following week and reminding you what you need to do to prepare, practice, and perform"
  },
  {
    "objectID": "slides/lec-1.html#diversity-inclusion",
    "href": "slides/lec-1.html#diversity-inclusion",
    "title": "Welcome to STA 210!",
    "section": "Diversity + inclusion",
    "text": "Diversity + inclusion\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength and benefit.\n\n\nIf you have a name that differs from those that appear in your official Duke records, please let me know!\nPlease let me know your preferred pronouns. You’ll also be able to note this in the Getting to know you survey.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. I want to be a resource for you. If you prefer to speak with someone outside of the course, your advisers and deans are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "slides/lec-1.html#accessibility",
    "href": "slides/lec-1.html#accessibility",
    "title": "Welcome to STA 210!",
    "section": "Accessibility",
    "text": "Accessibility\n\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments.\nI am committed to making all course materials accessible and I’m always learning how to do this better. If any course component is not accessible to you in any way, please don’t hesitate to let me know."
  },
  {
    "objectID": "slides/lec-1.html#covid-policies",
    "href": "slides/lec-1.html#covid-policies",
    "title": "Welcome to STA 210!",
    "section": "COVID policies",
    "text": "COVID policies\n\nWear a mask at all times!\nRead and follow university guidance"
  },
  {
    "objectID": "slides/lec-1.html#late-work-waivers-regrades-policy",
    "href": "slides/lec-1.html#late-work-waivers-regrades-policy",
    "title": "Welcome to STA 210!",
    "section": "Late work, waivers, regrades policy",
    "text": "Late work, waivers, regrades policy\n\nWe have policies!\nRead about them on the course syllabus and refer back to them when you need it"
  },
  {
    "objectID": "slides/lec-1.html#collaboration-policy",
    "href": "slides/lec-1.html#collaboration-policy",
    "title": "Welcome to STA 210!",
    "section": "Collaboration policy",
    "text": "Collaboration policy\n\nOnly work that is clearly assigned as team work should be completed collaboratively.\nHomeworks must be completed individually. You may not directly share answers / code with others, however you are welcome to discuss the problems in general and ask for advice.\nExams must be completed individually. You may not discuss any aspect of the exam with peers. If you have questions, post as private questions on the course forum, only the teaching team will see and answer."
  },
  {
    "objectID": "slides/lec-1.html#sharing-reusing-code-policy",
    "href": "slides/lec-1.html#sharing-reusing-code-policy",
    "title": "Welcome to STA 210!",
    "section": "Sharing / reusing code policy",
    "text": "Sharing / reusing code policy\n\nWe are aware that a huge volume of code is available on the web, and many tasks may have solutions posted\nUnless explicitly stated otherwise, this course’s policy is that you may make use of any online resources (e.g. RStudio Community, StackOverflow, etc.) but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solution(s).\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source"
  },
  {
    "objectID": "slides/lec-1.html#academic-integrity",
    "href": "slides/lec-1.html#academic-integrity",
    "title": "Welcome to STA 210!",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nTo uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "slides/lec-1.html#most-importantly",
    "href": "slides/lec-1.html#most-importantly",
    "title": "Welcome to STA 210!",
    "section": "Most importantly!",
    "text": "Most importantly!\nAsk if you’re not sure if something violates a policy!"
  },
  {
    "objectID": "slides/lec-1.html#five-tips-for-success",
    "href": "slides/lec-1.html#five-tips-for-success",
    "title": "Welcome to STA 210!",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work before class.\nAsk questions.\nDo the readings.\nDo the homework and lab.\nDon’t procrastinate and don’t let a week pass by with lingering questions."
  },
  {
    "objectID": "slides/lec-1.html#learning-during-a-pandemic",
    "href": "slides/lec-1.html#learning-during-a-pandemic",
    "title": "Welcome to STA 210!",
    "section": "Learning during a pandemic",
    "text": "Learning during a pandemic\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don’t hesitate to ask.\n\n\nYou never owe me personal information about your health (mental or physical) but you’re always welcome to talk to me. If I can’t help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded during this crisis."
  },
  {
    "objectID": "slides/lec-1.html#this-weeks-tasks",
    "href": "slides/lec-1.html#this-weeks-tasks",
    "title": "Welcome to STA 210!",
    "section": "This week’s tasks",
    "text": "This week’s tasks\n\nGet a GitHub account if you don’t have one (some advice for choosing a username here)\nComplete the Getting to know you survey if you haven’t yet done so!\nRead the syllabus\nWatch out for next week’s announcement email, in your inbox sometime tomorrow"
  },
  {
    "objectID": "slides/lec-1.html#midori-says",
    "href": "slides/lec-1.html#midori-says",
    "title": "Welcome to STA 210!",
    "section": "Midori says…",
    "text": "Midori says…"
  },
  {
    "objectID": "slides/01_introduction.html#workspace-image",
    "href": "slides/01_introduction.html#workspace-image",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Workspace image",
    "text": "Workspace image\n\nSaving workspace"
  },
  {
    "objectID": "slides/01_introduction.html#rainbow-parenthesis",
    "href": "slides/01_introduction.html#rainbow-parenthesis",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Rainbow parenthesis",
    "text": "Rainbow parenthesis\n\nParenthesis are important"
  },
  {
    "objectID": "slides/01_introduction.html#code-font",
    "href": "slides/01_introduction.html#code-font",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Code font",
    "text": "Code font\n\nUpdate background and font color of code"
  },
  {
    "objectID": "slides/01_introduction.html#pane-layout",
    "href": "slides/01_introduction.html#pane-layout",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Pane layout",
    "text": "Pane layout\n\nUpdate panes and their layout"
  },
  {
    "objectID": "slides/01_introduction.html#new-directory",
    "href": "slides/01_introduction.html#new-directory",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "New directory",
    "text": "New directory\n\nSelect “New Directory”"
  },
  {
    "objectID": "slides/01_introduction.html#directory-type",
    "href": "slides/01_introduction.html#directory-type",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Directory type",
    "text": "Directory type\n\nThen select “New Project”"
  },
  {
    "objectID": "slides/01_introduction.html#new-folder",
    "href": "slides/01_introduction.html#new-folder",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "New folder",
    "text": "New folder\nHit the “Browse” button, then the “New Folder” button and name it “Stats1010.” All work related to this class will be in this folder, and we will create a new document for each chapter and homework.\n\nthen hit “Create project.”"
  },
  {
    "objectID": "slides/01_introduction.html#name-the-document",
    "href": "slides/01_introduction.html#name-the-document",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Name the document",
    "text": "Name the document\nName the document “lec_01” and hit create\n\nNaming the document"
  },
  {
    "objectID": "slides/01_introduction.html#insert-an-r-code-chuck",
    "href": "slides/01_introduction.html#insert-an-r-code-chuck",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Insert an R code chuck",
    "text": "Insert an R code chuck\nInsert a code chunk\n\nInsert code chunk"
  },
  {
    "objectID": "slides/01_introduction.html#write-your-first-line-of-coding",
    "href": "slides/01_introduction.html#write-your-first-line-of-coding",
    "title": "Lec 1 - Downloading R and plot your first graph",
    "section": "Write your first line of coding",
    "text": "Write your first line of coding\nClick here or the qr code below to write your first line of code\n\n\n\nstat1010-f22.github.io/website"
  },
  {
    "objectID": "slides/lect_09.html#compute-the-sharpe-ratio",
    "href": "slides/lect_09.html#compute-the-sharpe-ratio",
    "title": "Chapter 10: Association between random variables",
    "section": "Compute the Sharpe ratio",
    "text": "Compute the Sharpe ratio\n\nhave them go back to the previous slide and see what they got\n\n\n\n\nCompany\nRandom Variable\nMean per month\nSD\n\n\n\n\nApple\nA\n2.45%\n13.3%\n\n\nMcDonalds\nM\n1.14%\n6.2%\n\n\n\nassume \\(r_f\\) is the risk-free rate of interest is \\(0.1\\%\\)"
  },
  {
    "objectID": "slides/lect_09.html#compute-the-sharpe-ratio-1",
    "href": "slides/lect_09.html#compute-the-sharpe-ratio-1",
    "title": "Chapter 10: Association between random variables",
    "section": "Compute the Sharpe ratio",
    "text": "Compute the Sharpe ratio\n\nwork through it, then what if we don’t know mu and sigma could we find it if we had a pdf?\n\n\n\n\n\n\\[\\begin{aligned}\nS(A) &= \\frac{\\mu_A - r_f}{\\sigma_A}\\\\\n&= \\frac{2.45 - 0.1}{13.3}\\\\\n&=  0.177\n\\end{aligned}\\]\n\\[\\begin{aligned}\nS(M) &= \\frac{\\mu_M - r_f}{\\sigma_M}\\\\\n&= \\frac{1.14 - 0.1}{6.2}\\\\\n&=  0.168\n\\end{aligned}\\]\n\n\n\nWe prefer Apple because it has a higher Sharpe ratio \\(0.177 > 0.168\\)"
  },
  {
    "objectID": "slides/lect_09.html#revision-2",
    "href": "slides/lect_09.html#revision-2",
    "title": "Chapter 10: Association between random variables",
    "section": "Revision 2",
    "text": "Revision 2\n\nfind the mean, stdev, and use them to find the Sharpe ratio\nput microsoft is y on the board and IBM is x\ncopy down the means and st deviation when they get them\n\nInstead of knowing \\(\\mu\\) and \\(\\sigma\\) we have a pdf\n\n\n\n\nIBM stock\nIBM stock\nMicrosoft\nMicrosoft\n\n\n\n\n\n\\(x\\)\n\\(P(X = x)\\)\n\\(y\\)\n\\(P(Y = y)\\)\n\n\nIncreases\n$5\n0.11\n$4\n0.18\n\n\nNo change\n0\n0.80\n0\n0.67\n\n\nDecreases\n-$5\n0.09\n-$4\n0.15"
  },
  {
    "objectID": "slides/lect_09.html#sharpe-ratio-by-hand",
    "href": "slides/lect_09.html#sharpe-ratio-by-hand",
    "title": "Chapter 10: Association between random variables",
    "section": "Sharpe ratio by hand",
    "text": "Sharpe ratio by hand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIBM\nIBM\nIBM\nIBM\nIBM\nIBM\nIBM\n\n\n\n\n\n\\(x\\)\n\\(P(X = x)\\)\n\\(\\mu_X\\)\n\\((x-\\mu_X)^2 \\cdot p_X(x)\\)\n\\(Var(X)\\)\n\\(sd(X)\\)\nSharpe ratio\n\n\nIncreases\n$5\n0.11\n\n\n\n\n\n\n\nNo change\n0\n0.80\n\n\n\n\n\n\n\nDecreases\n-$5\n0.09"
  },
  {
    "objectID": "slides/lect_09.html#sharpe-ratio-by-hand-1",
    "href": "slides/lect_09.html#sharpe-ratio-by-hand-1",
    "title": "Chapter 10: Association between random variables",
    "section": "Sharpe ratio by hand",
    "text": "Sharpe ratio by hand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIBM\nIBM\nIBM\nIBM\nIBM\nIBM\nIBM\n\n\n\n\n\n\\(x\\)\n\\(P(X = x)\\)\n\\(\\mu_X\\)\n\\((x-\\mu_X)^2 \\cdot p_X(x)\\)\n\\(Var(X)\\)\n\\(sd(X)\\)\nSharpe ratio\n\n\nIncreases\n$5\n0.11\n0.1\n2.6411\n4.99\n2.23\n0.03805123\n\n\nNo change\n0\n0.80\n0.1\n0.0080\n4.99\n2.23\n0.03805123\n\n\nDecreases\n-$5\n0.09\n0.1\n2.3409\n4.99\n2.23\n0.03805123"
  },
  {
    "objectID": "slides/lect_09.html#sharpe-ratio-by-hand-2",
    "href": "slides/lect_09.html#sharpe-ratio-by-hand-2",
    "title": "Chapter 10: Association between random variables",
    "section": "Sharpe ratio by hand",
    "text": "Sharpe ratio by hand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMCSFT\nMCSFT\nMCSFT\nMCSFT\nMCSFT\nMCSFT\nMCSFT\n\n\n\n\n\n\\(y\\)\n\\(P(Y =y)\\)\n\\(\\mu_Y\\)\n\\((y-\\mu_Y)^2 \\cdot p_Y(y)\\)\n\\(Var(Y)\\)\n\\(sd(Y)\\)\nSharpe ratio\n\n\nIncreases\n$4\n0.18\n\n\n\n\n\n\n\nNo change\n0\n0.67\n\n\n\n\n\n\n\nDecreases\n-$4\n0.15"
  },
  {
    "objectID": "slides/lect_09.html#sharpe-ratio-by-hand-3",
    "href": "slides/lect_09.html#sharpe-ratio-by-hand-3",
    "title": "Chapter 10: Association between random variables",
    "section": "Sharpe ratio by hand",
    "text": "Sharpe ratio by hand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMCSFT\nMCSFT\nMCSFT\nMCSFT\nMCSFT\nMCSFT\nMCSFT\n\n\n\n\n\n\\(y\\)\n\\(P(Y =y)\\)\n\\(\\mu_Y\\)\n\\((y-\\mu_Y)^2 \\cdot p_Y(y)\\)\n\\(Var(Y)\\)\n\\(sd(Y)\\)\nSharpe ratio\n\n\nIncreases\n$4\n0.18\n0.12\n2.709792\n5.2656\n2.29469\n0.04575782\n\n\nNo change\n0\n0.67\n0.12\n0.009648\n5.2656\n2.29469\n0.04575782\n\n\nDecreases\n-$4\n0.15\n0.12\n2.546160\n5.2656\n2.29469\n0.04575782"
  },
  {
    "objectID": "slides/lect_09.html#the-sharpe-ratio---in-r",
    "href": "slides/lect_09.html#the-sharpe-ratio---in-r",
    "title": "Chapter 10: Association between random variables",
    "section": "The Sharpe ratio - in R",
    "text": "The Sharpe ratio - in R\n\nCan an investor do better by diversifing the investment?\n\n\n# Input the data and the pdf\nstock <- tibble(x = c(5, 0, -5), \n                p_x = c(0.11, 0.8, 0.09), \n                y = c(4, 0, -4), \n                p_y = c(.18, .67, .15))\n\n\nSharpe_ratio <- # name this the Sharpe ratio\n  stock %>% # use the data from above\n  mutate(part_mean_x = x*p_x, # find mean for each part of x \n         part_mean_y = y*p_y, # now for y\n         mean_x = sum(part_mean_x), # find mean x \n         mean_y = sum(part_mean_y),# now for y\n         part_var_x = p_x * (mean_x - x)^2, # find var for each part of x \n         part_var_y = p_y * (mean_y - y)^2,# now for y\n         var_x = sum(part_var_x), # find var of x \n         var_y = sum(part_var_y),# now for y\n         sd_x = sqrt(var_x), # find sd of x \n         sd_y = sqrt(var_y)) %>% # now for y\n  summarise(S_x = (mean_x - 0.015)/sd_x, # find Sharpe ratio of x with rf = 0.015\n            S_y = (mean_y - 0.015)/sd_y) %>% # now for y\n  slice(1)"
  },
  {
    "objectID": "slides/lect_09.html#expected-value-of-x-y",
    "href": "slides/lect_09.html#expected-value-of-x-y",
    "title": "Chapter 10: Association between random variables",
    "section": "Expected value of \\(X + Y\\)",
    "text": "Expected value of \\(X + Y\\)\n\nIf x=-5 then we subtract 5 from all y’s and multiply by \\(p_{x, y}\\) algebraically it all washes out\nWhat about independence? We want to find the standard deviation to compute the Sharpe Ratio\n\n\n\n\n\n\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\n\n\n\n\n\n\\(x = -5\\)\n\\(x = 0\\)\n\\(x = 5\\)\n\\(p(y)\\)\n\n\n\\(Y\\)\n\\(y=4\\)\n\\(0.00\\)\n\\(0.11\\)\n\\(0.07\\)\n\\(0.18\\)\n\n\n\\(Y\\)\n\\(y=0\\)\n\\(0.03\\)\n\\(0.62\\)\n\\(0.02\\)\n\\(0.67\\)\n\n\n\\(Y\\)\n\\(y=-4\\)\n\\(0.06\\)\n\\(0.07\\)\n\\(0.02\\)\n\\(0.15\\)\n\n\n\n\\(p(x)\\)\n\\(0.09\\)\n\\(0.80\\)\n\\(0.11\\)\n\\(1\\)\n\n\n\n\n\n\\(E(X+Y) = E(X) + E(Y)\\)\n\\(E(X+Y) = E(X) + E(Y) = 0.1 + 0.12 = 0.22\\)"
  },
  {
    "objectID": "slides/lect_09.html#are-x-and-y-independent",
    "href": "slides/lect_09.html#are-x-and-y-independent",
    "title": "Chapter 10: Association between random variables",
    "section": "Are \\(X\\) and \\(Y\\) independent?",
    "text": "Are \\(X\\) and \\(Y\\) independent?\n\nfind any p(x) X p(y) != p(x,y)\nBut we still need to find the stdev in order to find Sharpes ratio\n\n\n\n\n\n\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\n\n\n\n\n\n\\(x = -5\\)\n\\(x = 0\\)\n\\(x = 5\\)\n\\(p(y)\\)\n\n\n\\(Y\\)\n\\(y=4\\)\n\\(0.00\\)\n\\(0.11\\)\n\\(0.07\\)\n\\(0.18\\)\n\n\n\\(Y\\)\n\\(y=0\\)\n\\(0.03\\)\n\\(0.62\\)\n\\(0.02\\)\n\\(0.67\\)\n\n\n\\(Y\\)\n\\(y=-4\\)\n\\(0.06\\)\n\\(0.07\\)\n\\(0.02\\)\n\\(0.15\\)\n\n\n\n\\(p(x)\\)\n\\(0.09\\)\n\\(0.80\\)\n\\(0.11\\)\n\\(1\\)\n\n\n\n\n\n\\(p_x(-5) \\cdot p_y(4) = 0.18 \\cdot 0.09 \\neq 0.00\\)\n\\(p_x(-5) \\cdot p_y(-4) = 0.09 \\cdot 0.15 = 0.0135 \\neq 0.06\\)\nNO! NO! NO!"
  },
  {
    "objectID": "slides/lect_09.html#covariance-of-sum",
    "href": "slides/lect_09.html#covariance-of-sum",
    "title": "Chapter 10: Association between random variables",
    "section": "Covariance of sum",
    "text": "Covariance of sum\n\nbecause we square things and \\((a + b)^2 = a^2 + b^2 + 2ab\\) we have to add 2 of the covariance\n\n\n\n\\((a+b)^2 = a^2 + 2ab +b^2\\)\nit follows that\n\\(Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)\\)"
  },
  {
    "objectID": "slides/lect_09.html#correlation",
    "href": "slides/lect_09.html#correlation",
    "title": "Chapter 10: Association between random variables",
    "section": "Correlation",
    "text": "Correlation\n\nwhat about our joint pdf? is it positively or negatively correlated? 3D plot\n\n\n\n\\(corr(x, y) = \\frac{cov(x, y)}{s_x \\cdot s_y}\\)\n\\[\\rho = Corr(X, Y) = \\frac{Cov(X, Y)}{\\sigma_X \\cdot \\sigma_Y}\\]\n\nAs \\(X\\) increases, what happens to \\(Y\\)?\n\n\n\n\n\n\n\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\n\n\n\n\n\n\\(x = -5\\)\n\\(x = 0\\)\n\\(x = 5\\)\n\\(p(y)\\)\n\n\n\\(Y\\)\n\\(y=4\\)\n\\(0.00\\)\n\\(0.11\\)\n\\(0.07\\)\n\\(0.18\\)\n\n\n\\(Y\\)\n\\(y=0\\)\n\\(0.03\\)\n\\(0.62\\)\n\\(0.02\\)\n\\(0.67\\)\n\n\n\\(Y\\)\n\\(y=-4\\)\n\\(0.06\\)\n\\(0.07\\)\n\\(0.02\\)\n\\(0.15\\)\n\n\n\n\\(p(x)\\)\n\\(0.09\\)\n\\(0.80\\)\n\\(0.11\\)\n\\(1\\)"
  },
  {
    "objectID": "slides/lect_09.html#independence-cov",
    "href": "slides/lect_09.html#independence-cov",
    "title": "Chapter 10: Association between random variables",
    "section": "Independence & Cov",
    "text": "Independence & Cov\nHow does independence impact upon correlation and covariance?\n\\[ E((X-\\mu_X)(Y-\\mu_Y))\\]\n\n\n\\[\\begin{aligned}\nCov(X,Y) &= E((X-\\mu_X)(Y-\\mu_Y))\\\\\n&= E(X - \\mu_X)E(Y-\\mu_Y)\\\\\n&= 0\n\\end{aligned}\\]\nIf \\(X\\),\\(Y\\) are independent, then \\(Cov(X, Y) = 0\\)\nThe opposite is not true (if \\(Cov(X, Y) = 0\\), then \\(X\\),\\(Y\\) are independent)"
  },
  {
    "objectID": "slides/lect_09.html#independence-var",
    "href": "slides/lect_09.html#independence-var",
    "title": "Chapter 10: Association between random variables",
    "section": "Independence & Var",
    "text": "Independence & Var\n\\(Var(X) + Var(Y) = Var(X) + Var(Y) + 2Cov(X, Y)\\)\n\n\nIf \\(X\\), \\(Y\\) are independent, \\(Cov(X, Y) = 0\\), so\n\\[Var(X+ Y) = Var(X) + Var(Y)\\]"
  },
  {
    "objectID": "slides/lect_09.html#sharpe-ratio-of-a-sum",
    "href": "slides/lect_09.html#sharpe-ratio-of-a-sum",
    "title": "Chapter 10: Association between random variables",
    "section": "Sharpe ratio of a sum",
    "text": "Sharpe ratio of a sum\n\\[ S(X + Y) = \\frac{(\\mu_X + \\mu_Y) - 2r_f}{\\sqrt{(Var(X+Y))}} \\]"
  },
  {
    "objectID": "slides/lect_09.html#sharpe-ratio-of-sum---r",
    "href": "slides/lect_09.html#sharpe-ratio-of-sum---r",
    "title": "Chapter 10: Association between random variables",
    "section": "Sharpe ratio of sum - R",
    "text": "Sharpe ratio of sum - R\n\nWhat if we want to invest in two subsequent days instead of investing in two stocks for 1 day?\n\n\n# input x & y\nx <- c(-5, 0, 5)\ny <- c(4, 0, -4)\n\n# input data\ndata <- bind_cols(expand_grid(x, y), \n                  probs = c(0, .03, .06, \n                            .11, .62, .07, \n                            .07, .02, .02), \n                  mu_x = rep(0.1, 9), \n                  mu_y = rep(0.12, 9), \n                  var_x = rep(4.99, 9), \n                  var_y = rep(5.2656, 9))\n\ndata %>% \n  mutate(cov_xy_part = (x - mu_x)*\n           (y - mu_y)*probs, # multiply together\n         cov_xy = sum(cov_xy_part), # sum them\n         var_xy = var_x + var_y + 2*cov_xy) %>% # using the rule\n  summarise(S_r = (mu_x + mu_y - 2*0.015)/\n                      (sqrt(var_xy))) %>% # find the Sharpe value\n  slice(1) # only the first\n\n# A tibble: 1 × 1\n     S_r\n   <dbl>\n1 0.0497\n\n\n\n\nThe Sharpe ratio for \\(X\\) is \\(0.038\\), for \\(Y\\) it’s \\(0.046\\), investing in both gives a better return \\(0.050\\)"
  },
  {
    "objectID": "slides/lect_09.html#double-for-one-day",
    "href": "slides/lect_09.html#double-for-one-day",
    "title": "Chapter 10: Association between random variables",
    "section": "Double for one day",
    "text": "Double for one day\n\\[S(2X) = \\frac{2\\mu_X - 2r_f}{\\sqrt{Var(2X)}}\\]\n\n\n\\(\\mu_X = 0.1\\)\n\\(Var(X) = 4.99\\)\n\\(r_f = .015\\)\n\\[\\begin{aligned}\n  &= \\frac{2 \\cdot 0.1 - 2 \\cdot 0.015}{\\sqrt{4 \\cdot 4.99}}\\\\\n  &= \\frac{.2 - 0.03}{\\sqrt{19.96}}\\\\\n  &= 0.038\n  \\end{aligned}\\]\nThis is the same as before, how do we compute if we invest for two subsequent days?"
  },
  {
    "objectID": "slides/lect_09.html#iid",
    "href": "slides/lect_09.html#iid",
    "title": "Chapter 10: Association between random variables",
    "section": "IID",
    "text": "IID\nIf we invested in a stock on 2 subsequent days instead of investing in 2 stocks on one day the return on those two days are:\nindependent and identically distributed (IID)\n\n\nidentically distributed - the outcomes on each day are likely to be different, but the probability of the outcomes is the same\nindependent - very common assumption for stocks (part of the reason for the 2008 financial crises)"
  },
  {
    "objectID": "slides/lect_09.html#addition-rules-for-iid-variables",
    "href": "slides/lect_09.html#addition-rules-for-iid-variables",
    "title": "Chapter 10: Association between random variables",
    "section": "Addition rules for IID variables",
    "text": "Addition rules for IID variables\n\n\n\n\n\n\nImportant\n\n\nIf \\(n\\) random variables \\((X_1, X_2, ..., X_n)\\) are iid with mean \\(\\mu_X\\) and standard deviation \\(\\sigma_X\\), then\n\\[ E(X_1 + X_2 + ... + X_n) = n \\cdot \\mu_X \\] \\[ Var(X_1 + X_2 + ... + X_n) = n \\cdot \\sigma^2_x \\] \\[ SD(X_1 + X_2 + ... + X_n) = \\sqrt{n} \\sigma_X \\]"
  },
  {
    "objectID": "slides/lect_09.html#sharpe-ratio---two-days",
    "href": "slides/lect_09.html#sharpe-ratio---two-days",
    "title": "Chapter 10: Association between random variables",
    "section": "Sharpe ratio - two days",
    "text": "Sharpe ratio - two days\n\nif you decide to leave money in the bank, you may end up with a constant\n\n\\[ S(X_1 + X_2) = \\frac{2\\mu_X - 2r_f}{\\sqrt{2 \\sigma^2_x}}\\]\n\n\n\\[\\begin{aligned}\n&= \\frac{0.20 - 0.03}{\\sqrt{9.98}}\\\\\n&= 0.054\n\\end{aligned}\\]\nyou expect more variance if you have two stock for one day b/c anything that happens that day is magnified\nif you have one stock for two days you reduce the variance"
  },
  {
    "objectID": "slides/lect_09.html#expectation-of-weighted-sum",
    "href": "slides/lect_09.html#expectation-of-weighted-sum",
    "title": "Chapter 10: Association between random variables",
    "section": "Expectation of weighted sum",
    "text": "Expectation of weighted sum\n\\[ E(2X + 4Y + 0.06)\\]\n\n\n\n\\[\\begin{aligned}\n&= 2E(X) + 4E(Y) + 0.06\\\\\n&= 2(0.10) + 4(0.12) + 0.06\\\\\n&= \\$0.74\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lect_09.html#variance-of-weighted-sum",
    "href": "slides/lect_09.html#variance-of-weighted-sum",
    "title": "Chapter 10: Association between random variables",
    "section": "Variance of weighted sum",
    "text": "Variance of weighted sum\n\\[ Var(2X + 4Y + 0.06)\\]\n\n\n\\[\\begin{aligned}\n&= 2^2Var(X) + 4^2Var(Y) + 2 \\cdot(2 \\cdot 4) \\cdot Cov(X,Y)\\\\\n&= 4(4.99) + 16(5.27) + 16(2.19)\\\\\n&= 139.32\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lect_08.html#price-vs-carat",
    "href": "slides/lect_08.html#price-vs-carat",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Price vs carat",
    "text": "Price vs carat"
  },
  {
    "objectID": "slides/lect_08.html#visual-association-test",
    "href": "slides/lect_08.html#visual-association-test",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Visual association test",
    "text": "Visual association test\n\nAsk them to run the coding in RStudio and see what they get - can connect some of their computers to the big screen\nCan they see a difference between their plots and the real plot? What is the difference?\n\n\n\n\n\n\n\n\n\n\ndiamonds %>% # filtered data\n  slice_sample(n = nrow(.)) %>% # random sample rows\n  pull(carat) %>% # take out the variable carat\n  bind_cols(., diamonds$price) %>% # price in the same order and bound to carat in different order\n  ggplot() + # into ggplot\n  geom_point(aes(y = ...2, x = ...1)) + # using the new names\n  labs(title = \"Simulated association test\", \n       x = \"Weight of diamond in carat\", \n       y = \"Price of diamonds in US$\")"
  },
  {
    "objectID": "slides/lect_08.html#describing-a-scatter-plot",
    "href": "slides/lect_08.html#describing-a-scatter-plot",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Describing a scatter plot",
    "text": "Describing a scatter plot\n\nWhat is the difference between the actual plot and the simulated plots?\nDiscuss each point.\nBig words make you sound smart to people that aren’t that smart.\n\n\n\n\nTrend or direction\n\npositive\nnegative\n\nCurvature\n\nlinear\nnonlinear\n\nexponential\nquadratic\n\n\n\n\n\nVariation\n\nhomoscedasticity (similar variance)\nheteroscedasticity (different variance)\n\nOutliers\n\nany weird points (explore these)\n\nGroupings"
  },
  {
    "objectID": "slides/lect_08.html#describe-these-plots",
    "href": "slides/lect_08.html#describe-these-plots",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Describe these plots",
    "text": "Describe these plots\n\nWhat are the relationships between each plot?\nWhat do the numbers mean?"
  },
  {
    "objectID": "slides/lect_08.html#measuring-association-1",
    "href": "slides/lect_08.html#measuring-association-1",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Measuring association",
    "text": "Measuring association\n\nRemind them of the variance and what we did with the squares. What squares could we use now?"
  },
  {
    "objectID": "slides/lect_08.html#covariance",
    "href": "slides/lect_08.html#covariance",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Covariance",
    "text": "Covariance\n\nIf the product is negative then there it contributes to a negative trend - downhill from left to right.\nSame if positive\nhard to get ones that are negative for this dataset b/c tbere is a really strong positive association"
  },
  {
    "objectID": "slides/lect_08.html#covariance-math",
    "href": "slides/lect_08.html#covariance-math",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Covariance math",
    "text": "Covariance math\n\nThis is a measure of the squares. We’d like a measure that is one dimensional. We want to look at more than just squares.\n\n\\(cov(x, y) = \\frac{(x_1 - \\bar{x})(y_1 - \\bar{y}) + (x_2 - \\bar{x})(y_2 - \\bar{y}) + \\ldots + (x_n - \\bar{x})(y_n - \\bar{y})}{n-1}\\)"
  },
  {
    "objectID": "slides/lect_08.html#correlation-math",
    "href": "slides/lect_08.html#correlation-math",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Correlation math",
    "text": "Correlation math\n\nStandardizes the strength of the association\n\n\\[corr(x, y) = \\frac{cov(x, y)}{s_x \\cdot s_y}\\]"
  },
  {
    "objectID": "slides/lect_08.html#correlation-characteristics",
    "href": "slides/lect_08.html#correlation-characteristics",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Correlation characteristics",
    "text": "Correlation characteristics\n\nStandardizes the strength of the association\n\n\n\nReferred to as \\(r\\)\nStrength of linear association\n\\(r\\) is always between \\(-1\\) and \\(+1\\), \\(-1 \\leq r \\leq 1\\).\n\\(r\\) does not have units"
  },
  {
    "objectID": "slides/lect_08.html#computing-in-r",
    "href": "slides/lect_08.html#computing-in-r",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Computing in R",
    "text": "Computing in R\n\nStandardizes the strength of the association\n\n\n# covariance of price and carat\ncov(diamonds$price, diamonds$carat)\n\n[1] 1742.765\n\n# correlation of price and carat\ncor(diamonds$price, diamonds$carat)\n\n[1] 0.9215913\n\n# coding for the pairs plots\n# library(GGally)\n# diamonds %>% # dataset\n#  select_if(is.numeric) %>% # numeric variables\n#  filter(y < 20, # y less than 20\n#         z < 20, # z less than 20\n#         table < 90) %>% # table less than 90\n#  ggpairs(.) # make the plot"
  },
  {
    "objectID": "slides/lect_08.html#gradient-and-slope",
    "href": "slides/lect_08.html#gradient-and-slope",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Gradient and slope",
    "text": "Gradient and slope\n\nPull out your calculators and work on this\n\n\\[ m = \\frac{r \\cdot s_y}{s_x} \\] \\[ b = \\bar{y} - m \\bar{x} \\]"
  },
  {
    "objectID": "slides/lect_08.html#fitting-by-hand",
    "href": "slides/lect_08.html#fitting-by-hand",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Fitting by hand",
    "text": "Fitting by hand\n\n\n\\(r = 0.9215913\\)\n\\(s_{price} = 3989.44\\)\n\\(s_{carat} = 0.4740112\\)\n\\(m = \\frac{r \\cdot s_{price}}{s_{carat}}\\)\n\\[\\begin{aligned}\n   m &= \\frac{r \\cdot s_{price}}{s_{carat}} \\\\\n&= \\frac{0.9215913 \\cdot 3989.44}{0.4740112} \\\\\n&= 7756.426\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lect_08.html#fitting-by-hand-1",
    "href": "slides/lect_08.html#fitting-by-hand-1",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Fitting by hand",
    "text": "Fitting by hand\n\n\n\\(b = \\overline{price} - m \\cdot \\overline{carat}\\)\n\\(\\overline{price} = 3932.8\\)\n\\(\\overline{carat} = 0.7979397\\)\n\\(m = 7756.426\\)\n\\[\\begin{aligned}\nb &= \\overline{price} - m \\cdot \\overline{carat} \\\\\n&= 3932.8 - 7756.426 \\cdot 0.7979397\\\\\n&= -2256.36\n\\end{aligned}\\]\nOur model is: \\(\\widehat{\\text{price}} = -2256.36 + 7756.426 \\cdot \\text{carat}\\)"
  },
  {
    "objectID": "slides/lect_08.html#prediction-by-hand",
    "href": "slides/lect_08.html#prediction-by-hand",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Prediction by hand",
    "text": "Prediction by hand\n\nPull out your calculators and work on this\n\nfor carat values \\(2.5\\)\n\n\n\\(\\widehat{\\text{price}} = -2256.36 + 7756.426 \\cdot \\text{carat}\\)\n\\[\\begin{aligned}\n\\widehat{\\text{price}}  &= -2256.36 + 7756.426 \\cdot \\text{carat} \\\\\n&= -2256.36 + 7756.426 \\cdot 2.5 \\\\\n&= \\$17135\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lect_08.html#fit-predict-in-r",
    "href": "slides/lect_08.html#fit-predict-in-r",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Fit & predict in R",
    "text": "Fit & predict in R\n\nfirst use of tidymodels package\n\n\n# library tidymodels\nlibrary(tidymodels)\n\n## Assign the least\n## squares line\nleast_squares_fit <- \n  linear_reg() %>% \n  set_engine(\"lm\") %>% \n  fit(price ~ carat, data = diamonds) \n## NOTE: outcome first, predictor second\n\n## Find the prediction\npredict(least_squares_fit, tibble(carat = c(1, 2, 2.5, 4)))\n\n# A tibble: 4 × 1\n   .pred\n   <dbl>\n1  5500.\n2 13256.\n3 17135.\n4 28769."
  },
  {
    "objectID": "slides/lect_08.html#warnings",
    "href": "slides/lect_08.html#warnings",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "Warnings",
    "text": "Warnings\n\n\nALWAYS draw plots first\nnumeric variables\nLinear relationship\nCheck for outliers\nLurking variables"
  },
  {
    "objectID": "slides/lect_08.html#more-spurious-correlations",
    "href": "slides/lect_08.html#more-spurious-correlations",
    "title": "Chapter 6: Association between quantitative variables",
    "section": "More spurious correlations",
    "text": "More spurious correlations\n\n\nThese variables are called lurking variables or confounders.\nLurking variables are not considered in the statistical analysis\nConfounders are considered\nThere are both known and unknown confounders, uknown confounders are lurking variables.\n\n\nClick here for more spurious correlations"
  },
  {
    "objectID": "slides/lect_10.html#ebernoulli",
    "href": "slides/lect_10.html#ebernoulli",
    "title": "Chapter 11: Probability models for counts",
    "section": "E(Bernoulli)",
    "text": "E(Bernoulli)\n\n\n\\(E(B) = 0 \\cdot P(B=0) + 1 \\cdot P(B=1)\\)\n\\[\\begin{aligned}\nE(B) &= 0 \\cdot P(B=0) + 1 \\cdot P(B=1)\\\\\n&= 0 \\cdot (1-p) + 1 \\cdot p\\\\\n&= p\n\\end{aligned}\\]\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_10.html#varbernoulli",
    "href": "slides/lect_10.html#varbernoulli",
    "title": "Chapter 11: Probability models for counts",
    "section": "Var(Bernoulli)",
    "text": "Var(Bernoulli)\n\nWe usually don’t want this information about 1 person, we want it about a whole plane full of people, or a whole town.\nIn that case, we sum together \\(n\\) independent Bernoulli trials.\n\n\n\n\\(Var(B) = (0 - p)^2 \\cdot P(B=0) + (1 - p)^2 \\cdot P(B=1)\\)\n\\[\\begin{aligned}\nVar(B) &= (0 - p)^2 \\cdot P(B=0) + (1 - p)^2 \\cdot P(B=1)\\\\\n&= p^2 \\cdot (1-p) + (1 - p)^2 \\cdot p\\\\\n&= p^2 - p^3 + p(1 - 2p + p^2) \\\\\n&= p^2 - p^3 + p - 2p^2 + p^3\\\\\n&= p(1-p)\n\\end{aligned}\\]\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_10.html#multiple-bernoulli-trials",
    "href": "slides/lect_10.html#multiple-bernoulli-trials",
    "title": "Chapter 11: Probability models for counts",
    "section": "Multiple Bernoulli trials",
    "text": "Multiple Bernoulli trials\n\nThis happens so often that we have another name for this distribution\n\n\\[Y = B_1 + B_2 + ...  + B_n\\]\n\n\n\\(Y\\) is the sum of independent and identically distributed random variables\nWhat is the mean and variance?"
  },
  {
    "objectID": "slides/lect_10.html#ebinomial",
    "href": "slides/lect_10.html#ebinomial",
    "title": "Chapter 11: Probability models for counts",
    "section": "E(Binomial)",
    "text": "E(Binomial)\n\\[E(Y) = E(B_1 + B_2 + ... + B_n)\\]\n\n\n\\[\\begin{aligned}\nE(Y) &= E(B_1) + E(B_2) + ... + E(B_n) \\\\\n&= p + p + ... + p \\\\\n&= np\n\\end{aligned}\\]\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_10.html#varbinomial",
    "href": "slides/lect_10.html#varbinomial",
    "title": "Chapter 11: Probability models for counts",
    "section": "Var(Binomial)",
    "text": "Var(Binomial)\n\nthere are multiple ways that we can have 5 successes, so we must also learn to count\n\n\\[Var(Y) = Var(B_1 + B_2 + ... + B_n)\\]\n\n\n\\[\\begin{aligned}\nVar(Y) &= Var(B_1) + Var(B_2) + ... + Var(B_n) \\\\\n&= p(1-p) + p(1-p) + ... + p(1-p) \\\\\n&= np(1-p)\n\\end{aligned}\\]\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_10.html#counting-things",
    "href": "slides/lect_10.html#counting-things",
    "title": "Chapter 11: Probability models for counts",
    "section": "Counting things",
    "text": "Counting things\n\nwe just went into groups how many groups are possible?\n\nIn a class of 85 students how many groups of 4 are possible?\n\n\n\\(85 \\cdot 84 \\cdot 83 \\cdot 82\\)\n\\(48,594,840\\) groups\nIf we don’t care about order\n\\(\\frac{85 \\cdot 84 \\cdot 83 \\cdot 82}{4 \\cdot 3 \\cdot 2 \\cdot 1}\\)\n\\(2,024,785\\) groups\nCombination written as \\({}_{n}C_{k}\\) here \\(n = 85\\), \\(k = 4\\) or \\(\\binom{85}{4}\\)\n\n\n\nexp(lfactorial(85))/exp(lfactorial(81))\n\n[1] 48594840\n\nexp(lfactorial(85))/(exp(lfactorial(81))*exp(lfactorial(4)))\n\n[1] 2024785"
  },
  {
    "objectID": "slides/lect_10.html#binomial-pdf",
    "href": "slides/lect_10.html#binomial-pdf",
    "title": "Chapter 11: Probability models for counts",
    "section": "Binomial pdf",
    "text": "Binomial pdf\nIf \\(Y \\sim Bin(n, p)\\) where\n\n$n = $ number of trials\n$p = $ probability of success\n$y = $ number of successes in \\(n\\) Bernoulli trials\n\n\nthen  \\[P(Y = y) = \\binom{n}{y}p^y(1-p)^{n-y}\\]"
  },
  {
    "objectID": "slides/lect_10.html#example",
    "href": "slides/lect_10.html#example",
    "title": "Chapter 11: Probability models for counts",
    "section": "Example",
    "text": "Example\n\n\nLet \\(Y \\sim Bin(n = 5, p = 0.2)\\) find the \\(E(Y)\\) and \\(Var(Y)\\)\n\n\n\n\\(E(Y) = np = 1\\)\n\\(Var(Y) = np(1-p) = 0.8\\)\nWhat is the probability that \\(y = 3\\)? In R: dbinom(size  = 5, prob = 0.2, x = 3) \\(0.0512\\)\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_10.html#warnings",
    "href": "slides/lect_10.html#warnings",
    "title": "Chapter 11: Probability models for counts",
    "section": "Warnings",
    "text": "Warnings\n10% Condition: if trials are selected at random, it is OK to ignore dependence caused by sampling from a finite population if the selected trials make up less than 10% of the population"
  },
  {
    "objectID": "slides/lect_10.html#limit-of-p--mins",
    "href": "slides/lect_10.html#limit-of-p--mins",
    "title": "Chapter 11: Probability models for counts",
    "section": "Limit of \\(p\\)- mins",
    "text": "Limit of \\(p\\)- mins\n\nsome of these cars may come by in the same minute..\n\nI know about 7 cars drive by my house in an hour, what’s the probability 18 cars drive by in the next 60 mins?\n\n\n\\(p = \\frac{7}{60}\\)\n\\(Y \\sim Bin(60, \\frac{7}{60})\\)\n\\(\\binom{60}{18}p^{18}(1-p)^{42}\\)"
  },
  {
    "objectID": "slides/lect_10.html#limit-of-p--secs",
    "href": "slides/lect_10.html#limit-of-p--secs",
    "title": "Chapter 11: Probability models for counts",
    "section": "Limit of \\(p\\)- secs",
    "text": "Limit of \\(p\\)- secs\nI know about 7 cars drive by my house in an hour, what’s the probability 18 cars drive by in the next 3600 secs?\n\n\n\\(p = \\frac{7}{3600}\\)\n\\(Y \\sim Bin(3600, \\frac{7}{3600})\\)\n\\(\\binom{3600}{18}p^{18}(1-p)^{3582}\\)"
  },
  {
    "objectID": "slides/lect_10.html#limit-of-p---smallest-interval",
    "href": "slides/lect_10.html#limit-of-p---smallest-interval",
    "title": "Chapter 11: Probability models for counts",
    "section": "Limit of \\(p\\) - smallest interval",
    "text": "Limit of \\(p\\) - smallest interval\nI know about 7 cars drive by my house in an hour, what’s the probability 18 cars drive by in the next much smaller than a second? Full derivation can be found here.\n\n\n\n\n\\(\\binom{n}{18}(\\frac{7}{n})^{18}(1-\\frac{7}{n})^{n-18}\\)\n\\(\\frac{n\\cdot(n-1)\\ldots(n-17)}{18!}\\cdot \\frac{7^{18}}{n^{18}}\\cdot (1-\\frac{7}{n})^{n} \\cdot \\frac{1}{(1-\\frac{7}{n})^{18}}\\)\n\\(\\frac{n\\cdot(n-1)\\ldots(n-17)}{n^{18}} \\rightarrow 1\\)\n\\(\\frac{1}{(1-\\frac{7}{n})^{18}} \\rightarrow 1\\)\n\n\n\n\n\\(\\frac{7^{18}}{18!}e^{-7}\\)"
  },
  {
    "objectID": "slides/lect_10.html#poisson-distribution---rips",
    "href": "slides/lect_10.html#poisson-distribution---rips",
    "title": "Chapter 11: Probability models for counts",
    "section": "Poisson distribution - RIPS",
    "text": "Poisson distribution - RIPS\n\n\nPoisson - fish - Rips\nR - randomly through space or time\nI - indepedent\nP - proportional to interval size\nS - singly - no multiple occurences in space or time"
  },
  {
    "objectID": "slides/lect_10.html#epois",
    "href": "slides/lect_10.html#epois",
    "title": "Chapter 11: Probability models for counts",
    "section": "E(Pois)",
    "text": "E(Pois)\n\n\\(\\lambda\\)"
  },
  {
    "objectID": "slides/lect_10.html#varpois",
    "href": "slides/lect_10.html#varpois",
    "title": "Chapter 11: Probability models for counts",
    "section": "Var(Pois)",
    "text": "Var(Pois)\n\n\\(\\lambda\\)"
  },
  {
    "objectID": "slides/lect_10.html#example-1",
    "href": "slides/lect_10.html#example-1",
    "title": "Chapter 11: Probability models for counts",
    "section": "Example",
    "text": "Example\n\n\nLet \\(Y \\sim Pois(\\lambda = 5)\\) find the \\(E(Y)\\) and \\(Var(Y)\\)\n\n\n\\(E(Y) = 5\\)\n\\(Var(Y) = 5\\)\nWhat is the probability that \\(y = 3\\)? In R: dpois(x = 3, lambda = 5) \\(0.1403739\\)\n\n\n\n\n\n02:00\n\n\n\n\n\nstat1010-f22.github.io/website"
  },
  {
    "objectID": "slides/here_lecture.html#why",
    "href": "slides/here_lecture.html#why",
    "title": "Here lecture",
    "section": "Why?",
    "text": "Why?\nToday we will explore the here package. This package is used to organize files within R:\n\ninstall.packages(\"here\")"
  },
  {
    "objectID": "slides/here_lecture.html#including-your-own-data",
    "href": "slides/here_lecture.html#including-your-own-data",
    "title": "Here lecture",
    "section": "Including your own data",
    "text": "Including your own data\nTo add data to your directory, first create a folder that\n\nFigure @ref(fig:3) Adding a folder.and name it “data”. All data related to this chapter should be contained in this folder. In your file management system place your data in this folder. Then in the “Files” menu double-click on the dataset that you would like to import into R."
  },
  {
    "objectID": "slides/here_lecture.html#importing-data",
    "href": "slides/here_lecture.html#importing-data",
    "title": "Here lecture",
    "section": "Importing data",
    "text": "Importing data\n\n\n\nDouble click on the data you would like to install in the data folder in your project and select “Import Dataset.”\n\n\nIf this is the first time you are importing data, you may be prompted to download a package first. Follow the Graphical User Interface (GUI) to import the data. Check that the data is as you expect and update “Import Options” until it seems reasonable. Copy the coding from “Code Preview” and save put it in your document “so that when you restart R, you have coding that will import the data.\n\n\n\nWhen the data is imported, check the “Data Preview” to make sure it is what you expect. If not, change the “Import Options.”\n\n\n\n\nstat1010-f22.github.io/website"
  },
  {
    "objectID": "slides/lect_14.html#revision",
    "href": "slides/lect_14.html#revision",
    "title": "Chapter 15: Confidence intervals",
    "section": "Revision",
    "text": "Revision"
  },
  {
    "objectID": "slides/lect_14.html#variability",
    "href": "slides/lect_14.html#variability",
    "title": "Chapter 15: Confidence intervals",
    "section": "Variability",
    "text": "Variability\n\nask them what they population mean should be\nwe also need to think about the variability\n\n\n\ndependent on \\(p\\) - fatter in middle, skinnier near 0 and 1\ndependent on \\(n\\), number of samples, bigger sample, less variability"
  },
  {
    "objectID": "slides/lect_14.html#variability---math",
    "href": "slides/lect_14.html#variability---math",
    "title": "Chapter 15: Confidence intervals",
    "section": "Variability - math",
    "text": "Variability - math\n\\[ \\hat{p} \\sim N(\\mu = p, \\sigma^2 = \\frac{p(1-p)}{n}) \\]\n\n\nnumerator of the variance is similar to that of a Binomial distribution\nas sample size increases, variance decreases"
  },
  {
    "objectID": "slides/lect_14.html#ci---math",
    "href": "slides/lect_14.html#ci---math",
    "title": "Chapter 15: Confidence intervals",
    "section": "CI - math",
    "text": "CI - math\n\\[\\hat{p} \\pm z_{\\alpha/2} \\cdot \\sqrt{\\hat{p}(1-\\hat{p})/n} \\] For \\(95\\%\\) CI, \\(z_{\\alpha/2} = 1.96\\)\nAs confidence level increases, interval widens\nIn R: qnorm(0.025)"
  },
  {
    "objectID": "slides/lect_14.html#assumptions",
    "href": "slides/lect_14.html#assumptions",
    "title": "Chapter 15: Confidence intervals",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nSRS condition The observed sample is SRS from the appropriate population. If population is finite, less than 10% of total population\nSample size condition (for proportion) both \\(n\\hat{p}\\) and \\(n(1-\\hat{p})\\) are larger than 10."
  },
  {
    "objectID": "slides/lect_14.html#example-1",
    "href": "slides/lect_14.html#example-1",
    "title": "Chapter 15: Confidence intervals",
    "section": "Example 1",
    "text": "Example 1\n\n\nLet \\(\\hat{p}= 0.14\\) and \\(n = 350\\), find a \\(95\\%\\) CI for the proportion\n\\(\\begin{aligned} se(\\hat{p}) &= \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} &= \\sqrt{\\frac{0.14(1-0.14)}{350}}\\end{aligned} \\approx 0.0185\\)\nlower value: \\(\\hat{p} - 1.96 \\cdot 0.0185 \\approx 0.10374\\)\nupper value: \\(\\hat{p} + 1.96 \\cdot 0.0185 \\approx 0.17626\\)"
  },
  {
    "objectID": "slides/lect_14.html#example-2",
    "href": "slides/lect_14.html#example-2",
    "title": "Chapter 15: Confidence intervals",
    "section": "Example 2",
    "text": "Example 2\nAn auditor checks a sample of 225 randomly chosen transactions from among the thousands processed in an office. Thirty-five contains errors in crediting or debiting the appropriate account\na. Does this situation meet the conditions required for a \\(z\\)-interval for the proportion?\nb. Find the \\(95\\%\\) confidence interval for \\(p\\), the proportion of all transactions processed in the this office that have these errors.\nc. Managers claim that the proportion of errors is about \\(10\\%\\). Does that seem reasonable?"
  },
  {
    "objectID": "slides/lect_14.html#example-2---solns",
    "href": "slides/lect_14.html#example-2---solns",
    "title": "Chapter 15: Confidence intervals",
    "section": "Example 2 - solns",
    "text": "Example 2 - solns\n\n\nyes, \\(\\hat{p} = 35/225 \\approx 0.156\\) so \\(n\\hat{p}, n(1-\\hat{p})> 10\\) and we assume sampled \\(<10\\%\\)\n\\(\\begin{aligned} \\hat{p} \\pm z_{\\alpha/2} \\cdot \\sqrt{\\hat{p}(1-\\hat{p})/n} &= 0.156 \\pm 1.96 \\sqrt{0.156(1-0.156)/225}\\\\ &= 0.156 \\pm 1.96 \\cdot 0.0242 \\\\ &= 0.156 \\pm 0.047\\\\ & [0.109, 0.203]\\end{aligned}\\)\nNo \\(10\\%\\) is too low, with \\(95\\%\\) confidence it is higher"
  },
  {
    "objectID": "slides/lect_14.html#variability-1",
    "href": "slides/lect_14.html#variability-1",
    "title": "Chapter 15: Confidence intervals",
    "section": "Variability",
    "text": "Variability\n\n\nto account for smaller sample size we use a different distribution\n\n\n\n\nstandard deviation of distribution\n\nhigher sd -> wider CI\n\nsample size\n\nsmaller sample size -> wider CI"
  },
  {
    "objectID": "slides/lect_14.html#section-3",
    "href": "slides/lect_14.html#section-3",
    "title": "Chapter 15: Confidence intervals",
    "section": "",
    "text": "t distribution more density in tails"
  },
  {
    "objectID": "slides/lect_14.html#ci---mean",
    "href": "slides/lect_14.html#ci---mean",
    "title": "Chapter 15: Confidence intervals",
    "section": "CI - mean",
    "text": "CI - mean\n\\[\\bar{x} \\pm t_{\\alpha/2, n-1} \\cdot \\frac{s}{\\sqrt{n}} \\]"
  },
  {
    "objectID": "slides/lect_14.html#t---distribution",
    "href": "slides/lect_14.html#t---distribution",
    "title": "Chapter 15: Confidence intervals",
    "section": "T - distribution",
    "text": "T - distribution\n\n\ndon’t know sigma so use s in it’s place\nas df get bigger (a proxy for n) S approaches sigma\n\n\n\n\n\\(Z = \\frac{X-\\mu_X}{\\sigma_X}\\)\n\\(\\bar{X} \\sim N(\\mu = \\mu\\_X, \\sigma^2 = \\frac{\\sigma_X^2}{n})\\)\n\\(T_{n-1} = \\frac{\\bar{X}-\\mu_X}{S/\\sqrt{n}}\\)\n\\(t\\) distribution has more density in the tails to account for smaller sample sizes"
  },
  {
    "objectID": "slides/lect_14.html#variability-2",
    "href": "slides/lect_14.html#variability-2",
    "title": "Chapter 15: Confidence intervals",
    "section": "Variability",
    "text": "Variability\n\n\ndependent on \\(confidence \\text{ }level\\) - as confidence level increases so does the width of the interval\ndependent on \\(n\\), number of samples, bigger sample, less variability"
  },
  {
    "objectID": "slides/lect_14.html#assumptions-1",
    "href": "slides/lect_14.html#assumptions-1",
    "title": "Chapter 15: Confidence intervals",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nSRS condition The observed sample is SRS from the appropriate population. If population is finite, less than 10% of total population\nSample size condition the sample size is larger than 10 times the absolute value of the kurtosis, \\(n>10|K_4|\\)."
  },
  {
    "objectID": "slides/lect_14.html#example-1-1",
    "href": "slides/lect_14.html#example-1-1",
    "title": "Chapter 15: Confidence intervals",
    "section": "Example 1",
    "text": "Example 1\n\n\nLet \\(\\bar{x}= \\$3285\\), \\(n = 150\\), and \\(s = \\$238\\) find a \\(95\\%\\) CI for the mean\n\\(\\begin{aligned} se(\\bar{x}) &= s/\\sqrt{n} &= 238/\\sqrt{150}\\end{aligned} \\approx 19.43262\\)\nqt(.025, df = 149) \\(\\approx -1.976\\)\nlower value: \\(3285 - 1.976 \\cdot 19.43262 \\approx \\$3,320\\)\nupper value: \\(3285 + 1.976 \\cdot 19.43262 \\approx \\$3,247\\)\n3285 + qt(.025, df = 149)* 238/sqrt(150)"
  },
  {
    "objectID": "slides/lect_14.html#example-2-1",
    "href": "slides/lect_14.html#example-2-1",
    "title": "Chapter 15: Confidence intervals",
    "section": "Example 2",
    "text": "Example 2\nOffice administrators claim that the average amount on a purchase order is $6,000. A SRS of 49 purchase orders averages \\(\\bar{x} = \\$4,200\\) with \\(s = \\$3,500\\).\n\nWhat is the relevant sampling distribution?\nFind the 95% confidence interval for \\(\\mu\\), the mean of purchase orders handled by this office during the sampling period.\nDo you think the administrators claim is reasonable?"
  },
  {
    "objectID": "slides/lect_14.html#example-2---solns-1",
    "href": "slides/lect_14.html#example-2---solns-1",
    "title": "Chapter 15: Confidence intervals",
    "section": "Example 2 - solns",
    "text": "Example 2 - solns\n\n\n\\(N(\\mu, \\sigma^2/49)\\)\n\\[\\begin{aligned} 4200 \\pm 2.01 \\cdot 3500/\\sqrt{49} &= 4200 \\pm 2.01 \\cdot 500 \\\\ &= 4200 \\pm -1005.317 \\\\ &\\approx [3195, 5205]\\end{aligned}\\]\nNo \\(\\$6,00\\) is way above the \\(95\\%\\) confidence interval"
  },
  {
    "objectID": "slides/lect_14.html#example-2---solns-2",
    "href": "slides/lect_14.html#example-2---solns-2",
    "title": "Chapter 15: Confidence intervals",
    "section": "Example 2 - solns",
    "text": "Example 2 - solns\n\\(10 \\cdot [3195, 5205] = [31950, 52050]\\)"
  },
  {
    "objectID": "slides/lect_14.html#sample-size---mean",
    "href": "slides/lect_14.html#sample-size---mean",
    "title": "Chapter 15: Confidence intervals",
    "section": "Sample size - mean",
    "text": "Sample size - mean\n\\[n = \\frac{4s^2}{\\text{(Margin of error)}^2} \\]\n\n\nFor \\(p\\) use \\(p = 0.5\\) for biggest variance"
  },
  {
    "objectID": "slides/lect_12.html#revision",
    "href": "slides/lect_12.html#revision",
    "title": "Chapter 13: Samples and surveys",
    "section": "Revision",
    "text": "Revision\n\nWe have not yet discussed the sampling process. Now is our chance!\nOne of the differences between statistics and machine learning - in ml not much thought of sampling."
  },
  {
    "objectID": "slides/lect_12.html#predictive-policing",
    "href": "slides/lect_12.html#predictive-policing",
    "title": "Chapter 13: Samples and surveys",
    "section": "Predictive policing",
    "text": "Predictive policing\n\n\nResearch question - Can police use crime data from disparate sources to anticipate and prevent future crime?\npopulation - all crime\nsample - arrests recorded in police database\nIs this a random sample?\narrests is a surrogate measurement because crime is hard to track"
  },
  {
    "objectID": "slides/lect_12.html#predictive-policing-1",
    "href": "slides/lect_12.html#predictive-policing-1",
    "title": "Chapter 13: Samples and surveys",
    "section": "Predictive policing",
    "text": "Predictive policing\n\n\nfor arrests to occur police must be present\nsome areas are over policed, so have more arrests\nthis algorithm sends police to those areas\nread more here\nuse census data to predict crime instead of arrests, we may do better"
  },
  {
    "objectID": "slides/lect_12.html#census",
    "href": "slides/lect_12.html#census",
    "title": "Chapter 13: Samples and surveys",
    "section": "Census",
    "text": "Census\n\n\nsample everyone in the population\nthis is often difficult because some individuals are hard to locate, and these people may have certain characterisitics that distinguish them from the rest of the population\nPopulations move so getting a perfect measure is hard.\nA census may be more complex than sampling"
  },
  {
    "objectID": "slides/lect_12.html#landon-vs-roosevelt",
    "href": "slides/lect_12.html#landon-vs-roosevelt",
    "title": "Chapter 13: Samples and surveys",
    "section": "Landon vs Roosevelt",
    "text": "Landon vs Roosevelt\n\n\nLiterary digest - correctly predicted presidential elections from 1916 - 1932 with mock ballots\n1936 election\n\n10 million ballots\nLandon win by landslide with 57%\n\n10 million names and addresses in 1936\npoor people were unlikely to have phones, so they over sampled the wealthy\nGallup predicted a Roosevelt win\nweighting can be used to correct biased samples"
  },
  {
    "objectID": "slides/lect_12.html#sampling-1",
    "href": "slides/lect_12.html#sampling-1",
    "title": "Chapter 13: Samples and surveys",
    "section": "Sampling",
    "text": "Sampling\n\n\ntasting is analogous to exploratory analysis\nstirring helps ensure that the taste is representative because it randomizes\nif we add ingredients and don’t stir we may get a biased sample\nif we generalize and decide that it needs more salt, that’s an inference"
  },
  {
    "objectID": "slides/lect_12.html#sampling-frame",
    "href": "slides/lect_12.html#sampling-frame",
    "title": "Chapter 13: Samples and surveys",
    "section": "Sampling frame",
    "text": "Sampling frame\n\n\nlists every member of the population of interest\ncan be complex to identify\nsample from registered voters, but really want people who will vote\nhypothetical populations are more complex\na brewery must sample hops across farmers, and different geographic regions prior to formalizing brewing"
  },
  {
    "objectID": "slides/lect_12.html#simple-random-sample",
    "href": "slides/lect_12.html#simple-random-sample",
    "title": "Chapter 13: Samples and surveys",
    "section": "Simple random sample",
    "text": "Simple random sample"
  },
  {
    "objectID": "slides/lect_12.html#stratified-sampling",
    "href": "slides/lect_12.html#stratified-sampling",
    "title": "Chapter 13: Samples and surveys",
    "section": "Stratified sampling",
    "text": "Stratified sampling"
  },
  {
    "objectID": "slides/lect_12.html#cluster-sample",
    "href": "slides/lect_12.html#cluster-sample",
    "title": "Chapter 13: Samples and surveys",
    "section": "Cluster sample",
    "text": "Cluster sample"
  },
  {
    "objectID": "slides/lect_12.html#sampling-2",
    "href": "slides/lect_12.html#sampling-2",
    "title": "Chapter 13: Samples and surveys",
    "section": "Sampling",
    "text": "Sampling\n\n\nschools or nursing homes for cluster sampling\nstrata imply some similarity, for example high income or low income people\n\n\n\n\nRandomly select cases from the population, where there is no implied connection between the points that are selected.\nStrata are made up of similar observations. We take a simple random sample from each stratum.\nClusters are usually not made up of homogeneous observations. We take a simple random sample of clusters, and then sample all observations in that cluster. Usually preferred for economical reasons."
  },
  {
    "objectID": "slides/lect_12.html#sampling-in-r",
    "href": "slides/lect_12.html#sampling-in-r",
    "title": "Chapter 13: Samples and surveys",
    "section": "Sampling in R",
    "text": "Sampling in R\n\n# SRS\ndata %>% \n  slice_sample(n = sample_size)\n\n# Stratified sampling\ndata %>% \n  group_by(strata) %>% \n  slice_sample(n = size_from_each_strata)\n\n# Cluster sampling\nrandom_cluster <- slice_sample(data$cluster, \n                               n = no_of_clusters)"
  },
  {
    "objectID": "slides/lect_12.html#not-great-sampling-methods",
    "href": "slides/lect_12.html#not-great-sampling-methods",
    "title": "Chapter 13: Samples and surveys",
    "section": "Not great sampling methods",
    "text": "Not great sampling methods\n\nmeta and other tech companies often use NZ as a sampling ground to see how a new product works prior to releasing it to the USA.\nmy cousin when she started her allergen-free cakes b/c her son couldn’t eat anything\n\n\n\nvoluntary response\nconvenience sampling - asking friends or families feedback about your product"
  },
  {
    "objectID": "slides/lect_07.html#using-proportions",
    "href": "slides/lect_07.html#using-proportions",
    "title": "Chapter 9: Random variables",
    "section": "Using proportions",
    "text": "Using proportions\n\n\n\n\\(x\\)\n\\(n\\)\n\\(P(X) = x\\)\n\n\n\n\n1.23\n3\n\n\n\n1.29\n5\n\n\n\n1.37\n4\n\n\n\n1.84\n1\n\n\n\n1.18\n6\n\n\n\n1.22\n2\n\n\n\n1.25\n4\n\n\n\nTotal\n25"
  },
  {
    "objectID": "slides/lect_07.html#mean-or-expected-value",
    "href": "slides/lect_07.html#mean-or-expected-value",
    "title": "Chapter 9: Random variables",
    "section": "Mean or expected value",
    "text": "Mean or expected value\nIf we know $P(X) = x$, then we can use this to find the mean or expected value of a random variable. The pdf includes information about both the total and the number of occurrences of \\(x\\), it does the computation for us.\n\\(\\mu = E(X)\\)\n\\(= x_1p(x_1) + x_2p(x_2) + … + x_np(x_n)\\)"
  },
  {
    "objectID": "slides/lect_07.html#price-increase-1",
    "href": "slides/lect_07.html#price-increase-1",
    "title": "Chapter 9: Random variables",
    "section": "Price increase",
    "text": "Price increase\n\n\n\n\\(x\\)\n\\(x + 5\\)\n\\(P(X) = x+5\\)\n\n\n\n\n1.23\n6.23\n0.12\n\n\n1.29\n6.29\n0.20\n\n\n1.37\n6.37\n0.16\n\n\n1.84\n6.84\n0.04\n\n\n1.18\n6.18\n0.24\n\n\n1.22\n6.22\n0.08\n\n\n1.25\n6.25\n0.16"
  },
  {
    "objectID": "slides/lect_07.html#price-increase-2",
    "href": "slides/lect_07.html#price-increase-2",
    "title": "Chapter 9: Random variables",
    "section": "Price increase",
    "text": "Price increase"
  },
  {
    "objectID": "slides/lect_07.html#stock-splits-1",
    "href": "slides/lect_07.html#stock-splits-1",
    "title": "Chapter 9: Random variables",
    "section": "Stock splits",
    "text": "Stock splits\n\n\n\n\\(x\\)\n\\(3x\\)\n\\(P(X) = x\\)\n\n\n\n\n1.23\n3.68\n0.12\n\n\n1.29\n3.87\n0.20\n\n\n1.37\n4.11\n0.16\n\n\n1.84\n5.52\n0.04\n\n\n1.18\n3.54\n0.24\n\n\n1.22\n3.66\n0.08\n\n\n1.25\n3.75\n0.16"
  },
  {
    "objectID": "slides/lect_07.html#stock-splits-2",
    "href": "slides/lect_07.html#stock-splits-2",
    "title": "Chapter 9: Random variables",
    "section": "Stock splits",
    "text": "Stock splits"
  },
  {
    "objectID": "slides/lect_04.html#definition-of-numeric-variable",
    "href": "slides/lect_04.html#definition-of-numeric-variable",
    "title": "Lec 4 - Exploring numeric data",
    "section": "Definition of numeric variable",
    "text": "Definition of numeric variable\nA numeric or quantitative variable is a variable that can be measured."
  },
  {
    "objectID": "slides/lect_04.html#the-purpose-of-exploratory-data-analysis-eda",
    "href": "slides/lect_04.html#the-purpose-of-exploratory-data-analysis-eda",
    "title": "Lec 4 - Exploring numeric data",
    "section": "The purpose of Exploratory Data Analysis (EDA)",
    "text": "The purpose of Exploratory Data Analysis (EDA)\n\nEDA is about learning the structure of a dataset through a series of numerical and graphical techniques.\nWhen you do EDA, you’ll look for both\n\ngeneral trends and\ninteresting outliers in your data.\n\n\ngenerate questions that will help inform subsequent analysis."
  },
  {
    "objectID": "slides/lect_15.html#does-esp-exist-1",
    "href": "slides/lect_15.html#does-esp-exist-1",
    "title": "Chapter 16: Statistical tests",
    "section": "Does ESP exist?",
    "text": "Does ESP exist?\nSince there are 5 cards, it would be possible to guess the correct card at random \\(p=1/5\\) times, or 20% of the time.\nA person with ESP should be able to guess the correct card more often than 20%. But how much more often do they need to get it right for us to believe that ESP exists?"
  },
  {
    "objectID": "slides/lect_15.html#statistical-test",
    "href": "slides/lect_15.html#statistical-test",
    "title": "Chapter 16: Statistical tests",
    "section": "Statistical test",
    "text": "Statistical test\nOne way to determine something like this is to use a statistical test. A statistical test is a procedure to determine if the results from the sample are convincing enough to allow us to conclude something about the population."
  },
  {
    "objectID": "slides/lect_15.html#hypotheses",
    "href": "slides/lect_15.html#hypotheses",
    "title": "Chapter 16: Statistical tests",
    "section": "Hypotheses",
    "text": "Hypotheses\nWhen we perform a statistical test, we set out our hypotheses before we begin. There are two hypotheses,\nnull hypothesis \\(H_0\\), a statement about there being no effect, or no difference.\nalternative hypothesis \\(H_A\\), the thing that we secretly hope will turn out to be true."
  },
  {
    "objectID": "slides/lect_15.html#esp-hypotheses",
    "href": "slides/lect_15.html#esp-hypotheses",
    "title": "Chapter 16: Statistical tests",
    "section": "ESP hypotheses",
    "text": "ESP hypotheses\nThinking about the ESP experiment, we could use words to state our hypotheses\n\\(\\begin{eqnarray*} &H_0:& \\text{ ESP does not exist} \\\\ &H_A:& \\text{ESP exists} \\end{eqnarray*}\\)\nWe could also write the hypotheses in terms of parameters,\n\\(\\begin{eqnarray*} H_0: p \\leq 1/5 \\\\ H_A: p > 1/5 \\end{eqnarray*}\\)"
  },
  {
    "objectID": "slides/lect_15.html#hypotheses-1",
    "href": "slides/lect_15.html#hypotheses-1",
    "title": "Chapter 16: Statistical tests",
    "section": "Hypotheses",
    "text": "Hypotheses\n\n\nHypotheses are always written about the population parameter (\\(\\mu\\), \\(p\\), \\(\\mu_1-\\mu_2\\), \\(p_1-p_2\\)), never about the sample statistics (\\(\\bar{x}\\), \\(\\hat{p}\\), \\(\\bar{x}_1-\\bar{x}_2\\), \\(\\hat{p}_1-\\hat{p}_2\\)).\nThe null hypothesis is the boring thing that we’re trying to gather evidence against\nThe alternative hypothesis is the exciting thing that would make headlines"
  },
  {
    "objectID": "slides/lect_15.html#one-and-two-sided-hypotheses",
    "href": "slides/lect_15.html#one-and-two-sided-hypotheses",
    "title": "Chapter 16: Statistical tests",
    "section": "One and two sided hypotheses",
    "text": "One and two sided hypotheses\n\n\n\\(H_A\\) has a \\(>\\) sign: upper tail, or “one-sided”\n\\(H_A\\) has a \\(<\\) sign: lower tail, or “one-sided”\n\\(H_A\\) has a \\(\\neq\\) sign: both sides, or “two-sided”"
  },
  {
    "objectID": "slides/lect_15.html#example-1",
    "href": "slides/lect_15.html#example-1",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 1",
    "text": "Example 1\nWhat are the null and alternate hypothesis for these decisions?\n\n\nA person interviews for a job opening. The company has to decide whether to hire the person\nAn inventor proposes a new way to wrap packages that they say will speed up the manufacturing process. Should they adopt the new method?\nA sales representative submits receipts from a recent business trip. Staff must determine whether the claims are legitimate."
  },
  {
    "objectID": "slides/lect_15.html#example-1-solns",
    "href": "slides/lect_15.html#example-1-solns",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 1 solns",
    "text": "Example 1 solns\n\n\n$H_0 = $ Person is not hired\n$H_0 = $ Retain the current method\n$H_0 = $ Treat claims as legitimate. The employee is innocent until evidence to the contrary is found."
  },
  {
    "objectID": "slides/lect_15.html#past-tests",
    "href": "slides/lect_15.html#past-tests",
    "title": "Chapter 16: Statistical tests",
    "section": "Past tests",
    "text": "Past tests\n\n\nVisual assocation tests\nqqplots\n\\(\\chi^2\\) tests"
  },
  {
    "objectID": "slides/lect_15.html#example-2",
    "href": "slides/lect_15.html#example-2",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 2",
    "text": "Example 2\nA snack-food chain runs a promotion in which shoppers are told that 1 in 4 kids’ meals includes a prize. A father buys two kids’ meals, and neither has a prize. He concludes that because neither has a prize, the chain is being deceptive.\n\nWhat is the null and alternative hypothesis?\nDescribe a Type I error?\nDescribe a Type II error?\nWhat is the probability that the father has made a Type I error?"
  },
  {
    "objectID": "slides/lect_15.html#example-2---solns",
    "href": "slides/lect_15.html#example-2---solns",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 2 - solns",
    "text": "Example 2 - solns\n\n\\(H_0\\) = chain is being honest; \\(H_A\\) = chain is not being honest\nFalsely accusing the chain of being deceptive\nFailting to realize that the chain is being deceptive\nFather rejects if both are missing a prize. \\(P(neither has a prize) = (1 - 1/4)^2 \\approx 0.56\\)"
  },
  {
    "objectID": "slides/lect_15.html#test-statistic",
    "href": "slides/lect_15.html#test-statistic",
    "title": "Chapter 16: Statistical tests",
    "section": "Test statistic",
    "text": "Test statistic\nIn the previous example, we used probability to determine how likely it is to get two meals neither of which contain a prize. This process involves computing a test statistic (\\(0.56\\))."
  },
  {
    "objectID": "slides/lect_15.html#testing-a-proportion---contd",
    "href": "slides/lect_15.html#testing-a-proportion---contd",
    "title": "Chapter 16: Statistical tests",
    "section": "Testing a proportion - contd",
    "text": "Testing a proportion - contd\nUnder \\(H_0\\), we find\n\\[z_0 = \\frac{\\hat{p} - p_0} {\\sqrt{\\frac{p_0(1-p_0)}{n}}} \\]"
  },
  {
    "objectID": "slides/lect_15.html#assumptions",
    "href": "slides/lect_15.html#assumptions",
    "title": "Chapter 16: Statistical tests",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nSRS and sample must be < \\(10\\%\\) of population\nBoth \\(np_0\\) and \\(n(1-p_0)\\) are larger than 10"
  },
  {
    "objectID": "slides/lect_15.html#example-3",
    "href": "slides/lect_15.html#example-3",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 3",
    "text": "Example 3\nIn the ESP example, we know that the population average for guessing ESP cards is 9 out of 24 cards. An interested participant took a training course to enhance their ESP. In the followup exam, they guessed 17 out of 36 cards. Did the course improve their ESP abilities?"
  },
  {
    "objectID": "slides/lect_15.html#steps-for-a-hypothesis-test",
    "href": "slides/lect_15.html#steps-for-a-hypothesis-test",
    "title": "Chapter 16: Statistical tests",
    "section": "Steps for a hypothesis test",
    "text": "Steps for a hypothesis test\n\nState hypotheses\nDetermine the level of significance\nCheck conditions\nCalculate test statistic\nCompute p-value\nGeneric conclusion\nInterpret in context"
  },
  {
    "objectID": "slides/lect_15.html#example-3---solns",
    "href": "slides/lect_15.html#example-3---solns",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 3 - solns",
    "text": "Example 3 - solns\n\n\n\\(H_0: p \\leq 9/24\\) and \\(H_A: p > 9/24\\)\n\\(\\alpha = 0.05\\)\nYes 9/24*36 and (1-9/24)*36 both > 10\n\\(\\hat{p} = \\frac{x}{n} = \\frac{17}{36} =0.472\\)\n\\(\\begin{aligned} z_0 &= \\frac{\\hat{p} - p_0} {\\sqrt{\\frac{p_0(1-p_0)}{n}}} \\\\ &= \\frac{0.472 - 0.375} {\\sqrt{\\frac{0.375(1-0.375)}{36}}} \\\\ &= \\frac{0.0972}{0.0807} \\approx 1.2 \\end{aligned}\\)"
  },
  {
    "objectID": "slides/lect_15.html#example-3---solns-1",
    "href": "slides/lect_15.html#example-3---solns-1",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 3 - solns",
    "text": "Example 3 - solns\n\n\n1 -pnorm(1.2) or 1- pnorm((17/36- 9/24)/sqrt((0.375*(1-0.375))/36)) \\(\\approx 0.115\\)\nSince \\(0.115 > \\alpha = 0.05\\) this is not significant.\nWe fail to reject \\(H_0\\). The score does not provide evidence that the intervention improved ESP."
  },
  {
    "objectID": "slides/lect_15.html#testing-a-mean---contd",
    "href": "slides/lect_15.html#testing-a-mean---contd",
    "title": "Chapter 16: Statistical tests",
    "section": "Testing a mean - contd",
    "text": "Testing a mean - contd\nUnder \\(H_0\\), we find\n\\[t = \\frac{\\bar{X} - \\mu_0} {s/\\sqrt{{n}}} \\]"
  },
  {
    "objectID": "slides/lect_15.html#assumptions-1",
    "href": "slides/lect_15.html#assumptions-1",
    "title": "Chapter 16: Statistical tests",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nSRS and sample must be < \\(10\\%\\) of population\nIf we do not know if the population is normal, \\(n > 10|K_4|\\)"
  },
  {
    "objectID": "slides/lect_15.html#example-4",
    "href": "slides/lect_15.html#example-4",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 4",
    "text": "Example 4\nLet \\(\\bar{x} = 3281\\), \\(s = 529\\), and \\(n = 59\\). Perform a hypothesis test that the sample comes from a distribution where the population mean is less than \\(\\mu_0 = 4000\\)"
  },
  {
    "objectID": "slides/lect_15.html#steps-for-a-hypothesis-test-1",
    "href": "slides/lect_15.html#steps-for-a-hypothesis-test-1",
    "title": "Chapter 16: Statistical tests",
    "section": "Steps for a hypothesis test",
    "text": "Steps for a hypothesis test\n\nState hypotheses\nDetermine the level of significance\nCheck conditions\nCalculate test statistic\nCompute p-value\nGeneric conclusion\nInterpret in context"
  },
  {
    "objectID": "slides/lect_15.html#example-4---solns",
    "href": "slides/lect_15.html#example-4---solns",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 4 - solns",
    "text": "Example 4 - solns\n\n\n\\(H_0: p \\geq 4000\\) and \\(H_A: p < 4000\\)\n\\(\\alpha = 0.05\\)\nYes\n\\(\\bar{x} = 3281\\)\n\\(\\begin{aligned} t &= \\frac{\\bar{X} - \\mu_0} {s/\\sqrt{{n}}}\\\\ &= \\frac{3281-4000}{529/\\sqrt{59}} \\\\ &= \\frac{-719}{68.87} \\approx -10.44 \\end{aligned}\\)"
  },
  {
    "objectID": "slides/lect_15.html#example-4---solns-1",
    "href": "slides/lect_15.html#example-4---solns-1",
    "title": "Chapter 16: Statistical tests",
    "section": "Example 4 - solns",
    "text": "Example 4 - solns\n\n\npt(-10.44, df = 58) or pt((3281-4000)/(529/sqrt(59)), df = 58) \\(\\approx 3.07e-15\\)\nSince \\(3.07e-15 < \\alpha = 0.05\\) this is significant.\nWe reject \\(H_0\\). There is very strong evidence that the mean value is less than 4000."
  },
  {
    "objectID": "slides/lect_15.html#errors",
    "href": "slides/lect_15.html#errors",
    "title": "Chapter 16: Statistical tests",
    "section": "Errors",
    "text": "Errors\nThere are two types of errors defined in hypothesis testing:\n\nType I error, rejecting a true null\nType II error, not rejecting a false null"
  },
  {
    "objectID": "slides/lect_15.html#law-analogy",
    "href": "slides/lect_15.html#law-analogy",
    "title": "Chapter 16: Statistical tests",
    "section": "Law analogy",
    "text": "Law analogy\nIn the US, a person is innocent until proven guilty, and evidence of guilt must be beyond “the shadow of a doubt.” We can make two types of mistakes:\n\nConvict an innocent person (type I error)\nRelease a guilty person (type II error)"
  },
  {
    "objectID": "slides/lect_15.html#extremists-and-black-and-white",
    "href": "slides/lect_15.html#extremists-and-black-and-white",
    "title": "Chapter 16: Statistical tests",
    "section": "Extremists and black and white",
    "text": "Extremists and black and white\nTwo options:\n\nthe original study (p-value 0.01) made a Type I error, and the \\(H_0\\) was really true\nthe second study (p-value 0.59) made a Type II error, and \\(H_A\\) is really true\n\nor…\n\nmaybe there were no errors made, just different studies found different things"
  },
  {
    "objectID": "slides/lect_15.html#multiple-testing",
    "href": "slides/lect_15.html#multiple-testing",
    "title": "Chapter 16: Statistical tests",
    "section": "Multiple testing",
    "text": "Multiple testing\nBecause the probability of a Type I error is \\(\\alpha\\), if you do many tests you will find significance in \\(\\alpha\\) of them just by chance.\nIf you do 100 tests, you should expect to find 5 of them to be significant, just by chance.\nThis is the problem of multiple testing."
  },
  {
    "objectID": "slides/lect_15.html#multiple-testing-publication-bias",
    "href": "slides/lect_15.html#multiple-testing-publication-bias",
    "title": "Chapter 16: Statistical tests",
    "section": "Multiple testing + publication bias",
    "text": "Multiple testing + publication bias\nOkay, so \\(\\alpha\\) of all tests show significance, just by random chance.\nAnd things that look significant get published…\nThat means that a fair number of things that are published are actually false! This is pretty scary."
  },
  {
    "objectID": "slides/lect_15.html#how-to-fix-the-problem",
    "href": "slides/lect_15.html#how-to-fix-the-problem",
    "title": "Chapter 16: Statistical tests",
    "section": "How to fix the problem",
    "text": "How to fix the problem\nAs a researcher:\n\nmake sure your results can be replicated (like Motyl tried to do with the politics and grey study)\npublish code and data so others can study your work\n\nAs someone who reads about statistics:\n\nbe skeptical about claims that are just one of many tests\nlook for replication and reproducibility!"
  },
  {
    "objectID": "slides/lect_15.html#reducing-the-probability-of-type-ii-error",
    "href": "slides/lect_15.html#reducing-the-probability-of-type-ii-error",
    "title": "Chapter 16: Statistical tests",
    "section": "Reducing the probability of Type II error",
    "text": "Reducing the probability of Type II error\nIn order to reduce the probability of making a Type II error, we can either\n\nincrease the significance level\nincrease the sample size"
  },
  {
    "objectID": "slides/lect_16.html#different-types-of-comparisons",
    "href": "slides/lect_16.html#different-types-of-comparisons",
    "title": "Chapter 17: Comparison",
    "section": "Different types of comparisons",
    "text": "Different types of comparisons\nFrom our last lecture, there are two courses that participants can take to improve their ESP abilities. One series of courses focuses on somatic training of the participant and the other on eye contact between the sender and receiver. For the purposes of this exercise, we let S be the event that a person took the first somatic course, and I be the event that a person took the first eye training course. This lesson will teach us how to assess these two courses. We will answer these questions:\n\n\nDoes a higher proportion of people sign up for subsequent eye contact or somatic courses?\nAre the earning for the somatic group higher than those of the eye contact group?"
  },
  {
    "objectID": "slides/lect_16.html#data---contd",
    "href": "slides/lect_16.html#data---contd",
    "title": "Chapter 17: Comparison",
    "section": "Data - cont’d",
    "text": "Data - cont’d\nThere are a few ways that we can get data to explore these hypotheses:\n\n\nExperiment: random sample, assigns treatment, compares between treatments.\nObtain random samples from two populations.\nCompare two sets of observations: can be problematic"
  },
  {
    "objectID": "slides/lect_16.html#confounding",
    "href": "slides/lect_16.html#confounding",
    "title": "Chapter 17: Comparison",
    "section": "Confounding",
    "text": "Confounding\nWhen levels of one variable are associated with levels of another the variables are said to be confounded.\nIn our example, perhaps we run a course Sedona, AZ where belief in ESP is high and people are likely to take the whole course. If the other was run in Phoenix, AZ this would not be the case. The belief in ESP in the location is confounded with taking subsequent courses."
  },
  {
    "objectID": "slides/lect_16.html#example-1",
    "href": "slides/lect_16.html#example-1",
    "title": "Chapter 17: Comparison",
    "section": "Example 1",
    "text": "Example 1\nWhich of the following appear safe from confounding and which appear to be contaminated?\n\nA comparison of two promotional displays using average daily sales in one store with one type of display and another store with a different type.\nA comparison of two promotional diplays using average daily sales in one store with one type of display on Monday and the other display on Friday.\nA comparison of two landscaping offers sent at random to potential customers in the same zip code."
  },
  {
    "objectID": "slides/lect_16.html#example-1---solns",
    "href": "slides/lect_16.html#example-1---solns",
    "title": "Chapter 17: Comparison",
    "section": "Example 1 - solns",
    "text": "Example 1 - solns\n\nConfounded by differences between the store, such as location or sales volume\nConfounded by differences in shopping patterns during the week.\nFree of confounding, though may not generalize to other zip codes"
  },
  {
    "objectID": "slides/lect_16.html#assumptions",
    "href": "slides/lect_16.html#assumptions",
    "title": "Chapter 17: Comparison",
    "section": "Assumptions",
    "text": "Assumptions\n\nNo obvious lurking variable or confounders\nSRS condition, or independent random samples from two populations\nWe need to check \\(\\begin{eqnarray*} n_1\\cdot \\hat{p}_1 \\geq 10 \\\\ n_1\\cdot(1-\\hat{p}_1) \\geq 10\\\\ n_2\\cdot \\hat{p}_2 \\geq 10\\\\ n_2\\cdot(1-\\hat{p}_2) \\geq 10 \\end{eqnarray*}\\)"
  },
  {
    "objectID": "slides/lect_16.html#hypothesis-test-for-a-difference-in-proportions",
    "href": "slides/lect_16.html#hypothesis-test-for-a-difference-in-proportions",
    "title": "Chapter 17: Comparison",
    "section": "Hypothesis test for a difference in proportions",
    "text": "Hypothesis test for a difference in proportions\n\\[ z = \\frac{\\hat{p}_1 - \\hat{p}_2 - D_0}{se(\\hat{p}_1 - \\hat{p}_2)}\\]"
  },
  {
    "objectID": "slides/lect_16.html#example-2",
    "href": "slides/lect_16.html#example-2",
    "title": "Chapter 17: Comparison",
    "section": "Example 2",
    "text": "Example 2\nIn the ESP example, spiritual groups decided to randomly assign members to either the somatic (\\(n_s = 809\\)) or eye contact group (\\(n_i = 646\\)). From those in the somatic group \\(280\\) re-enrolled, subsequent eye contact groups had \\(197\\). Perform a two sample \\(z\\)-test for proportions to determine if the enrollment in the somatic course was 2% more, on average, than that in the eye contact course."
  },
  {
    "objectID": "slides/lect_16.html#steps-for-a-hypothesis-test",
    "href": "slides/lect_16.html#steps-for-a-hypothesis-test",
    "title": "Chapter 17: Comparison",
    "section": "Steps for a hypothesis test",
    "text": "Steps for a hypothesis test\n\nState hypotheses\nDetermine the level of significance\nCheck conditions\nCalculate test statistic\nCompute p-value\nGeneric conclusion\nInterpret in context"
  },
  {
    "objectID": "slides/lect_16.html#example-2---solns",
    "href": "slides/lect_16.html#example-2---solns",
    "title": "Chapter 17: Comparison",
    "section": "Example 2 - solns",
    "text": "Example 2 - solns\n\n\n\\(H_0: p_s - p_i \\leq 0.02\\) and \\(H_A: p_s - p_i > 0.02\\)\n\\(\\alpha = 0.05\\)\nYes, 280/809*809 and (1-280/809)*809 both > 10 Yes, 197/646*646 and (1-197/646)*646 both > 10\n\\(\\hat{p_i} = \\frac{x_i}{n_i} = \\frac{197}{646} = 0.305\\) \\(\\hat{p_s} = \\frac{x_s}{n_s} = \\frac{280}{809} = 0.346\\)"
  },
  {
    "objectID": "slides/lect_16.html#example-2---solns-1",
    "href": "slides/lect_16.html#example-2---solns-1",
    "title": "Chapter 17: Comparison",
    "section": "Example 2 - solns",
    "text": "Example 2 - solns\n\n\n\\(\\begin{aligned} se(\\hat{p}_s - \\hat{p}_i) &= \\sqrt{\\frac{\\hat{p}_s(1-\\hat{p}_s)}{n_s}+\\frac{\\hat{p}_i(1-\\hat{p}_i)}{n_i}} \\\\ &= \\sqrt{\\frac{0.305(1-0.305)}{646}+\\frac{0.346(1-0.346)}{809}} \\\\ &= \\sqrt{0.0003281 + 0.0002797} \\approx 0.02465 \\end{aligned}\\) \\(\\begin{aligned} z_0 &= \\frac{\\hat{p_s} - \\hat{p_i} - D_0} {se(\\hat{p_s} - \\hat{p_i})} \\\\ &= \\frac{0.346 - 0.305 - 0.02} {0.02465} \\\\ &\\approx 0.851927 \\end{aligned}\\)\n\n1 - pnorm(0.851927) or \\(\\approx 0.197\\)\n\nSince \\(0.197 > \\alpha = 0.05\\) this is not significant.\nWe fail to reject \\(H_0\\). The reenrollment in the somatic course is not significantly more than the reenrollment in the eye contact course."
  },
  {
    "objectID": "slides/lect_16.html#math---two-sample-ci-for-props",
    "href": "slides/lect_16.html#math---two-sample-ci-for-props",
    "title": "Chapter 17: Comparison",
    "section": "Math - two sample CI for props",
    "text": "Math - two sample CI for props\n\\[ \\hat{p}_1 - \\hat{p}_2 - z_{\\alpha/2} se(\\hat{p}_1 - \\hat{p}_2) \\text{  to  }\\\\ \\hat{p}_1 - \\hat{p}_2 + z_{\\alpha/2} se(\\hat{p}_1 - \\hat{p}_2)\\]"
  },
  {
    "objectID": "slides/lect_16.html#assumptions-1",
    "href": "slides/lect_16.html#assumptions-1",
    "title": "Chapter 17: Comparison",
    "section": "Assumptions",
    "text": "Assumptions\n\nNo obvious lurking variables or confounders\nSRS condition\nSample size condition"
  },
  {
    "objectID": "slides/lect_16.html#example-3",
    "href": "slides/lect_16.html#example-3",
    "title": "Chapter 17: Comparison",
    "section": "Example 3",
    "text": "Example 3\nIn the ESP example, spiritual groups decided to randomly assign members to either the somatic (\\(n_s = 809\\)) or eye contact group (\\(n_i = 646\\)). From those in the somatic contact group \\(280\\) re-enrolled, subsequent eye contact groups had \\(197\\). Find the 95% confidence interval for the difference between the proportions who take subsequent courses on the somatic and eye contact group."
  },
  {
    "objectID": "slides/lect_16.html#example-3---solns",
    "href": "slides/lect_16.html#example-3---solns",
    "title": "Chapter 17: Comparison",
    "section": "Example 3 - solns",
    "text": "Example 3 - solns\n\n\n\\(\\begin{aligned} \\text{lower bound} &= \\hat{p}_s - \\hat{p}_i - z_{\\alpha/2} se(\\hat{p}_s - \\hat{p}_i) \\\\ &= 0.346 - 0.305 - 1.96 \\cdot 0.02465 \\\\ &= 0.041 - 0.0483 \\approx -0.0073 \\end{aligned}\\)\n\\(\\begin{aligned} \\text{upper bound} &= \\hat{p}_s - \\hat{p}_i + z_{\\alpha/2} se(\\hat{p}_s - \\hat{p}_i) \\\\ &= 0.346 - 0.305 + 1.96 \\cdot 0.02465 \\\\ &= 0.041 + 0.0483 \\approx 0.0893 \\end{aligned}\\)"
  },
  {
    "objectID": "slides/lect_16.html#interpretation",
    "href": "slides/lect_16.html#interpretation",
    "title": "Chapter 17: Comparison",
    "section": "Interpretation",
    "text": "Interpretation\nSince \\(0 \\text{ is in } [-0.0073, 0.0893]\\) the difference is not statistically signficant at the \\(95\\%\\) significance level. It is possible that the reenrollment rates for eye contact and the somatic course come from populations with the same proportion. We do not know which is higher, either could be.\nIf \\(0\\) is not in the \\(95\\%\\) confidence interval, then the difference is significant and we can be \\(95\\%\\) confident that the difference is between the lower and upper bounds. Any value in the region could plausibly be the difference in proportions between the two populations."
  },
  {
    "objectID": "slides/lect_16.html#assumptions-2",
    "href": "slides/lect_16.html#assumptions-2",
    "title": "Chapter 17: Comparison",
    "section": "Assumptions",
    "text": "Assumptions\n\nNo lurking variables\nSRS condition\nSimilar variances\nEach sample must exceed \\(10|K_4|\\)"
  },
  {
    "objectID": "slides/lect_16.html#in-r",
    "href": "slides/lect_16.html#in-r",
    "title": "Chapter 17: Comparison",
    "section": "In R",
    "text": "In R\nThis computation is long and complicated, so it’s best done in R: t.test()\nSince we only have summary data for the ESP example, we’ll show with the diamonds dataset. In this situation, we’ll ask if there is a relationship between carat (weight of the diamond) and color (D is best and J is worst). Which color has on average the largest diamonds?\n\\[ H_0: \\mu_D - \\mu_j \\leq D_0\\]"
  },
  {
    "objectID": "slides/lect_16.html#in-r-1",
    "href": "slides/lect_16.html#in-r-1",
    "title": "Chapter 17: Comparison",
    "section": "In R",
    "text": "In R\n\n## install packages\nlibrary(tidyverse)\n\nd.color <- diamonds %>% \n  filter(color == \"D\") %>% \n  select(carat)\n  \nj.color <- diamonds %>% \n  filter(color == \"J\") %>% \n  select(carat)\n\n## 2 - sided test\n## Alternative hypothesis: x != y\nt.test(x = d.color, y = j.color, alternative = \"two.sided\")\n\n## Alternative hypothesis: x < y by 0.1\nt.test(x = d.color, y = j.color, alternative  = \"less\", mu = .1)"
  },
  {
    "objectID": "slides/lect_16.html#in-r-2",
    "href": "slides/lect_16.html#in-r-2",
    "title": "Chapter 17: Comparison",
    "section": "In R",
    "text": "In R\n\n\n\n    Welch Two Sample t-test\n\ndata:  d.color and j.color\nt = -41.811, df = 3683.7, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5279915 -0.4806923\nsample estimates:\nmean of x mean of y \n0.6577948 1.1621368 \n\n\n\n    Welch Two Sample t-test\n\ndata:  d.color and j.color\nt = -50.101, df = 3683.7, p-value < 2.2e-16\nalternative hypothesis: true difference in means is less than 0.1\n95 percent confidence interval:\n       -Inf -0.4844961\nsample estimates:\nmean of x mean of y \n0.6577948 1.1621368"
  },
  {
    "objectID": "slides/lect_16.html#interpretation-1",
    "href": "slides/lect_16.html#interpretation-1",
    "title": "Chapter 17: Comparison",
    "section": "Interpretation",
    "text": "Interpretation\nIf the p-value is less than \\(\\alpha\\) there is evidence against \\(H_0\\) and we can reject \\(H_0\\) in favor of the alternative.\n“If the p-value is low then the null must go.”\n2 - sided\nThere is extremely strong evidence that the mean carat for diamonds with the best color (D) is not the same as that for diamonds with the worst color (J).\nalternative = “less”\nThere is extremely strong evidence that the mean carat for diamonds with the best color (D) is smaller by at least 0.1 then diamonds with the worst color (J).\nBut what is the magnitude of the difference?"
  },
  {
    "objectID": "slides/lect_16.html#in-r-3",
    "href": "slides/lect_16.html#in-r-3",
    "title": "Chapter 17: Comparison",
    "section": "In R",
    "text": "In R\n\nt.test(x = d.color, y = j.color, conf.level = 0.95)$conf.int\n\n[1] -0.5279915 -0.4806923\nattr(,\"conf.level\")\n[1] 0.95"
  },
  {
    "objectID": "slides/lect_16.html#another-example",
    "href": "slides/lect_16.html#another-example",
    "title": "Chapter 17: Comparison",
    "section": "Another example",
    "text": "Another example\nCertain people refused to get vacinated from COVID-19. To compare COVID infection rates it’s best to pair people who are vaccinated and not vaccinated but who also have similar education levels, incomes, and risk of infection. This hapenned last year in a UK government report."
  },
  {
    "objectID": "slides/lect_16.html#math",
    "href": "slides/lect_16.html#math",
    "title": "Chapter 17: Comparison",
    "section": "Math",
    "text": "Math\nGiven the paired data we find the differences (\\(d_i = x_i – y_i\\))\nThe \\(100(1 - \\alpha)%\\) confidence paired t- interval is\n\\[\\bar{d} \\pm  t_{\\alpha/2, n-1} \\frac{s_d}{\\sqrt{n}}\\]\nChecklist: No obvious lurking variables. SRS condition. Sample size condition."
  },
  {
    "objectID": "slides/lect_16.html#in-r-4",
    "href": "slides/lect_16.html#in-r-4",
    "title": "Chapter 17: Comparison",
    "section": "In R",
    "text": "In R\n\n# Weight of mice before treatment\nbefore <-c(200.1, 190.9, 192.7, 213, 241.4, 196.9, 172.2, 185.5, 205.2, 193.7)\n# Weight of mice after treatment\nafter <-c(392.9, 393.2, 345.1, 393, 434, 427.9, 422, 383.9, 392.3, 352.2)\n\n# A tibble\nmice <- tibble(\n  group = rep(c(\"before\", \"after\"), each = 10),\n  weight = c(before, after)\n  )\n\nt.test(weight ~ group, data = mice, paired = TRUE)"
  },
  {
    "objectID": "slides/lect_16.html#in-r-5",
    "href": "slides/lect_16.html#in-r-5",
    "title": "Chapter 17: Comparison",
    "section": "In R",
    "text": "In R\n\n\n\n    Paired t-test\n\ndata:  weight by group\nt = 20.883, df = 9, p-value = 6.2e-09\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 173.4219 215.5581\nsample estimates:\nmean difference \n         194.49"
  },
  {
    "objectID": "slides/lect_16.html#plot",
    "href": "slides/lect_16.html#plot",
    "title": "Chapter 17: Comparison",
    "section": "Plot",
    "text": "Plot\n\nggplot(data = mice) +\n  geom_boxplot(aes(x = group, y = weight))"
  },
  {
    "objectID": "slides/lect_16.html#interpretation-2",
    "href": "slides/lect_16.html#interpretation-2",
    "title": "Chapter 17: Comparison",
    "section": "Interpretation",
    "text": "Interpretation\nThere is very strong evidence that the underlying population means of the mice before and after treatment are not the same. On average, the mice are much heavier after treatment.\n\n\nstat1010-f22.github.io/website"
  },
  {
    "objectID": "slides/lect_18.html#variable-names",
    "href": "slides/lect_18.html#variable-names",
    "title": "Chapter 19: Linear patterns",
    "section": "Variable names",
    "text": "Variable names\n\n\nvariable on the \\(y\\) axis\n\ndependent\nresponse\noutcome\n\nvariable on the \\(x\\) axis\n\nindependent (not this kind of independence)\nexplanatory\npredictor"
  },
  {
    "objectID": "slides/lect_18.html#plot-the-data",
    "href": "slides/lect_18.html#plot-the-data",
    "title": "Chapter 19: Linear patterns",
    "section": "Plot the data",
    "text": "Plot the data"
  },
  {
    "objectID": "slides/lect_18.html#fit",
    "href": "slides/lect_18.html#fit",
    "title": "Chapter 19: Linear patterns",
    "section": "Fit",
    "text": "Fit\n\\[ \\hat{y} = b_0 + b_1\\cdot x \\]\n\n\n\\(b-1\\) is gradient \\(b_0\\) is the \\(y\\) intercept\n$ $ is the fitted value, the result of the fitted line"
  },
  {
    "objectID": "slides/lect_18.html#possible-fits",
    "href": "slides/lect_18.html#possible-fits",
    "title": "Chapter 19: Linear patterns",
    "section": "Possible fits",
    "text": "Possible fits"
  },
  {
    "objectID": "slides/lect_18.html#least-squares",
    "href": "slides/lect_18.html#least-squares",
    "title": "Chapter 19: Linear patterns",
    "section": "Least squares",
    "text": "Least squares"
  },
  {
    "objectID": "slides/lect_18.html#gradient-and-slope",
    "href": "slides/lect_18.html#gradient-and-slope",
    "title": "Chapter 19: Linear patterns",
    "section": "Gradient and slope",
    "text": "Gradient and slope\n\nPull out your calculators and work on this\n\n\\[ m = \\frac{r \\cdot s_y}{s_x} \\] \\[ b = \\bar{y} - m \\bar{x} \\] ## Example 1\n\ncarat is on the bottom b/c it is the x, always x on the bottom.\n\n\n\n\\(r = 0.9215913\\)\n\\(s_{price} = 3989.44\\)\n\\(s_{carat} = 0.4740112\\)\n\\(m = \\frac{r \\cdot s_{price}}{s_{carat}}\\)\n\\(\\begin{aligned} m &= \\frac{r \\cdot s_{price}}{s_{carat}} \\\\ &= \\frac{0.9215913 \\cdot 3989.44}{0.4740112} \\\\ &= 7756.426 \\end{aligned}\\)"
  },
  {
    "objectID": "slides/lect_18.html#example-1",
    "href": "slides/lect_18.html#example-1",
    "title": "Chapter 19: Linear patterns",
    "section": "Example 1",
    "text": "Example 1\n\nPull out your calculators and work on this\n\nfor carat values \\(2.5\\)\n\n\n\\(\\widehat{\\text{price}} = -2256.36 + 7756.426 \\cdot \\text{carat}\\)\n\\(\\hat{y} = -2256.36 + 7756.426 \\cdot x\\)\n\\(\\begin{aligned}\\hat{y} &= -2256.36 + 7756.426 \\cdot x \\\\ &= -2256.36 + 7756.426 \\cdot 2.5 \\\\&= \\$17135 \\end{aligned}\\)"
  },
  {
    "objectID": "slides/lect_18.html#residuals",
    "href": "slides/lect_18.html#residuals",
    "title": "Chapter 19: Linear patterns",
    "section": "Residuals",
    "text": "Residuals\nThe residual for the \\(i^{th}\\) observation is \\[ \\begin{aligned} e_i &= y_i - \\hat{y_i} \\\\\n&= y_i - b_0 - b_1 \\cdot x_i\n\\end{aligned}\\]\n\nThe sum of squared residuals is\n\n\\[e^2_1 + e^2_2 + \\dots + e^2_n\\]\n\nThe least squares line is the one that minimizes the sum of squared residuals"
  },
  {
    "objectID": "slides/lect_18.html#example-2",
    "href": "slides/lect_18.html#example-2",
    "title": "Chapter 19: Linear patterns",
    "section": "Example 2",
    "text": "Example 2\nFind the residual for the pair \\((2.5, 16955)\\)\n\n\n\\[ \\begin{aligned} e &= y - \\hat{y} \\\\\n&= 16955 - (2256.36 + 7756.426 \\cdot 2.5) \\\\\n&= 16955 - 21647.43\\\\\n&= -4692.425\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lect_18.html#in-r",
    "href": "slides/lect_18.html#in-r",
    "title": "Chapter 19: Linear patterns",
    "section": "In R",
    "text": "In R\n\n# for model fitting and to get residuals\nlibrary(tidymodels)\n\n\n## Assign the least\n## squares line\nlm_fit<- lm(price ~ carat, diamonds) \n## NOTE: outcome first, predictor second\n\n## find predictions and residuals\naugment(lm_fit)\n\n# A tibble: 53,940 × 8\n   price carat .fitted .resid      .hat .sigma     .cooksd .std.resid\n   <int> <dbl>   <dbl>  <dbl>     <dbl>  <dbl>       <dbl>      <dbl>\n 1   326  0.23 -472.     798. 0.0000452  1549. 0.00000600       0.516\n 2   326  0.21 -628.     954. 0.0000471  1549. 0.00000892       0.616\n 3   327  0.23 -472.     799. 0.0000452  1549. 0.00000602       0.516\n 4   334  0.29   -7.00   341. 0.0000398  1549. 0.000000966      0.220\n 5   335  0.31  148.     187. 0.0000382  1549. 0.000000278      0.121\n 6   336  0.24 -395.     731. 0.0000442  1549. 0.00000493       0.472\n 7   336  0.24 -395.     731. 0.0000442  1549. 0.00000493       0.472\n 8   337  0.26 -240.     577. 0.0000424  1549. 0.00000294       0.372\n 9   337  0.22 -550.     887. 0.0000461  1549. 0.00000756       0.573\n10   338  0.23 -472.     810. 0.0000452  1549. 0.00000618       0.523\n# … with 53,930 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "slides/lect_18.html#visual-interpretation",
    "href": "slides/lect_18.html#visual-interpretation",
    "title": "Chapter 19: Linear patterns",
    "section": "Visual interpretation",
    "text": "Visual interpretation"
  },
  {
    "objectID": "slides/lect_18.html#extrapolation",
    "href": "slides/lect_18.html#extrapolation",
    "title": "Chapter 19: Linear patterns",
    "section": "Extrapolation",
    "text": "Extrapolation\nThis classic example is my favorite.\nThe model is only useful inside the values for which we have data. Outside of those values anything could happen."
  },
  {
    "objectID": "slides/lect_18.html#example-3",
    "href": "slides/lect_18.html#example-3",
    "title": "Chapter 19: Linear patterns",
    "section": "Example 3",
    "text": "Example 3\n\nthink about this in a manufacturing sense\n\nA manufacturing plant receives orders for customized mechanical parts. The orders vary in size, from about 30 to 130 units. After configuring the production line, a supervisor oversees the production. The least squares regression line that predicts time in hours using number of units to produce is:\n\\[\\widehat{\\text{time}} = 2.1 + 0.031 \\cdot \\text{Number of units}\\] a. Interpret the intercept of the estimated line.\nb. Interpret the slope of the estimated line.\nc. Using the fitted line, estimate the amount of time needed for an order with 100 unit. Is this estimate an extrapolation?\nd. Based on the fitted line, how much more time does an order with 100 units require over an order with 50 units?"
  },
  {
    "objectID": "slides/lect_18.html#example-3---solns",
    "href": "slides/lect_18.html#example-3---solns",
    "title": "Chapter 19: Linear patterns",
    "section": "Example 3 - solns",
    "text": "Example 3 - solns\n\n\nThe intercept (2.1 hrs) is the estimated time for any orders, regardless of size (to set up production)\nOnce it is running, the estimated tiem for an order is 0.031 hours per unit (or 60 * 0.031 = 1.9 minutes)\n\\(2.1 + 0.031\\cdot 100 = 5.2 hours\\), 100 units is inside the range\nFifty more units would need \\(0.031\\cdot 50 = 1.55\\) more hours"
  },
  {
    "objectID": "slides/lect_18.html#in-r-1",
    "href": "slides/lect_18.html#in-r-1",
    "title": "Chapter 19: Linear patterns",
    "section": "In R",
    "text": "In R\nThis plot has an obvious trend, the model does not fit the data well. Can use the visual association test to check.\n\n## plotting residuals\naugment(lm_fit) %>%\n  ggplot() + \n  geom_point(aes(x = carat, \n                 y = .resid))"
  },
  {
    "objectID": "slides/lect_18.html#sd-of-residuals",
    "href": "slides/lect_18.html#sd-of-residuals",
    "title": "Chapter 19: Linear patterns",
    "section": "sd of residuals",
    "text": "sd of residuals\n\n\nstat1010-f22.github.io/website"
  },
  {
    "objectID": "slides/lect_13.html#sampling",
    "href": "slides/lect_13.html#sampling",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Sampling",
    "text": "Sampling\n\n\nSRS\nStratified sampling\nCluster sampling\nit would be nice to sample repeatedly to see how the mean values compare"
  },
  {
    "objectID": "slides/lect_13.html#benefits-of-averaging",
    "href": "slides/lect_13.html#benefits-of-averaging",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Benefits of averaging",
    "text": "Benefits of averaging\n\n\nreduces variation\nmore normal than the original distribution"
  },
  {
    "objectID": "slides/lect_13.html#normal-model",
    "href": "slides/lect_13.html#normal-model",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Normal model",
    "text": "Normal model\n\n\nFind kurtosis(\\(K_4\\))\nIf \\(n > 10 |K_4|\\), where \\(n\\) is the sample size, then a normal model adequately approximates the distribution of the sample mean \\(\\bar{X}\\).\nIf we know the data come from a normal distribution this is also true."
  },
  {
    "objectID": "slides/lect_13.html#standard-error",
    "href": "slides/lect_13.html#standard-error",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Standard error",
    "text": "Standard error\n\\[ SD(\\bar{X}) = SE(\\bar{X}) =\n\\frac{\\sigma}{\\sqrt{n}} \\]"
  },
  {
    "objectID": "slides/lect_13.html#sampling-distribution",
    "href": "slides/lect_13.html#sampling-distribution",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Sampling distribution",
    "text": "Sampling distribution\nIf \\(X \\sim N(\\mu_X, \\sigma_X^2)\\)\n\\[ \\bar{X} \\sim N(\\mu = \\mu_X, \\sigma^2 = \\frac{\\sigma_X^2}{n}) \\]"
  },
  {
    "objectID": "slides/lect_13.html#example",
    "href": "slides/lect_13.html#example",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Example",
    "text": "Example\n\n\nLet \\(Y \\sim N(\\mu = 5, \\sigma^2 = 16)\\), find the distribution of the mean of repeated samples of size 4.\n\n\\(\\bar{Y} \\sim N(\\mu = 5, \\sigma^2 = 4)\\)"
  },
  {
    "objectID": "slides/lect_13.html#control-limits",
    "href": "slides/lect_13.html#control-limits",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Control limits",
    "text": "Control limits\n\n\nif mean production is outside certain values, we may need to stop and recallibrate machinery\nthese values are called control limits\n\\(\\mu - L \\leq \\bar{X} \\leq \\mu + L\\)\n\\(\\mu - L\\) and \\(\\mu + L\\) are control limits"
  },
  {
    "objectID": "slides/lect_13.html#types-of-errors",
    "href": "slides/lect_13.html#types-of-errors",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Types of errors",
    "text": "Types of errors\n\n\nFalse positives - type 1 error\n\nact when you should not\nprobabiliy of occurence denoted \\(\\alpha\\)\n\nFalse negatives - type 2 error\n\ndon’t act when you should\nprobabiliy of occurence denoted \\(\\beta\\)"
  },
  {
    "objectID": "slides/lect_13.html#setting-control-limits---1",
    "href": "slides/lect_13.html#setting-control-limits---1",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Setting control limits - 1",
    "text": "Setting control limits - 1\nIf \\(\\bar{X} \\sim N(\\mu = 12, \\sigma^2 = 2.3)\\) how can we find the control limits?\n\n\nSet the control limits and find the \\(\\alpha\\) value\n\nWe want the control limits to be between 10 and 14\n\\(Pr(\\bar{X} < 10 \\textrm{ or } \\bar{X} > 14)\\)\n\\(\\begin{aligned}P(\\bar{X} < 10) &= P(\\frac{\\bar{X} - \\mu_\\bar{X}}{\\sigma_\\bar{X}} < \\frac{10-\\mu_\\bar{X}}{\\sigma_\\bar{X}}) \\\\ & = P(Z < \\frac{10-12}{\\sqrt{2.3}} = -1.318761)\\end{aligned}\\)\npnorm(-1.318761) \\(\\approx 0.09362451\\)\npnorm(10, mean = 12, sd = sqrt(2.3))"
  },
  {
    "objectID": "slides/lect_13.html#setting-control-limits---1-contd",
    "href": "slides/lect_13.html#setting-control-limits---1-contd",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Setting control limits - 1 cont’d",
    "text": "Setting control limits - 1 cont’d\nIf \\(\\bar{X} \\sim N(\\mu = 12, \\sigma^2 = 2.3)\\) how can we find the control limits?\n\n\nSet the control limits and find the \\(\\alpha\\) value\n\nWe want the control limits to be between 10 and 14\n\\(\\begin{aligned}P(\\bar{X} > 14) &= P(\\frac{\\bar{X} - \\mu_\\bar{X} }{\\sigma_\\bar{X}} > \\frac{14-\\mu_\\bar{X}}{\\sigma_\\bar{X}}) \\\\ & = P(Z > \\frac{14-12}{\\sqrt{2.3}} = 1.318761)\\end{aligned}\\)\n1 - pnorm(1.318761) \\(\\approx 0.09362451\\)\n1 - pnorm(14, mean = 12, sd = sqrt(2.3))\nthe \\(\\alpha\\) value is \\(19\\%\\) which is very high!"
  },
  {
    "objectID": "slides/lect_13.html#setting-control-limits---2",
    "href": "slides/lect_13.html#setting-control-limits---2",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Setting control limits - 2",
    "text": "Setting control limits - 2\n\n\nSet the \\(\\alpha\\) value and find the control limits\n\n\nWe want the \\(\\alpha\\) to be \\(0.025\\)\n\n\\(Pr(\\bar{X} < z_{0.0125} \\textrm{ or } \\bar{X} > z_{0.0125})\\)\nqnorm(0.0125) \\(\\approx -2.241403\\)\n\\(\\begin{aligned}-2.241403 =& \\frac{X - \\mu_\\bar{X}}{\\sigma_\\bar{X}}\\\\ & = \\frac{X - 12}{\\sqrt{2.3}} \\\\ &= -2.241403\\sqrt{2.3} +12 = 8.600744 \\end{aligned}\\)\nqnorm(0.0125, mean = 12, sd = sqrt(2.3))"
  },
  {
    "objectID": "slides/lect_13.html#setting-control-limits---2---contd",
    "href": "slides/lect_13.html#setting-control-limits---2---contd",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Setting control limits - 2 - cont’d",
    "text": "Setting control limits - 2 - cont’d\n\n\nSet the \\(\\alpha\\) value and find the control limits\n\n\nWe want the \\(\\alpha\\) to be \\(0.025\\)\n\n\\(Pr(\\bar{X} < z_{0.0125} \\textrm{ or } \\bar{X} > z_{0.0125})\\)\nqnorm(1- 0.0125) \\(\\approx 2.241403\\)\n\\(\\begin{aligned}2.241403 =& \\frac{X - \\mu_\\bar{X}}{\\sigma_\\bar{X}}\\\\ & = \\frac{X - 12}{\\sqrt{2.3}} \\\\ &= 2.241403\\sqrt{2.3} +12 = 15.39926 \\end{aligned}\\)\nqnorm(1-0.0125, mean = 12, sd = sqrt(2.3))"
  },
  {
    "objectID": "slides/lect_13.html#repeated-testing",
    "href": "slides/lect_13.html#repeated-testing",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Repeated testing",
    "text": "Repeated testing\nWe are not testing once, but multiple times. Assuming independence:\n\n\n\\(\\begin{aligned}P(\\textrm{within limits for 10 days}) =& P(\\textrm{within limits for day 1}) \\cdot P(\\textrm{within limits for day 2}) \\cdot \\dots \\cdot P(\\textrm{within limits for day 10})\\\\ &= 0.975^{10} \\approx 0.7763296 \\end{aligned}\\)\nThere is a \\(1-0.7763296 = 0.2236704\\) percent false positive rate\nManagement must decide if there is a false positive by checking for mechanical errors and inspecting equipment\nAdjust \\(\\alpha\\) value to address this"
  },
  {
    "objectID": "slides/lect_13.html#control-charts-for-variation",
    "href": "slides/lect_13.html#control-charts-for-variation",
    "title": "Chapter 14: Sampling variation and quality",
    "section": "Control charts for variation",
    "text": "Control charts for variation\nX-bar charts are slow to detect under or over filling\n\n\nS-Chart tracks the standard deviation from sample to sample\nR-Chart tracks the range from sample to sample\n\n\n\n\nstat1010-f22.github.io/website"
  },
  {
    "objectID": "slides/lect_17.html#revision",
    "href": "slides/lect_17.html#revision",
    "title": "Chapter 18: Inference for counts",
    "section": "Revision",
    "text": "Revision\nWe have discussed this before and now we will formalize this work. Like walking up a lighthouse."
  },
  {
    "objectID": "slides/lect_17.html#test-of-independence",
    "href": "slides/lect_17.html#test-of-independence",
    "title": "Chapter 18: Inference for counts",
    "section": "Test of independence",
    "text": "Test of independence\n\\[H_0: \\text{Qualitative variable 1 and Qualitative variable 2 are independent}\\]\n\\[H_A: \\text{Qualitative variable 1 and Qualitative variable 2 are not independent}\\]"
  },
  {
    "objectID": "slides/lect_17.html#example-1",
    "href": "slides/lect_17.html#example-1",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 1",
    "text": "Example 1\nA manufacturing firm is considering a shift from a 5-day workweek (8 hours per day) to a 4-day workweek (10 hours per day). Samples of the preferences of 188 employees in two divisions produced the following contingency table:\nObserved counts\n\n\n\n\n\nDivisions\n\n\n\n\n\n\n\n\nClerical\nProduction\nTotal\n\n\nPreferences\n5-day\n17\n46\n63\n\n\n\n4-day\n28\n38\n66\n\n\n\nTotal\n45\n84\n129\n\n\n\na. What would it mean if the preference of employees is independent of division?\nb. State \\(H_0\\) for the \\(\\chi^2\\) test of independence in terms of the parameters of two segments of the population of employees."
  },
  {
    "objectID": "slides/lect_17.html#example-1---solns",
    "href": "slides/lect_17.html#example-1---solns",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 1 - solns",
    "text": "Example 1 - solns\n\nIndependence means that all the percent of people who prefer a 5-day work week is the same in all divisions. Lack of independence implies different percentages across divisions\n\\(H_0: p_{clerical} = p_{production}\\) \\(H_A: \\text{at least one of these differs}\\)"
  },
  {
    "objectID": "slides/lect_17.html#calculating-chi2",
    "href": "slides/lect_17.html#calculating-chi2",
    "title": "Chapter 18: Inference for counts",
    "section": "Calculating \\(\\chi^2\\)",
    "text": "Calculating \\(\\chi^2\\)\nIf one variable is independent of the other, then the variable should have the same proportion in each level as the totals have in each level.\nExpected counts\n\n\n\n\n\n\n\n\n\n\n\n\nDivisions\n\n\n\n\n\n\n\n\nClerical\nProduction\nTotal\n\n\nPreferences\n5-day\n\\(45/129\\cdot63 \\approx 22\\)\n\\(84/129\\cdot63 \\approx 41\\)\n63\n\n\n\n4-day\n\\(45/129\\cdot66 \\approx 23\\)\n\\(84/129\\cdot66 \\approx 43\\)\n66\n\n\n\nTotal\n45\n84\n129"
  },
  {
    "objectID": "slides/lect_17.html#calculating-chi2-1",
    "href": "slides/lect_17.html#calculating-chi2-1",
    "title": "Chapter 18: Inference for counts",
    "section": "Calculating \\(\\chi^2\\)",
    "text": "Calculating \\(\\chi^2\\)\n\\[ \\chi^2 = sum\\frac{(observed - expected)^2}{expected}\\]\nWe want to know how much this varies from the expected that is why the expected is the denominator."
  },
  {
    "objectID": "slides/lect_17.html#example-2",
    "href": "slides/lect_17.html#example-2",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 2",
    "text": "Example 2\nFind the expected counts for the following observed variables\n\n\n\n\n \n  \n    gender \n    happy \n    meh \n    sad \n  \n \n\n  \n    female \n    100 \n    30 \n    110 \n  \n  \n    male \n    70 \n    32 \n    120"
  },
  {
    "objectID": "slides/lect_17.html#example-2---solns",
    "href": "slides/lect_17.html#example-2---solns",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 2 - solns",
    "text": "Example 2 - solns\n\n\n\n\n \n  \n    happy \n    meh \n    sad \n  \n \n\n  \n    88.31169 \n    32.20779 \n    119.4805 \n  \n  \n    81.68831 \n    29.79221 \n    110.5195"
  },
  {
    "objectID": "slides/lect_17.html#plot",
    "href": "slides/lect_17.html#plot",
    "title": "Chapter 18: Inference for counts",
    "section": "Plot",
    "text": "Plot\n\n\n\ngender <- c(rep(\"male\", 222), \n            rep(\"female\", 240))\nmood <- c(rep(\"happy\", 70), rep(\"meh\",32),\n          rep(\"sad\", 120), rep(\"happy\", 100),\n          rep(\"meh\", 30), rep(\"sad\", 110))\nd <- tibble(gender, mood)\n \n## draw the plot\nggplot(data = d) +\n  geom_bar(aes(x = mood, fill = gender),\n           position = \"fill\")"
  },
  {
    "objectID": "slides/lect_17.html#reading-plots",
    "href": "slides/lect_17.html#reading-plots",
    "title": "Chapter 18: Inference for counts",
    "section": "Reading plots",
    "text": "Reading plots\nIf the color change in the bars are approximately at the same height in every level of the variable on the \\(x\\) axis, this is evidence against rejecting \\(H_0\\). We might be more inclined to think that \\(H_0\\) is true."
  },
  {
    "objectID": "slides/lect_17.html#assumptions",
    "href": "slides/lect_17.html#assumptions",
    "title": "Chapter 18: Inference for counts",
    "section": "Assumptions",
    "text": "Assumptions\n\nNo lurking variables\nData is a random sample from the population\nCategories must be mutually exclusive\nEach cell count must be at least 5 or 10 (more here)"
  },
  {
    "objectID": "slides/lect_17.html#degrees-of-freedom",
    "href": "slides/lect_17.html#degrees-of-freedom",
    "title": "Chapter 18: Inference for counts",
    "section": "degrees of freedom",
    "text": "degrees of freedom\n\\[df \\text{ for a } \\chi^2 \\text{ test of independence} = (r-1)(c-1)\\]\nwhere \\(r= \\text{number of rows}\\) and \\(c= \\text{number of columns}\\)\nAssumptions:\n\nEach cell count ideally 10, or 5 if the test has 4 or more degrees of freedom."
  },
  {
    "objectID": "slides/lect_17.html#in-r",
    "href": "slides/lect_17.html#in-r",
    "title": "Chapter 18: Inference for counts",
    "section": "In R",
    "text": "In R\n\n\nFind the totals\nCompute the expected counts\nCalculate \\(\\chi^2 = sum\\frac{(observed - expected)^2}{expected}\\)\nFind the df \\((r-1)(c-1)\\)\nSolve for the p-value 1 - pchisq(chi-square, df)"
  },
  {
    "objectID": "slides/lect_17.html#example-3",
    "href": "slides/lect_17.html#example-3",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 3",
    "text": "Example 3\nIn example 2, perform a hypothesis test and find the \\(\\chi^2\\) value and the p-value. Are gender and mood independent?"
  },
  {
    "objectID": "slides/lect_17.html#steps-for-a-hypothesis-test",
    "href": "slides/lect_17.html#steps-for-a-hypothesis-test",
    "title": "Chapter 18: Inference for counts",
    "section": "Steps for a hypothesis test",
    "text": "Steps for a hypothesis test\n\nState hypotheses\nDetermine the level of significance\nCheck conditions\nCalculate test statistic\nCompute p-value\nGeneric conclusion\nInterpret in context"
  },
  {
    "objectID": "slides/lect_17.html#example-3---solns",
    "href": "slides/lect_17.html#example-3---solns",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 3 - solns",
    "text": "Example 3 - solns\n\n\n\\(H_0: p_h = p_m = p_s\\) and \\(H_A: \\text{at least one differs}\\)\n\\(\\alpha = 0.05\\)\nYes.\n\\(\\begin{aligned} \\chi^2 &= sum\\frac{(observed - expected)^2}{expected} \\\\ &= \\frac{(100-88.3)^2}{88.3} + \\frac{(70-81.7)^2}{81.7} + ... + \\frac{(120-119.5)^2}{119.5}\\\\ &\\approx 5.0999\\end{aligned}\\) \\(df = (3-1)\\cdot(2-1)\\)"
  },
  {
    "objectID": "slides/lect_17.html#example-3---solns-1",
    "href": "slides/lect_17.html#example-3---solns-1",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 3 - solns",
    "text": "Example 3 - solns\n\n\n1 - pchisq(5.099, df = 2)\\(\\approx 0.078\\)\nWe fail to reject \\(H_0\\)\nWe do not have evidence that gender and mood are associated, the data suggests they are independent."
  },
  {
    "objectID": "slides/lect_17.html#example-3---solns-2",
    "href": "slides/lect_17.html#example-3---solns-2",
    "title": "Chapter 18: Inference for counts",
    "section": "Example 3 - solns",
    "text": "Example 3 - solns\n\nd %>% \n  count(mood, gender) %>% \n  pivot_wider(names_from = mood, values_from = n) %>% \n  select(-gender) %>% \n  chisq.test()\n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 5.0999, df = 2, p-value = 0.07809"
  },
  {
    "objectID": "slides/lect_17.html#test-for-goodness-of-fit",
    "href": "slides/lect_17.html#test-for-goodness-of-fit",
    "title": "Chapter 18: Inference for counts",
    "section": "Test for goodness of fit",
    "text": "Test for goodness of fit\nWhen an outcome is either Binomial or Poisson one way to check that predictions are correct is to perform a \\(\\chi^2\\) test on the actual and predicted.\n\n\nA small p-value to indicates that these values are not independent."
  },
  {
    "objectID": "slides/lect_03.html#definition-of-categorical-variable",
    "href": "slides/lect_03.html#definition-of-categorical-variable",
    "title": "Lec 3 - Exploring categorical data",
    "section": "Definition of categorical variable",
    "text": "Definition of categorical variable\nA categorical or qualitative variable is a variable that can not be measured. They are descriptors or grouping factors."
  },
  {
    "objectID": "slides/lect_03.html#the-purpose-of-exploring-categorical-variables",
    "href": "slides/lect_03.html#the-purpose-of-exploring-categorical-variables",
    "title": "Lec 3 - Exploring categorical data",
    "section": "The purpose of exploring categorical variables",
    "text": "The purpose of exploring categorical variables\n\nExploratory Data Analysis is about learning the structure of a dataset through a series of numerical and graphical techniques.\nWhen you do EDA, you’ll look for both\n\ngeneral trends and\ninteresting outliers in your data.\n\n\ngenerate questions that will help inform subsequent analysis."
  },
  {
    "objectID": "slides/lect_11.html#binomial---normal",
    "href": "slides/lect_11.html#binomial---normal",
    "title": "Chapter 12: The normal probability model",
    "section": "Binomial -> Normal",
    "text": "Binomial -> Normal\nAs the number of trials increases, the binomial pdf becomes well approximated by a normal distribution.\n“Observed data often represent the accumulation of many small factors.”"
  },
  {
    "objectID": "slides/lect_11.html#central-limit-theorem",
    "href": "slides/lect_11.html#central-limit-theorem",
    "title": "Chapter 12: The normal probability model",
    "section": "Central limit theorem",
    "text": "Central limit theorem\n\nThere are multiple versions of the CLT\n\nThe probability distribution of a sum of independent random variables of comparable variance approaches a normal distribution as the number of summed random variables increases."
  },
  {
    "objectID": "slides/lect_11.html#shifts-scales",
    "href": "slides/lect_11.html#shifts-scales",
    "title": "Chapter 12: The normal probability model",
    "section": "Shifts & scales",
    "text": "Shifts & scales"
  },
  {
    "objectID": "slides/lect_11.html#standardizing",
    "href": "slides/lect_11.html#standardizing",
    "title": "Chapter 12: The normal probability model",
    "section": "Standardizing",
    "text": "Standardizing\n\nBooks and tables of just normal probabilities to multiple decimal places.\n\nHistorically, it could be quite hard to find probabilities, so standardizing was important.\n\n\nuse shift and scale information from above\n\\(Z = \\frac{X-\\mu_X}{\\sigma_X}\\)\nFind \\(E(Z)\\)\n\\(E(Z) = E(\\frac{X-\\mu_X}{\\sigma_X})\\)\n\\[\\begin{aligned}\nE(Z) & = E(\\frac{X-\\mu_X}{\\sigma_X})\\\\\n& = \\frac{1}{\\sigma_X}E(X-\\mu_X) \\\\\n&= 0\n\\end{aligned}\\]\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_11.html#standardizing-1",
    "href": "slides/lect_11.html#standardizing-1",
    "title": "Chapter 12: The normal probability model",
    "section": "Standardizing",
    "text": "Standardizing\n\n\nuse shift and scale information from above\n\\(Z = \\frac{X-\\mu_X}{\\sigma_X}\\)\nFind \\(Var(Z)\\)\n\\[\\begin{aligned}\nVar(Z) & = Var(\\frac{X-\\mu_X}{\\sigma_X})\\\\\n& = \\frac{1}{\\sigma^2}Var(X-\\mu_X) \\\\\n&= \\frac{\\sigma^2}{\\sigma^2} = 1\n\\end{aligned}\\]\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_11.html#example",
    "href": "slides/lect_11.html#example",
    "title": "Chapter 12: The normal probability model",
    "section": "Example",
    "text": "Example\nLet \\(Y \\sim N(\\mu_Y = 5, \\sigma_Y^2 = 4)\\), standardize the following and find the probability:\n\n\n\\(P(Y < 3)\\)\n\\[\\begin{aligned}\nP(Y < 3) &= P(\\frac{Y - \\mu_Y}{\\sigma_Y} < \\frac{3-\\mu_Y}{\\sigma_Y}) \\\\\n& = P(Z < \\frac{3-5}{2} = -1)\n\\end{aligned}\\]\npnorm(-1) \\(0.1586553\\)\npnorm(-1, mean = 5, sd = 2)\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_11.html#queen-bee",
    "href": "slides/lect_11.html#queen-bee",
    "title": "Chapter 12: The normal probability model",
    "section": "Queen Bee",
    "text": "Queen Bee\n\nTo the left, to the left, pnorm is to the left\n\nPercentiles are always to the left."
  },
  {
    "objectID": "slides/lect_11.html#example-1",
    "href": "slides/lect_11.html#example-1",
    "title": "Chapter 12: The normal probability model",
    "section": "Example",
    "text": "Example\n\nBoth of these probabilities are the same because of symmetry of the normal distribution. How does this generalize to all normal distributions?\n\nLet \\(Y \\sim N(\\mu_Y = 5, \\sigma_Y^2 = 4)\\), standardize the following and find the probability:\n\n\n\\(P(Y > 7)\\)\n\\[\\begin{aligned}\nP(Y > 7) &= P(\\frac{Y - \\mu_Y}{\\sigma_Y} > \\frac{7-\\mu_Y}{\\sigma_Y}) \\\\\n& = P(Z > \\frac{7-5}{2} = 1)\n\\end{aligned}\\]\npnorm(1) \\(0.8413447\\)\npnorm(1, mean = 5, sd = 2)\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_11.html#graph",
    "href": "slides/lect_11.html#graph",
    "title": "Chapter 12: The normal probability model",
    "section": "Graph",
    "text": "Graph"
  },
  {
    "objectID": "slides/lect_11.html#example-contd",
    "href": "slides/lect_11.html#example-contd",
    "title": "Chapter 12: The normal probability model",
    "section": "Example (cont’d)",
    "text": "Example (cont’d)\n\nThis and the previous example have the same probabilities attached to them b/c the distribution is symmetric\nSince they generally have the same shape what other things are true?\n\n\n\n\\(P(Z > 1) = 1 - P(Z<1)\\)\n1 - pnorm(1) \\(0.1586553\\)\n1 - pnorm(1, mean = 5, sd = 2)\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lect_11.html#rule",
    "href": "slides/lect_11.html#rule",
    "title": "Chapter 12: The normal probability model",
    "section": "Rule",
    "text": "Rule\n\nThis is true of all normal distributions, point of inflection, other things.\nWe will also need to learn to undo things. If we have a probability go backwards and find the z-score."
  },
  {
    "objectID": "slides/lect_11.html#undo",
    "href": "slides/lect_11.html#undo",
    "title": "Chapter 12: The normal probability model",
    "section": "Undo",
    "text": "Undo"
  },
  {
    "objectID": "slides/lect_11.html#plot---normal",
    "href": "slides/lect_11.html#plot---normal",
    "title": "Chapter 12: The normal probability model",
    "section": "Plot - normal",
    "text": "Plot - normal"
  },
  {
    "objectID": "slides/lect_11.html#plot---tale-of-2-tails",
    "href": "slides/lect_11.html#plot---tale-of-2-tails",
    "title": "Chapter 12: The normal probability model",
    "section": "Plot - tale of 2 tails",
    "text": "Plot - tale of 2 tails"
  },
  {
    "objectID": "slides/lect_11.html#tale-of-2-tails",
    "href": "slides/lect_11.html#tale-of-2-tails",
    "title": "Chapter 12: The normal probability model",
    "section": "Tale of 2 tails",
    "text": "Tale of 2 tails"
  },
  {
    "objectID": "slides/lect_11.html#fat-tails",
    "href": "slides/lect_11.html#fat-tails",
    "title": "Chapter 12: The normal probability model",
    "section": "Fat tails",
    "text": "Fat tails"
  },
  {
    "objectID": "slides/lect_11.html#thin-tails",
    "href": "slides/lect_11.html#thin-tails",
    "title": "Chapter 12: The normal probability model",
    "section": "Thin tails",
    "text": "Thin tails"
  },
  {
    "objectID": "slides/lect_11.html#bimodal",
    "href": "slides/lect_11.html#bimodal",
    "title": "Chapter 12: The normal probability model",
    "section": "Bimodal",
    "text": "Bimodal"
  },
  {
    "objectID": "slides/lect_11.html#qqplot-in-r",
    "href": "slides/lect_11.html#qqplot-in-r",
    "title": "Chapter 12: The normal probability model",
    "section": "QQPlot in R",
    "text": "QQPlot in R\n\nggplot(diamonds, aes(sample=price)) +\n  stat_qq() + # add the dots\n  stat_qq_line() # and the line"
  },
  {
    "objectID": "slides/lect_11.html#skewness",
    "href": "slides/lect_11.html#skewness",
    "title": "Chapter 12: The normal probability model",
    "section": "Skewness",
    "text": "Skewness\n\n\nfind the \\(z\\) scores for all data (\\(z_i = \\frac{x_i - \\bar{x}}{s}\\))\n\\[K_3 = \\frac{z_1^3 +z_2^3 + ... + z_n^3}{n}\\]\nIf \\(K_3 \\approx 0\\), then \\(x\\) is symmetric\nAs \\(K_3\\) gets larger than 0, more right-skewed\nAs \\(K_3\\) gets smaller than 0, more left-skewed"
  },
  {
    "objectID": "slides/lect_11.html#kurtosis",
    "href": "slides/lect_11.html#kurtosis",
    "title": "Chapter 12: The normal probability model",
    "section": "Kurtosis",
    "text": "Kurtosis\n\n\nfind the \\(z\\) scores for all data (\\(z_i = \\frac{x_i - \\bar{x}}{s}\\))\n\\[K_4 = \\frac{z_1^4 +z_2^4 + ... + z_n^4}{n} - 3\\]\nIf \\(K_4 \\approx 0\\), then \\(x\\) is approximately normal\nAs \\(K_4 < 0\\) flat uniform distribution without tails\nAs \\(K_4 > 0\\) many outliers"
  },
  {
    "objectID": "slides/lect_11.html#take-home",
    "href": "slides/lect_11.html#take-home",
    "title": "Chapter 12: The normal probability model",
    "section": "Take home",
    "text": "Take home\nIf you see departures from normality (large or small kurtosis, QQ plots that deviate from a straight line) PLOT the data and check.\n\n\nstat1010-f22.github.io/website"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "STAT 1010 - Fall 2022",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  }
]